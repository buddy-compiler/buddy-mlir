#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<()[s0] -> (s0 * 151936)>
#map2 = affine_map<(d0, d1, d2) -> (d0, d1)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1) -> (d0, 0)>
#map5 = affine_map<()[s0] -> (s0 * 64)>
#map6 = affine_map<(d0, d1, d2) -> (d0, 0, d2)>
#map7 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map8 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map9 = affine_map<(d0, d1, d2) -> (d0, d1, 0)>
#map10 = affine_map<()[s0] -> (s0 * 1536)>
#map11 = affine_map<()[s0] -> (s0 * 256)>
#map12 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map13 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, 0)>
module {
  memref.global "private" constant @__constant_1x1x2x64xf32 : memref<1x1x2x64xf32> = dense<0.000000e+00> {alignment = 64 : i64}
  memref.global "private" constant @__constant_1x1x1xf32_1 : memref<1x1x1xf32> = dense<9.99999997E-7> {alignment = 64 : i64}
  memref.global "private" constant @__constant_1x1536xf32 : memref<1x1536xf32> = dense<0.000000e+00> {alignment = 64 : i64}
  memref.global "private" constant @__constant_1x256xf32 : memref<1x256xf32> = dense<0.000000e+00> {alignment = 64 : i64}
  memref.global "private" constant @__constant_1x2x1x128xi64 : memref<1x2x1x128xi64> = dense<0> {alignment = 64 : i64}
  memref.global "private" constant @__constant_1x8960xf32 : memref<1x8960xf32> = dense<0.000000e+00> {alignment = 64 : i64}
  memref.global "private" constant @__constant_1x151936xf32 : memref<1x151936xf32> = dense<0.000000e+00> {alignment = 64 : i64}
  memref.global "private" constant @__constant_1x1024xi64 : memref<1x1024xi64> = dense<"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{alignment = 64 : i64}
  memref.global "private" constant @__constant_1x1x1xf32 : memref<1x1x1xf32> = dense<6.51041686E-4> {alignment = 64 : i64}
  memref.global "private" constant @__constant_1x1x1x1024xf32_0 : memref<1x1x1x1024xf32> = dense<0.000000e+00> {alignment = 64 : i64}
  memref.global "private" constant @__constant_1x1x1x1024xf32 : memref<1x1x1x1024xf32> = dense<0xFF800000> {alignment = 64 : i64}
  func.func @subgraph0_decode(%arg0: memref<151936x1536xf32, strided<[?, ?], offset: ?>>, %arg1: memref<1x1xi64, strided<[?, ?], offset: ?>>, %arg2: memref<1xi64, strided<[?], offset: ?>>, %arg3: memref<1xi64, strided<[?], offset: ?>>, %arg4: memref<64xf32, strided<[?], offset: ?>>, %arg5: memref<1536xf32, strided<[?], offset: ?>>, %arg6: memref<1536xf32, strided<[?], offset: ?>>, %arg7: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg8: memref<256xf32, strided<[?], offset: ?>>, %arg9: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg10: memref<256xf32, strided<[?], offset: ?>>, %arg11: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg12: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg13: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg14: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg15: memref<1536xf32, strided<[?], offset: ?>>, %arg16: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg17: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg18: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg19: memref<1536xf32, strided<[?], offset: ?>>, %arg20: memref<1536xf32, strided<[?], offset: ?>>, %arg21: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg22: memref<256xf32, strided<[?], offset: ?>>, %arg23: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg24: memref<256xf32, strided<[?], offset: ?>>, %arg25: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg26: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg27: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg28: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg29: memref<1536xf32, strided<[?], offset: ?>>, %arg30: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg31: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg32: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg33: memref<1536xf32, strided<[?], offset: ?>>, %arg34: memref<1536xf32, strided<[?], offset: ?>>, %arg35: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg36: memref<256xf32, strided<[?], offset: ?>>, %arg37: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg38: memref<256xf32, strided<[?], offset: ?>>, %arg39: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg40: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg41: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg42: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg43: memref<1536xf32, strided<[?], offset: ?>>, %arg44: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg45: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg46: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg47: memref<1536xf32, strided<[?], offset: ?>>, %arg48: memref<1536xf32, strided<[?], offset: ?>>, %arg49: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg50: memref<256xf32, strided<[?], offset: ?>>, %arg51: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg52: memref<256xf32, strided<[?], offset: ?>>, %arg53: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg54: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg55: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg56: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg57: memref<1536xf32, strided<[?], offset: ?>>, %arg58: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg59: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg60: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg61: memref<1536xf32, strided<[?], offset: ?>>, %arg62: memref<1536xf32, strided<[?], offset: ?>>, %arg63: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg64: memref<256xf32, strided<[?], offset: ?>>, %arg65: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg66: memref<256xf32, strided<[?], offset: ?>>, %arg67: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg68: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg69: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg70: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg71: memref<1536xf32, strided<[?], offset: ?>>, %arg72: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg73: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg74: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg75: memref<1536xf32, strided<[?], offset: ?>>, %arg76: memref<1536xf32, strided<[?], offset: ?>>, %arg77: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg78: memref<256xf32, strided<[?], offset: ?>>, %arg79: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg80: memref<256xf32, strided<[?], offset: ?>>, %arg81: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg82: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg83: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg84: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg85: memref<1536xf32, strided<[?], offset: ?>>, %arg86: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg87: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg88: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg89: memref<1536xf32, strided<[?], offset: ?>>, %arg90: memref<1536xf32, strided<[?], offset: ?>>, %arg91: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg92: memref<256xf32, strided<[?], offset: ?>>, %arg93: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg94: memref<256xf32, strided<[?], offset: ?>>, %arg95: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg96: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg97: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg98: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg99: memref<1536xf32, strided<[?], offset: ?>>, %arg100: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg101: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg102: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg103: memref<1536xf32, strided<[?], offset: ?>>, %arg104: memref<1536xf32, strided<[?], offset: ?>>, %arg105: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg106: memref<256xf32, strided<[?], offset: ?>>, %arg107: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg108: memref<256xf32, strided<[?], offset: ?>>, %arg109: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg110: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg111: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg112: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg113: memref<1536xf32, strided<[?], offset: ?>>, %arg114: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg115: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg116: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg117: memref<1536xf32, strided<[?], offset: ?>>, %arg118: memref<1536xf32, strided<[?], offset: ?>>, %arg119: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg120: memref<256xf32, strided<[?], offset: ?>>, %arg121: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg122: memref<256xf32, strided<[?], offset: ?>>, %arg123: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg124: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg125: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg126: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg127: memref<1536xf32, strided<[?], offset: ?>>, %arg128: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg129: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg130: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg131: memref<1536xf32, strided<[?], offset: ?>>, %arg132: memref<1536xf32, strided<[?], offset: ?>>, %arg133: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg134: memref<256xf32, strided<[?], offset: ?>>, %arg135: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg136: memref<256xf32, strided<[?], offset: ?>>, %arg137: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg138: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg139: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg140: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg141: memref<1536xf32, strided<[?], offset: ?>>, %arg142: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg143: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg144: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg145: memref<1536xf32, strided<[?], offset: ?>>, %arg146: memref<1536xf32, strided<[?], offset: ?>>, %arg147: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg148: memref<256xf32, strided<[?], offset: ?>>, %arg149: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg150: memref<256xf32, strided<[?], offset: ?>>, %arg151: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg152: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg153: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg154: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg155: memref<1536xf32, strided<[?], offset: ?>>, %arg156: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg157: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg158: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg159: memref<1536xf32, strided<[?], offset: ?>>, %arg160: memref<1536xf32, strided<[?], offset: ?>>, %arg161: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg162: memref<256xf32, strided<[?], offset: ?>>, %arg163: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg164: memref<256xf32, strided<[?], offset: ?>>, %arg165: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg166: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg167: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg168: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg169: memref<1536xf32, strided<[?], offset: ?>>, %arg170: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg171: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg172: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg173: memref<1536xf32, strided<[?], offset: ?>>, %arg174: memref<1536xf32, strided<[?], offset: ?>>, %arg175: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg176: memref<256xf32, strided<[?], offset: ?>>, %arg177: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg178: memref<256xf32, strided<[?], offset: ?>>, %arg179: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg180: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg181: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg182: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg183: memref<1536xf32, strided<[?], offset: ?>>, %arg184: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg185: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg186: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg187: memref<1536xf32, strided<[?], offset: ?>>, %arg188: memref<1536xf32, strided<[?], offset: ?>>, %arg189: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg190: memref<256xf32, strided<[?], offset: ?>>, %arg191: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg192: memref<256xf32, strided<[?], offset: ?>>, %arg193: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg194: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg195: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg196: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg197: memref<1536xf32, strided<[?], offset: ?>>, %arg198: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg199: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg200: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg201: memref<1536xf32, strided<[?], offset: ?>>, %arg202: memref<1536xf32, strided<[?], offset: ?>>, %arg203: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg204: memref<256xf32, strided<[?], offset: ?>>, %arg205: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg206: memref<256xf32, strided<[?], offset: ?>>, %arg207: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg208: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg209: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg210: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg211: memref<1536xf32, strided<[?], offset: ?>>, %arg212: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg213: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg214: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg215: memref<1536xf32, strided<[?], offset: ?>>, %arg216: memref<1536xf32, strided<[?], offset: ?>>, %arg217: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg218: memref<256xf32, strided<[?], offset: ?>>, %arg219: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg220: memref<256xf32, strided<[?], offset: ?>>, %arg221: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg222: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg223: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg224: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg225: memref<1536xf32, strided<[?], offset: ?>>, %arg226: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg227: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg228: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg229: memref<1536xf32, strided<[?], offset: ?>>, %arg230: memref<1536xf32, strided<[?], offset: ?>>, %arg231: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg232: memref<256xf32, strided<[?], offset: ?>>, %arg233: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg234: memref<256xf32, strided<[?], offset: ?>>, %arg235: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg236: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg237: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg238: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg239: memref<1536xf32, strided<[?], offset: ?>>, %arg240: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg241: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg242: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg243: memref<1536xf32, strided<[?], offset: ?>>, %arg244: memref<1536xf32, strided<[?], offset: ?>>, %arg245: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg246: memref<256xf32, strided<[?], offset: ?>>, %arg247: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg248: memref<256xf32, strided<[?], offset: ?>>, %arg249: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg250: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg251: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg252: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg253: memref<1536xf32, strided<[?], offset: ?>>, %arg254: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg255: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg256: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg257: memref<1536xf32, strided<[?], offset: ?>>, %arg258: memref<1536xf32, strided<[?], offset: ?>>, %arg259: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg260: memref<256xf32, strided<[?], offset: ?>>, %arg261: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg262: memref<256xf32, strided<[?], offset: ?>>, %arg263: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg264: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg265: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg266: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg267: memref<1536xf32, strided<[?], offset: ?>>, %arg268: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg269: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg270: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg271: memref<1536xf32, strided<[?], offset: ?>>, %arg272: memref<1536xf32, strided<[?], offset: ?>>, %arg273: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg274: memref<256xf32, strided<[?], offset: ?>>, %arg275: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg276: memref<256xf32, strided<[?], offset: ?>>, %arg277: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg278: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg279: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg280: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg281: memref<1536xf32, strided<[?], offset: ?>>, %arg282: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg283: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg284: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg285: memref<1536xf32, strided<[?], offset: ?>>, %arg286: memref<1536xf32, strided<[?], offset: ?>>, %arg287: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg288: memref<256xf32, strided<[?], offset: ?>>, %arg289: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg290: memref<256xf32, strided<[?], offset: ?>>, %arg291: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg292: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg293: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg294: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg295: memref<1536xf32, strided<[?], offset: ?>>, %arg296: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg297: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg298: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg299: memref<1536xf32, strided<[?], offset: ?>>, %arg300: memref<1536xf32, strided<[?], offset: ?>>, %arg301: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg302: memref<256xf32, strided<[?], offset: ?>>, %arg303: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg304: memref<256xf32, strided<[?], offset: ?>>, %arg305: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg306: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg307: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg308: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg309: memref<1536xf32, strided<[?], offset: ?>>, %arg310: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg311: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg312: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg313: memref<1536xf32, strided<[?], offset: ?>>, %arg314: memref<1536xf32, strided<[?], offset: ?>>, %arg315: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg316: memref<256xf32, strided<[?], offset: ?>>, %arg317: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg318: memref<256xf32, strided<[?], offset: ?>>, %arg319: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg320: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg321: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg322: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg323: memref<1536xf32, strided<[?], offset: ?>>, %arg324: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg325: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg326: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg327: memref<1536xf32, strided<[?], offset: ?>>, %arg328: memref<1536xf32, strided<[?], offset: ?>>, %arg329: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg330: memref<256xf32, strided<[?], offset: ?>>, %arg331: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg332: memref<256xf32, strided<[?], offset: ?>>, %arg333: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg334: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg335: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg336: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg337: memref<1536xf32, strided<[?], offset: ?>>, %arg338: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg339: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg340: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg341: memref<1536xf32, strided<[?], offset: ?>>, %arg342: memref<1536xf32, strided<[?], offset: ?>>, %arg343: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg344: memref<256xf32, strided<[?], offset: ?>>, %arg345: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg346: memref<256xf32, strided<[?], offset: ?>>, %arg347: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg348: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg349: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg350: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg351: memref<1536xf32, strided<[?], offset: ?>>, %arg352: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg353: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg354: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg355: memref<1536xf32, strided<[?], offset: ?>>, %arg356: memref<1536xf32, strided<[?], offset: ?>>, %arg357: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg358: memref<256xf32, strided<[?], offset: ?>>, %arg359: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg360: memref<256xf32, strided<[?], offset: ?>>, %arg361: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg362: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg363: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg364: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg365: memref<1536xf32, strided<[?], offset: ?>>, %arg366: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg367: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg368: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg369: memref<1536xf32, strided<[?], offset: ?>>, %arg370: memref<1536xf32, strided<[?], offset: ?>>, %arg371: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg372: memref<256xf32, strided<[?], offset: ?>>, %arg373: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg374: memref<256xf32, strided<[?], offset: ?>>, %arg375: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg376: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg377: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg378: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg379: memref<1536xf32, strided<[?], offset: ?>>, %arg380: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg381: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg382: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg383: memref<1536xf32, strided<[?], offset: ?>>, %arg384: memref<1536xf32, strided<[?], offset: ?>>, %arg385: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg386: memref<256xf32, strided<[?], offset: ?>>, %arg387: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg388: memref<256xf32, strided<[?], offset: ?>>, %arg389: memref<1536x256xf32, strided<[?, ?], offset: ?>>, %arg390: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg391: memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>>, %arg392: memref<1536x1536xf32, strided<[?, ?], offset: ?>>, %arg393: memref<1536xf32, strided<[?], offset: ?>>, %arg394: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg395: memref<1536x8960xf32, strided<[?, ?], offset: ?>>, %arg396: memref<8960x1536xf32, strided<[?, ?], offset: ?>>, %arg397: memref<1536xf32, strided<[?], offset: ?>>, %arg398: memref<1536x151936xf32, strided<[?, ?], offset: ?>>) -> (memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x1x151936xf32>) {
    %true = arith.constant true
    %c2_i32 = arith.constant 2 : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant 1.000000e+00 : f32
    %cst_1 = arith.constant 0.0883883461 : f32
    %cst_2 = arith.constant -1.000000e+30 : f32
    %c6 = arith.constant 6 : index
    %cst_3 = arith.constant dense<0.000000e+00> : vector<16xf32>
    %0 = memref.get_global @__constant_1x1x1x1024xf32 : memref<1x1x1x1024xf32>
    %1 = memref.get_global @__constant_1x1x1x1024xf32_0 : memref<1x1x1x1024xf32>
    %2 = memref.get_global @__constant_1x1x1xf32 : memref<1x1x1xf32>
    %3 = memref.get_global @__constant_1x1024xi64 : memref<1x1024xi64>
    %4 = memref.get_global @__constant_1x151936xf32 : memref<1x151936xf32>
    %5 = memref.get_global @__constant_1x8960xf32 : memref<1x8960xf32>
    %6 = memref.get_global @__constant_1x2x1x128xi64 : memref<1x2x1x128xi64>
    %7 = memref.get_global @__constant_1x256xf32 : memref<1x256xf32>
    %8 = memref.get_global @__constant_1x1536xf32 : memref<1x1536xf32>
    %9 = memref.get_global @__constant_1x1x1xf32_1 : memref<1x1x1xf32>
    %10 = memref.get_global @__constant_1x1x2x64xf32 : memref<1x1x2x64xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x1xi32>
    linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg1 : memref<1x1xi64, strided<[?, ?], offset: ?>>) outs(%alloc : memref<1x1xi32>) {
    ^bb0(%in: i64, %out: i32):
      %211 = arith.trunci %in : i64 to i32
      linalg.yield %211 : i32
    }
    %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %arg0 : memref<151936x1536xf32, strided<[?, ?], offset: ?>> -> memref<f32>, index, index, index, index, index
    %11 = affine.apply #map1()[%strides#0]
    %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%offset], sizes: [1, 151936, 1536], strides: [%11, %strides#0, %strides#1] : memref<f32> to memref<1x151936x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc : memref<1x1xi32>) outs(%alloc_4 : memref<1x1x1536xf32>) {
    ^bb0(%in: i32, %out: f32):
      %211 = arith.index_cast %in : i32 to index
      %212 = linalg.index 2 : index
      %213 = memref.load %reinterpret_cast[%c0, %211, %212] : memref<1x151936x1536xf32, strided<[?, ?, ?], offset: ?>>
      linalg.yield %213 : f32
    }
    %base_buffer_5, %offset_6, %sizes_7, %strides_8 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_9 = memref.reinterpret_cast %base_buffer_5 to offset: [%offset_6], sizes: [1, 1], strides: [%strides_8, %strides_8] : memref<i64> to memref<1x1xi64, strided<[?, ?], offset: ?>>
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<1x1024xi1>
    linalg.generic {indexing_maps = [#map4, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_9, %3 : memref<1x1xi64, strided<[?, ?], offset: ?>>, memref<1x1024xi64>) outs(%alloc_10 : memref<1x1024xi1>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i1):
      %211 = arith.cmpi sge, %in, %in_3095 : i64
      linalg.yield %211 : i1
    }
    %base_buffer_11, %offset_12, %sizes_13, %strides_14 = memref.extract_strided_metadata %arg4 : memref<64xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %12 = affine.apply #map5()[%strides_14]
    %reinterpret_cast_15 = memref.reinterpret_cast %base_buffer_11 to offset: [%offset_12], sizes: [1, 64, 1], strides: [%12, %strides_14, %strides_14] : memref<f32> to memref<1x64x1xf32, strided<[?, ?, ?], offset: ?>>
    %base_buffer_16, %offset_17, %sizes_18, %strides_19 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_20 = memref.reinterpret_cast %base_buffer_16 to offset: [%offset_17], sizes: [1, 1, 1], strides: [%strides_19, %strides_19, %strides_19] : memref<i64> to memref<1x1x1xi64, strided<[?, ?, ?], offset: ?>>
    %alloc_21 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_20 : memref<1x1x1xi64, strided<[?, ?, ?], offset: ?>>) outs(%alloc_21 : memref<1x1x1xf32>) {
    ^bb0(%in: i64, %out: f32):
      %211 = arith.sitofp %in : i64 to f32
      linalg.yield %211 : f32
    }
    %alloc_22 = memref.alloc() {alignment = 64 : i64} : memref<1x64x1xf32>
    linalg.generic {indexing_maps = [#map3, #map6, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_15, %alloc_21 : memref<1x64x1xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1xf32>) outs(%alloc_22 : memref<1x64x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_23 = memref.reinterpret_cast %alloc_22 to offset: [0], sizes: [1, 1, 1, 64], strides: [64, 64, 64, 1] : memref<1x64x1xf32> to memref<1x1x1x64xf32>
    %alloc_24 = memref.alloc() {alignment = 64 : i64} : memref<1x1x2x64xf32>
    linalg.generic {indexing_maps = [#map7, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_23, %10 : memref<1x1x1x64xf32>, memref<1x1x2x64xf32>) outs(%alloc_24 : memref<1x1x2x64xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_25 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 128], strides: [128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x128xf32>
    %alloc_26 = memref.alloc() {alignment = 64 : i64} : memref<1x1x128xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_25 : memref<1x1x128xf32>) outs(%alloc_26 : memref<1x1x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.cos %in : f32
      linalg.yield %211 : f32
    }
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_25 : memref<1x1x128xf32>) outs(%reinterpret_cast_25 : memref<1x1x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.sin %in : f32
      linalg.yield %211 : f32
    }
    %alloc_27 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_4 : memref<1x1x1536xf32>) outs(%alloc_27 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_28 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_28 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_27 : memref<1x1x1536xf32>) outs(%alloc_28 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_29 = memref.reinterpret_cast %alloc_28 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_30 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_29, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_30 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_31 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_30, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_31 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_32 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_31 : memref<1x1x1xf32>) outs(%alloc_32 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_33 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_4, %alloc_32 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_33 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_34, %offset_35, %sizes_36, %strides_37 = memref.extract_strided_metadata %arg5 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %13 = affine.apply #map10()[%strides_37]
    %14 = affine.apply #map10()[%strides_37]
    %reinterpret_cast_38 = memref.reinterpret_cast %base_buffer_34 to offset: [%offset_35], sizes: [1, 1, 1536], strides: [%13, %14, %strides_37] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_39 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_38, %alloc_33 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_39 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_40 = memref.reinterpret_cast %alloc_39 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_41 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_41 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_40, %arg7 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_41 : memref<1x1536xf32>)
    %base_buffer_42, %offset_43, %sizes_44, %strides_45 = memref.extract_strided_metadata %arg6 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %15 = affine.apply #map10()[%strides_45]
    %reinterpret_cast_46 = memref.reinterpret_cast %base_buffer_42 to offset: [%offset_43], sizes: [1, 1536], strides: [%15, %strides_45] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_47 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_46, %alloc_41 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_47 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_48 = memref.reinterpret_cast %alloc_47 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_49 = memref.reinterpret_cast %alloc_39 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_50 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_50 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_49, %arg9 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_50 : memref<1x256xf32>)
    %base_buffer_51, %offset_52, %sizes_53, %strides_54 = memref.extract_strided_metadata %arg8 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %16 = affine.apply #map11()[%strides_54]
    %reinterpret_cast_55 = memref.reinterpret_cast %base_buffer_51 to offset: [%offset_52], sizes: [1, 256], strides: [%16, %strides_54] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_56 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_55, %alloc_50 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_56 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_57 = memref.reinterpret_cast %alloc_56 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_58 = memref.reinterpret_cast %alloc_39 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_59 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_59 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_58, %arg11 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_59 : memref<1x256xf32>)
    %base_buffer_60, %offset_61, %sizes_62, %strides_63 = memref.extract_strided_metadata %arg10 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %17 = affine.apply #map11()[%strides_63]
    %reinterpret_cast_64 = memref.reinterpret_cast %base_buffer_60 to offset: [%offset_61], sizes: [1, 256], strides: [%17, %strides_63] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_65 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_64, %alloc_59 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_65 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_66 = memref.reinterpret_cast %alloc_65 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_67 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_68 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_69 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_48, %reinterpret_cast_67 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_69 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_70 = memref.reinterpret_cast %alloc_47 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_71 = memref.reinterpret_cast %alloc_47 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_72 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_71 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_72 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_73 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_74 = memref.reinterpret_cast %alloc_73 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_72, %reinterpret_cast_74 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_75 = memref.reinterpret_cast %alloc_73 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_70, %reinterpret_cast_75 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_76 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_73, %reinterpret_cast_68 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_76 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_77 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_69, %alloc_76 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_77 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_78 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_57, %reinterpret_cast_67 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_78 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_79 = memref.reinterpret_cast %alloc_56 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_80 = memref.reinterpret_cast %alloc_56 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_81 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_80 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_81 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_82 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_83 = memref.reinterpret_cast %alloc_82 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_81, %reinterpret_cast_83 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_84 = memref.reinterpret_cast %alloc_82 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_79, %reinterpret_cast_84 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_85 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_82, %reinterpret_cast_68 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_85 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_86 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_78, %alloc_85 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_86 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_87 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg12, %alloc_87 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_88, %offset_89, %sizes_90, %strides_91 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_92 = memref.reinterpret_cast %base_buffer_88 to offset: [%offset_89], sizes: [1, 1, 1, 1], strides: [%strides_91, %strides_91, %strides_91, %strides_91] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_93 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_92, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_93 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_86[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_93[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_87[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_94 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg13, %alloc_94 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_95, %offset_96, %sizes_97, %strides_98 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_99 = memref.reinterpret_cast %base_buffer_95 to offset: [%offset_96], sizes: [1, 1, 1, 1], strides: [%strides_98, %strides_98, %strides_98, %strides_98] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_100 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_99, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_100 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_66[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_100[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_94[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_101 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_102 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_101, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_102 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_103 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_104 = memref.alloc() : memref<1x12x1xf32>
    %alloc_105 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_105[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_77[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_87[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_102[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_94[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_105[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_105[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_104[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_105[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_103[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_106 = memref.reinterpret_cast %alloc_103 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_107 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_107 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_106, %arg14 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_107 : memref<1x1536xf32>)
    %reinterpret_cast_108 = memref.reinterpret_cast %alloc_107 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_109 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_4, %reinterpret_cast_108 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_109 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_110 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_109 : memref<1x1x1536xf32>) outs(%alloc_110 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_111 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_111 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_110 : memref<1x1x1536xf32>) outs(%alloc_111 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_112 = memref.reinterpret_cast %alloc_111 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_113 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_112, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_113 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_114 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_113, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_114 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_115 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_114 : memref<1x1x1xf32>) outs(%alloc_115 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_116 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_109, %alloc_115 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_116 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_117, %offset_118, %sizes_119, %strides_120 = memref.extract_strided_metadata %arg15 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %18 = affine.apply #map10()[%strides_120]
    %19 = affine.apply #map10()[%strides_120]
    %reinterpret_cast_121 = memref.reinterpret_cast %base_buffer_117 to offset: [%offset_118], sizes: [1, 1, 1536], strides: [%18, %19, %strides_120] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_122 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_121, %alloc_116 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_122 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_123 = memref.reinterpret_cast %alloc_122 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_124 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_124 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_123, %arg16 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_124 : memref<1x8960xf32>)
    %reinterpret_cast_125 = memref.reinterpret_cast %alloc_124 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_126 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_125 : memref<1x1x8960xf32>) outs(%alloc_126 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_127 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_125, %alloc_126 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_127 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_128 = memref.reinterpret_cast %alloc_122 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_129 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_129 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_128, %arg17 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_129 : memref<1x8960xf32>)
    %reinterpret_cast_130 = memref.reinterpret_cast %alloc_129 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_131 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_127, %reinterpret_cast_130 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_131 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_132 = memref.reinterpret_cast %alloc_131 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_133 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_133 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_132, %arg18 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_133 : memref<1x1536xf32>)
    %reinterpret_cast_134 = memref.reinterpret_cast %alloc_133 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_135 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_109, %reinterpret_cast_134 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_135 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_136 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_135 : memref<1x1x1536xf32>) outs(%alloc_136 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_137 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_137 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_136 : memref<1x1x1536xf32>) outs(%alloc_137 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_138 = memref.reinterpret_cast %alloc_137 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_139 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_138, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_139 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_140 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_139, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_140 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_141 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_140 : memref<1x1x1xf32>) outs(%alloc_141 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_142 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_135, %alloc_141 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_142 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_143, %offset_144, %sizes_145, %strides_146 = memref.extract_strided_metadata %arg19 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %20 = affine.apply #map10()[%strides_146]
    %21 = affine.apply #map10()[%strides_146]
    %reinterpret_cast_147 = memref.reinterpret_cast %base_buffer_143 to offset: [%offset_144], sizes: [1, 1, 1536], strides: [%20, %21, %strides_146] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_148 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_147, %alloc_142 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_148 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_149 = memref.reinterpret_cast %alloc_148 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_150 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_150 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_149, %arg21 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_150 : memref<1x1536xf32>)
    %base_buffer_151, %offset_152, %sizes_153, %strides_154 = memref.extract_strided_metadata %arg20 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %22 = affine.apply #map10()[%strides_154]
    %reinterpret_cast_155 = memref.reinterpret_cast %base_buffer_151 to offset: [%offset_152], sizes: [1, 1536], strides: [%22, %strides_154] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_156 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_155, %alloc_150 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_156 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_157 = memref.reinterpret_cast %alloc_156 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_158 = memref.reinterpret_cast %alloc_148 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_159 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_159 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_158, %arg23 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_159 : memref<1x256xf32>)
    %base_buffer_160, %offset_161, %sizes_162, %strides_163 = memref.extract_strided_metadata %arg22 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %23 = affine.apply #map11()[%strides_163]
    %reinterpret_cast_164 = memref.reinterpret_cast %base_buffer_160 to offset: [%offset_161], sizes: [1, 256], strides: [%23, %strides_163] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_165 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_164, %alloc_159 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_165 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_166 = memref.reinterpret_cast %alloc_165 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_167 = memref.reinterpret_cast %alloc_148 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_168 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_168 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_167, %arg25 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_168 : memref<1x256xf32>)
    %base_buffer_169, %offset_170, %sizes_171, %strides_172 = memref.extract_strided_metadata %arg24 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %24 = affine.apply #map11()[%strides_172]
    %reinterpret_cast_173 = memref.reinterpret_cast %base_buffer_169 to offset: [%offset_170], sizes: [1, 256], strides: [%24, %strides_172] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_174 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_173, %alloc_168 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_174 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_175 = memref.reinterpret_cast %alloc_174 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_176 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_177 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_178 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_157, %reinterpret_cast_176 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_178 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_179 = memref.reinterpret_cast %alloc_156 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_180 = memref.reinterpret_cast %alloc_156 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_181 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_180 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_181 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_182 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_183 = memref.reinterpret_cast %alloc_182 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_181, %reinterpret_cast_183 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_184 = memref.reinterpret_cast %alloc_182 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_179, %reinterpret_cast_184 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_185 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_182, %reinterpret_cast_177 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_185 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_186 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_178, %alloc_185 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_186 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_187 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_166, %reinterpret_cast_176 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_187 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_188 = memref.reinterpret_cast %alloc_165 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_189 = memref.reinterpret_cast %alloc_165 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_190 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_189 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_190 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_191 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_192 = memref.reinterpret_cast %alloc_191 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_190, %reinterpret_cast_192 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_193 = memref.reinterpret_cast %alloc_191 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_188, %reinterpret_cast_193 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_194 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_191, %reinterpret_cast_177 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_194 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_195 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_187, %alloc_194 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_195 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_196 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg26, %alloc_196 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_197, %offset_198, %sizes_199, %strides_200 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_201 = memref.reinterpret_cast %base_buffer_197 to offset: [%offset_198], sizes: [1, 1, 1, 1], strides: [%strides_200, %strides_200, %strides_200, %strides_200] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_202 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_201, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_202 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_195[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_202[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_196[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_203 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg27, %alloc_203 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_204, %offset_205, %sizes_206, %strides_207 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_208 = memref.reinterpret_cast %base_buffer_204 to offset: [%offset_205], sizes: [1, 1, 1, 1], strides: [%strides_207, %strides_207, %strides_207, %strides_207] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_209 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_208, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_209 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_175[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_209[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_203[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_210 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_211 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_210, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_211 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_212 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_213 = memref.alloc() : memref<1x12x1xf32>
    %alloc_214 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_214[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_186[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_196[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_211[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_203[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_214[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_214[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_213[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_214[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_212[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_215 = memref.reinterpret_cast %alloc_212 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_216 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_216 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_215, %arg28 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_216 : memref<1x1536xf32>)
    %reinterpret_cast_217 = memref.reinterpret_cast %alloc_216 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_218 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_135, %reinterpret_cast_217 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_218 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_219 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_218 : memref<1x1x1536xf32>) outs(%alloc_219 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_220 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_220 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_219 : memref<1x1x1536xf32>) outs(%alloc_220 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_221 = memref.reinterpret_cast %alloc_220 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_222 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_221, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_222 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_223 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_222, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_223 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_224 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_223 : memref<1x1x1xf32>) outs(%alloc_224 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_225 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_218, %alloc_224 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_225 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_226, %offset_227, %sizes_228, %strides_229 = memref.extract_strided_metadata %arg29 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %25 = affine.apply #map10()[%strides_229]
    %26 = affine.apply #map10()[%strides_229]
    %reinterpret_cast_230 = memref.reinterpret_cast %base_buffer_226 to offset: [%offset_227], sizes: [1, 1, 1536], strides: [%25, %26, %strides_229] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_231 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_230, %alloc_225 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_231 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_232 = memref.reinterpret_cast %alloc_231 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_233 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_233 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_232, %arg30 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_233 : memref<1x8960xf32>)
    %reinterpret_cast_234 = memref.reinterpret_cast %alloc_233 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_235 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_234 : memref<1x1x8960xf32>) outs(%alloc_235 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_236 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_234, %alloc_235 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_236 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_237 = memref.reinterpret_cast %alloc_231 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_238 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_238 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_237, %arg31 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_238 : memref<1x8960xf32>)
    %reinterpret_cast_239 = memref.reinterpret_cast %alloc_238 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_240 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_236, %reinterpret_cast_239 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_240 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_241 = memref.reinterpret_cast %alloc_240 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_242 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_242 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_241, %arg32 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_242 : memref<1x1536xf32>)
    %reinterpret_cast_243 = memref.reinterpret_cast %alloc_242 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_244 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_218, %reinterpret_cast_243 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_244 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_245 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_244 : memref<1x1x1536xf32>) outs(%alloc_245 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_246 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_246 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_245 : memref<1x1x1536xf32>) outs(%alloc_246 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_247 = memref.reinterpret_cast %alloc_246 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_248 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_247, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_248 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_249 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_248, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_249 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_250 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_249 : memref<1x1x1xf32>) outs(%alloc_250 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_251 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_244, %alloc_250 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_251 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_252, %offset_253, %sizes_254, %strides_255 = memref.extract_strided_metadata %arg33 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %27 = affine.apply #map10()[%strides_255]
    %28 = affine.apply #map10()[%strides_255]
    %reinterpret_cast_256 = memref.reinterpret_cast %base_buffer_252 to offset: [%offset_253], sizes: [1, 1, 1536], strides: [%27, %28, %strides_255] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_257 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_256, %alloc_251 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_257 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_258 = memref.reinterpret_cast %alloc_257 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_259 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_259 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_258, %arg35 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_259 : memref<1x1536xf32>)
    %base_buffer_260, %offset_261, %sizes_262, %strides_263 = memref.extract_strided_metadata %arg34 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %29 = affine.apply #map10()[%strides_263]
    %reinterpret_cast_264 = memref.reinterpret_cast %base_buffer_260 to offset: [%offset_261], sizes: [1, 1536], strides: [%29, %strides_263] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_265 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_264, %alloc_259 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_265 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_266 = memref.reinterpret_cast %alloc_265 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_267 = memref.reinterpret_cast %alloc_257 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_268 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_268 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_267, %arg37 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_268 : memref<1x256xf32>)
    %base_buffer_269, %offset_270, %sizes_271, %strides_272 = memref.extract_strided_metadata %arg36 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %30 = affine.apply #map11()[%strides_272]
    %reinterpret_cast_273 = memref.reinterpret_cast %base_buffer_269 to offset: [%offset_270], sizes: [1, 256], strides: [%30, %strides_272] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_274 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_273, %alloc_268 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_274 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_275 = memref.reinterpret_cast %alloc_274 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_276 = memref.reinterpret_cast %alloc_257 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_277 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_277 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_276, %arg39 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_277 : memref<1x256xf32>)
    %base_buffer_278, %offset_279, %sizes_280, %strides_281 = memref.extract_strided_metadata %arg38 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %31 = affine.apply #map11()[%strides_281]
    %reinterpret_cast_282 = memref.reinterpret_cast %base_buffer_278 to offset: [%offset_279], sizes: [1, 256], strides: [%31, %strides_281] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_283 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_282, %alloc_277 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_283 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_284 = memref.reinterpret_cast %alloc_283 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_285 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_286 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_287 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_266, %reinterpret_cast_285 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_287 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_288 = memref.reinterpret_cast %alloc_265 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_289 = memref.reinterpret_cast %alloc_265 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_290 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_289 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_290 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_291 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_292 = memref.reinterpret_cast %alloc_291 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_290, %reinterpret_cast_292 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_293 = memref.reinterpret_cast %alloc_291 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_288, %reinterpret_cast_293 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_294 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_291, %reinterpret_cast_286 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_294 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_295 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_287, %alloc_294 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_295 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_296 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_275, %reinterpret_cast_285 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_296 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_297 = memref.reinterpret_cast %alloc_274 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_298 = memref.reinterpret_cast %alloc_274 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_299 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_298 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_299 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_300 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_301 = memref.reinterpret_cast %alloc_300 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_299, %reinterpret_cast_301 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_302 = memref.reinterpret_cast %alloc_300 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_297, %reinterpret_cast_302 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_303 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_300, %reinterpret_cast_286 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_303 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_304 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_296, %alloc_303 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_304 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_305 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg40, %alloc_305 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_306, %offset_307, %sizes_308, %strides_309 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_310 = memref.reinterpret_cast %base_buffer_306 to offset: [%offset_307], sizes: [1, 1, 1, 1], strides: [%strides_309, %strides_309, %strides_309, %strides_309] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_311 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_310, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_311 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_304[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_311[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_305[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_312 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg41, %alloc_312 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_313, %offset_314, %sizes_315, %strides_316 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_317 = memref.reinterpret_cast %base_buffer_313 to offset: [%offset_314], sizes: [1, 1, 1, 1], strides: [%strides_316, %strides_316, %strides_316, %strides_316] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_318 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_317, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_318 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_284[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_318[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_312[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_319 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_320 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_319, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_320 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_321 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_322 = memref.alloc() : memref<1x12x1xf32>
    %alloc_323 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_323[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_295[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_305[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_320[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_312[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_323[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_323[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_322[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_323[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_321[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_324 = memref.reinterpret_cast %alloc_321 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_325 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_325 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_324, %arg42 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_325 : memref<1x1536xf32>)
    %reinterpret_cast_326 = memref.reinterpret_cast %alloc_325 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_327 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_244, %reinterpret_cast_326 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_327 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_328 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_327 : memref<1x1x1536xf32>) outs(%alloc_328 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_329 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_329 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_328 : memref<1x1x1536xf32>) outs(%alloc_329 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_330 = memref.reinterpret_cast %alloc_329 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_331 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_330, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_331 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_332 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_331, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_332 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_333 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_332 : memref<1x1x1xf32>) outs(%alloc_333 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_334 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_327, %alloc_333 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_334 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_335, %offset_336, %sizes_337, %strides_338 = memref.extract_strided_metadata %arg43 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %32 = affine.apply #map10()[%strides_338]
    %33 = affine.apply #map10()[%strides_338]
    %reinterpret_cast_339 = memref.reinterpret_cast %base_buffer_335 to offset: [%offset_336], sizes: [1, 1, 1536], strides: [%32, %33, %strides_338] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_340 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_339, %alloc_334 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_340 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_341 = memref.reinterpret_cast %alloc_340 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_342 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_342 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_341, %arg44 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_342 : memref<1x8960xf32>)
    %reinterpret_cast_343 = memref.reinterpret_cast %alloc_342 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_344 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_343 : memref<1x1x8960xf32>) outs(%alloc_344 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_345 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_343, %alloc_344 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_345 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_346 = memref.reinterpret_cast %alloc_340 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_347 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_347 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_346, %arg45 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_347 : memref<1x8960xf32>)
    %reinterpret_cast_348 = memref.reinterpret_cast %alloc_347 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_349 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_345, %reinterpret_cast_348 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_349 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_350 = memref.reinterpret_cast %alloc_349 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_351 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_351 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_350, %arg46 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_351 : memref<1x1536xf32>)
    %reinterpret_cast_352 = memref.reinterpret_cast %alloc_351 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_353 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_327, %reinterpret_cast_352 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_353 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_354 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_353 : memref<1x1x1536xf32>) outs(%alloc_354 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_355 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_355 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_354 : memref<1x1x1536xf32>) outs(%alloc_355 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_356 = memref.reinterpret_cast %alloc_355 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_357 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_356, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_357 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_358 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_357, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_358 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_359 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_358 : memref<1x1x1xf32>) outs(%alloc_359 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_360 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_353, %alloc_359 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_360 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_361, %offset_362, %sizes_363, %strides_364 = memref.extract_strided_metadata %arg47 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %34 = affine.apply #map10()[%strides_364]
    %35 = affine.apply #map10()[%strides_364]
    %reinterpret_cast_365 = memref.reinterpret_cast %base_buffer_361 to offset: [%offset_362], sizes: [1, 1, 1536], strides: [%34, %35, %strides_364] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_366 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_365, %alloc_360 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_366 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_367 = memref.reinterpret_cast %alloc_366 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_368 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_368 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_367, %arg49 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_368 : memref<1x1536xf32>)
    %base_buffer_369, %offset_370, %sizes_371, %strides_372 = memref.extract_strided_metadata %arg48 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %36 = affine.apply #map10()[%strides_372]
    %reinterpret_cast_373 = memref.reinterpret_cast %base_buffer_369 to offset: [%offset_370], sizes: [1, 1536], strides: [%36, %strides_372] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_374 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_373, %alloc_368 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_374 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_375 = memref.reinterpret_cast %alloc_374 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_376 = memref.reinterpret_cast %alloc_366 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_377 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_377 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_376, %arg51 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_377 : memref<1x256xf32>)
    %base_buffer_378, %offset_379, %sizes_380, %strides_381 = memref.extract_strided_metadata %arg50 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %37 = affine.apply #map11()[%strides_381]
    %reinterpret_cast_382 = memref.reinterpret_cast %base_buffer_378 to offset: [%offset_379], sizes: [1, 256], strides: [%37, %strides_381] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_383 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_382, %alloc_377 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_383 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_384 = memref.reinterpret_cast %alloc_383 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_385 = memref.reinterpret_cast %alloc_366 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_386 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_386 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_385, %arg53 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_386 : memref<1x256xf32>)
    %base_buffer_387, %offset_388, %sizes_389, %strides_390 = memref.extract_strided_metadata %arg52 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %38 = affine.apply #map11()[%strides_390]
    %reinterpret_cast_391 = memref.reinterpret_cast %base_buffer_387 to offset: [%offset_388], sizes: [1, 256], strides: [%38, %strides_390] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_392 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_391, %alloc_386 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_392 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_393 = memref.reinterpret_cast %alloc_392 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_394 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_395 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_396 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_375, %reinterpret_cast_394 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_396 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_397 = memref.reinterpret_cast %alloc_374 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_398 = memref.reinterpret_cast %alloc_374 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_399 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_398 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_399 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_400 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_401 = memref.reinterpret_cast %alloc_400 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_399, %reinterpret_cast_401 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_402 = memref.reinterpret_cast %alloc_400 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_397, %reinterpret_cast_402 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_403 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_400, %reinterpret_cast_395 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_403 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_404 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_396, %alloc_403 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_404 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_405 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_384, %reinterpret_cast_394 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_405 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_406 = memref.reinterpret_cast %alloc_383 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_407 = memref.reinterpret_cast %alloc_383 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_408 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_407 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_408 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_409 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_410 = memref.reinterpret_cast %alloc_409 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_408, %reinterpret_cast_410 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_411 = memref.reinterpret_cast %alloc_409 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_406, %reinterpret_cast_411 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_412 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_409, %reinterpret_cast_395 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_412 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_413 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_405, %alloc_412 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_413 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_414 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg54, %alloc_414 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_415, %offset_416, %sizes_417, %strides_418 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_419 = memref.reinterpret_cast %base_buffer_415 to offset: [%offset_416], sizes: [1, 1, 1, 1], strides: [%strides_418, %strides_418, %strides_418, %strides_418] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_420 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_419, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_420 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_413[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_420[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_414[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_421 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg55, %alloc_421 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_422, %offset_423, %sizes_424, %strides_425 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_426 = memref.reinterpret_cast %base_buffer_422 to offset: [%offset_423], sizes: [1, 1, 1, 1], strides: [%strides_425, %strides_425, %strides_425, %strides_425] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_427 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_426, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_427 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_393[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_427[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_421[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_428 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_429 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_428, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_429 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_430 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_431 = memref.alloc() : memref<1x12x1xf32>
    %alloc_432 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_432[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_404[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_414[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_429[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_421[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_432[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_432[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_431[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_432[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_430[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_433 = memref.reinterpret_cast %alloc_430 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_434 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_434 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_433, %arg56 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_434 : memref<1x1536xf32>)
    %reinterpret_cast_435 = memref.reinterpret_cast %alloc_434 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_436 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_353, %reinterpret_cast_435 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_436 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_437 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_436 : memref<1x1x1536xf32>) outs(%alloc_437 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_438 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_438 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_437 : memref<1x1x1536xf32>) outs(%alloc_438 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_439 = memref.reinterpret_cast %alloc_438 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_440 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_439, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_440 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_441 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_440, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_441 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_442 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_441 : memref<1x1x1xf32>) outs(%alloc_442 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_443 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_436, %alloc_442 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_443 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_444, %offset_445, %sizes_446, %strides_447 = memref.extract_strided_metadata %arg57 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %39 = affine.apply #map10()[%strides_447]
    %40 = affine.apply #map10()[%strides_447]
    %reinterpret_cast_448 = memref.reinterpret_cast %base_buffer_444 to offset: [%offset_445], sizes: [1, 1, 1536], strides: [%39, %40, %strides_447] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_449 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_448, %alloc_443 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_449 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_450 = memref.reinterpret_cast %alloc_449 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_451 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_451 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_450, %arg58 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_451 : memref<1x8960xf32>)
    %reinterpret_cast_452 = memref.reinterpret_cast %alloc_451 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_453 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_452 : memref<1x1x8960xf32>) outs(%alloc_453 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_454 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_452, %alloc_453 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_454 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_455 = memref.reinterpret_cast %alloc_449 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_456 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_456 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_455, %arg59 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_456 : memref<1x8960xf32>)
    %reinterpret_cast_457 = memref.reinterpret_cast %alloc_456 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_458 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_454, %reinterpret_cast_457 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_458 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_459 = memref.reinterpret_cast %alloc_458 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_460 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_460 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_459, %arg60 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_460 : memref<1x1536xf32>)
    %reinterpret_cast_461 = memref.reinterpret_cast %alloc_460 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_462 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_436, %reinterpret_cast_461 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_462 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_463 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_462 : memref<1x1x1536xf32>) outs(%alloc_463 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_464 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_464 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_463 : memref<1x1x1536xf32>) outs(%alloc_464 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_465 = memref.reinterpret_cast %alloc_464 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_466 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_465, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_466 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_467 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_466, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_467 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_468 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_467 : memref<1x1x1xf32>) outs(%alloc_468 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_469 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_462, %alloc_468 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_469 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_470, %offset_471, %sizes_472, %strides_473 = memref.extract_strided_metadata %arg61 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %41 = affine.apply #map10()[%strides_473]
    %42 = affine.apply #map10()[%strides_473]
    %reinterpret_cast_474 = memref.reinterpret_cast %base_buffer_470 to offset: [%offset_471], sizes: [1, 1, 1536], strides: [%41, %42, %strides_473] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_475 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_474, %alloc_469 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_475 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_476 = memref.reinterpret_cast %alloc_475 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_477 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_477 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_476, %arg63 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_477 : memref<1x1536xf32>)
    %base_buffer_478, %offset_479, %sizes_480, %strides_481 = memref.extract_strided_metadata %arg62 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %43 = affine.apply #map10()[%strides_481]
    %reinterpret_cast_482 = memref.reinterpret_cast %base_buffer_478 to offset: [%offset_479], sizes: [1, 1536], strides: [%43, %strides_481] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_483 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_482, %alloc_477 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_483 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_484 = memref.reinterpret_cast %alloc_483 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_485 = memref.reinterpret_cast %alloc_475 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_486 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_486 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_485, %arg65 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_486 : memref<1x256xf32>)
    %base_buffer_487, %offset_488, %sizes_489, %strides_490 = memref.extract_strided_metadata %arg64 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %44 = affine.apply #map11()[%strides_490]
    %reinterpret_cast_491 = memref.reinterpret_cast %base_buffer_487 to offset: [%offset_488], sizes: [1, 256], strides: [%44, %strides_490] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_492 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_491, %alloc_486 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_492 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_493 = memref.reinterpret_cast %alloc_492 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_494 = memref.reinterpret_cast %alloc_475 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_495 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_495 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_494, %arg67 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_495 : memref<1x256xf32>)
    %base_buffer_496, %offset_497, %sizes_498, %strides_499 = memref.extract_strided_metadata %arg66 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %45 = affine.apply #map11()[%strides_499]
    %reinterpret_cast_500 = memref.reinterpret_cast %base_buffer_496 to offset: [%offset_497], sizes: [1, 256], strides: [%45, %strides_499] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_501 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_500, %alloc_495 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_501 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_502 = memref.reinterpret_cast %alloc_501 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_503 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_504 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_505 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_484, %reinterpret_cast_503 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_505 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_506 = memref.reinterpret_cast %alloc_483 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_507 = memref.reinterpret_cast %alloc_483 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_508 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_507 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_508 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_509 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_510 = memref.reinterpret_cast %alloc_509 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_508, %reinterpret_cast_510 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_511 = memref.reinterpret_cast %alloc_509 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_506, %reinterpret_cast_511 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_512 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_509, %reinterpret_cast_504 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_512 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_513 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_505, %alloc_512 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_513 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_514 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_493, %reinterpret_cast_503 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_514 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_515 = memref.reinterpret_cast %alloc_492 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_516 = memref.reinterpret_cast %alloc_492 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_517 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_516 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_517 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_518 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_519 = memref.reinterpret_cast %alloc_518 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_517, %reinterpret_cast_519 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_520 = memref.reinterpret_cast %alloc_518 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_515, %reinterpret_cast_520 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_521 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_518, %reinterpret_cast_504 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_521 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_522 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_514, %alloc_521 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_522 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_523 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg68, %alloc_523 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_524, %offset_525, %sizes_526, %strides_527 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_528 = memref.reinterpret_cast %base_buffer_524 to offset: [%offset_525], sizes: [1, 1, 1, 1], strides: [%strides_527, %strides_527, %strides_527, %strides_527] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_529 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_528, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_529 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_522[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_529[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_523[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_530 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg69, %alloc_530 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_531, %offset_532, %sizes_533, %strides_534 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_535 = memref.reinterpret_cast %base_buffer_531 to offset: [%offset_532], sizes: [1, 1, 1, 1], strides: [%strides_534, %strides_534, %strides_534, %strides_534] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_536 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_535, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_536 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_502[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_536[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_530[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_537 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_538 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_537, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_538 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_539 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_540 = memref.alloc() : memref<1x12x1xf32>
    %alloc_541 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_541[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_513[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_523[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_538[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_530[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_541[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_541[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_540[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_541[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_539[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_542 = memref.reinterpret_cast %alloc_539 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_543 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_543 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_542, %arg70 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_543 : memref<1x1536xf32>)
    %reinterpret_cast_544 = memref.reinterpret_cast %alloc_543 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_545 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_462, %reinterpret_cast_544 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_545 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_546 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_545 : memref<1x1x1536xf32>) outs(%alloc_546 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_547 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_547 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_546 : memref<1x1x1536xf32>) outs(%alloc_547 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_548 = memref.reinterpret_cast %alloc_547 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_549 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_548, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_549 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_550 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_549, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_550 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_551 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_550 : memref<1x1x1xf32>) outs(%alloc_551 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_552 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_545, %alloc_551 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_552 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_553, %offset_554, %sizes_555, %strides_556 = memref.extract_strided_metadata %arg71 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %46 = affine.apply #map10()[%strides_556]
    %47 = affine.apply #map10()[%strides_556]
    %reinterpret_cast_557 = memref.reinterpret_cast %base_buffer_553 to offset: [%offset_554], sizes: [1, 1, 1536], strides: [%46, %47, %strides_556] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_558 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_557, %alloc_552 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_558 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_559 = memref.reinterpret_cast %alloc_558 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_560 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_560 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_559, %arg72 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_560 : memref<1x8960xf32>)
    %reinterpret_cast_561 = memref.reinterpret_cast %alloc_560 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_562 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_561 : memref<1x1x8960xf32>) outs(%alloc_562 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_563 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_561, %alloc_562 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_563 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_564 = memref.reinterpret_cast %alloc_558 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_565 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_565 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_564, %arg73 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_565 : memref<1x8960xf32>)
    %reinterpret_cast_566 = memref.reinterpret_cast %alloc_565 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_567 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_563, %reinterpret_cast_566 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_567 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_568 = memref.reinterpret_cast %alloc_567 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_569 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_569 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_568, %arg74 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_569 : memref<1x1536xf32>)
    %reinterpret_cast_570 = memref.reinterpret_cast %alloc_569 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_571 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_545, %reinterpret_cast_570 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_571 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_572 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_571 : memref<1x1x1536xf32>) outs(%alloc_572 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_573 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_573 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_572 : memref<1x1x1536xf32>) outs(%alloc_573 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_574 = memref.reinterpret_cast %alloc_573 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_575 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_574, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_575 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_576 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_575, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_576 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_577 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_576 : memref<1x1x1xf32>) outs(%alloc_577 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_578 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_571, %alloc_577 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_578 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_579, %offset_580, %sizes_581, %strides_582 = memref.extract_strided_metadata %arg75 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %48 = affine.apply #map10()[%strides_582]
    %49 = affine.apply #map10()[%strides_582]
    %reinterpret_cast_583 = memref.reinterpret_cast %base_buffer_579 to offset: [%offset_580], sizes: [1, 1, 1536], strides: [%48, %49, %strides_582] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_584 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_583, %alloc_578 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_584 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_585 = memref.reinterpret_cast %alloc_584 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_586 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_586 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_585, %arg77 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_586 : memref<1x1536xf32>)
    %base_buffer_587, %offset_588, %sizes_589, %strides_590 = memref.extract_strided_metadata %arg76 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %50 = affine.apply #map10()[%strides_590]
    %reinterpret_cast_591 = memref.reinterpret_cast %base_buffer_587 to offset: [%offset_588], sizes: [1, 1536], strides: [%50, %strides_590] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_592 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_591, %alloc_586 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_592 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_593 = memref.reinterpret_cast %alloc_592 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_594 = memref.reinterpret_cast %alloc_584 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_595 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_595 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_594, %arg79 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_595 : memref<1x256xf32>)
    %base_buffer_596, %offset_597, %sizes_598, %strides_599 = memref.extract_strided_metadata %arg78 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %51 = affine.apply #map11()[%strides_599]
    %reinterpret_cast_600 = memref.reinterpret_cast %base_buffer_596 to offset: [%offset_597], sizes: [1, 256], strides: [%51, %strides_599] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_601 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_600, %alloc_595 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_601 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_602 = memref.reinterpret_cast %alloc_601 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_603 = memref.reinterpret_cast %alloc_584 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_604 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_604 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_603, %arg81 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_604 : memref<1x256xf32>)
    %base_buffer_605, %offset_606, %sizes_607, %strides_608 = memref.extract_strided_metadata %arg80 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %52 = affine.apply #map11()[%strides_608]
    %reinterpret_cast_609 = memref.reinterpret_cast %base_buffer_605 to offset: [%offset_606], sizes: [1, 256], strides: [%52, %strides_608] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_610 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_609, %alloc_604 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_610 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_611 = memref.reinterpret_cast %alloc_610 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_612 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_613 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_614 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_593, %reinterpret_cast_612 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_614 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_615 = memref.reinterpret_cast %alloc_592 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_616 = memref.reinterpret_cast %alloc_592 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_617 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_616 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_617 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_618 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_619 = memref.reinterpret_cast %alloc_618 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_617, %reinterpret_cast_619 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_620 = memref.reinterpret_cast %alloc_618 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_615, %reinterpret_cast_620 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_621 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_618, %reinterpret_cast_613 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_621 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_622 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_614, %alloc_621 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_622 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_623 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_602, %reinterpret_cast_612 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_623 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_624 = memref.reinterpret_cast %alloc_601 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_625 = memref.reinterpret_cast %alloc_601 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_626 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_625 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_626 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_627 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_628 = memref.reinterpret_cast %alloc_627 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_626, %reinterpret_cast_628 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_629 = memref.reinterpret_cast %alloc_627 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_624, %reinterpret_cast_629 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_630 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_627, %reinterpret_cast_613 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_630 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_631 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_623, %alloc_630 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_631 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_632 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg82, %alloc_632 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_633, %offset_634, %sizes_635, %strides_636 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_637 = memref.reinterpret_cast %base_buffer_633 to offset: [%offset_634], sizes: [1, 1, 1, 1], strides: [%strides_636, %strides_636, %strides_636, %strides_636] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_638 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_637, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_638 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_631[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_638[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_632[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_639 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg83, %alloc_639 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_640, %offset_641, %sizes_642, %strides_643 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_644 = memref.reinterpret_cast %base_buffer_640 to offset: [%offset_641], sizes: [1, 1, 1, 1], strides: [%strides_643, %strides_643, %strides_643, %strides_643] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_645 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_644, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_645 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_611[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_645[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_639[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_646 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_647 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_646, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_647 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_648 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_649 = memref.alloc() : memref<1x12x1xf32>
    %alloc_650 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_650[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_622[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_632[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_647[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_639[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_650[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_650[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_649[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_650[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_648[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_651 = memref.reinterpret_cast %alloc_648 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_652 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_652 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_651, %arg84 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_652 : memref<1x1536xf32>)
    %reinterpret_cast_653 = memref.reinterpret_cast %alloc_652 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_654 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_571, %reinterpret_cast_653 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_654 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_655 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_654 : memref<1x1x1536xf32>) outs(%alloc_655 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_656 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_656 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_655 : memref<1x1x1536xf32>) outs(%alloc_656 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_657 = memref.reinterpret_cast %alloc_656 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_658 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_657, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_658 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_659 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_658, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_659 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_660 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_659 : memref<1x1x1xf32>) outs(%alloc_660 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_661 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_654, %alloc_660 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_661 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_662, %offset_663, %sizes_664, %strides_665 = memref.extract_strided_metadata %arg85 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %53 = affine.apply #map10()[%strides_665]
    %54 = affine.apply #map10()[%strides_665]
    %reinterpret_cast_666 = memref.reinterpret_cast %base_buffer_662 to offset: [%offset_663], sizes: [1, 1, 1536], strides: [%53, %54, %strides_665] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_667 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_666, %alloc_661 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_667 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_668 = memref.reinterpret_cast %alloc_667 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_669 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_669 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_668, %arg86 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_669 : memref<1x8960xf32>)
    %reinterpret_cast_670 = memref.reinterpret_cast %alloc_669 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_671 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_670 : memref<1x1x8960xf32>) outs(%alloc_671 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_672 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_670, %alloc_671 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_672 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_673 = memref.reinterpret_cast %alloc_667 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_674 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_674 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_673, %arg87 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_674 : memref<1x8960xf32>)
    %reinterpret_cast_675 = memref.reinterpret_cast %alloc_674 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_676 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_672, %reinterpret_cast_675 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_676 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_677 = memref.reinterpret_cast %alloc_676 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_678 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_678 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_677, %arg88 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_678 : memref<1x1536xf32>)
    %reinterpret_cast_679 = memref.reinterpret_cast %alloc_678 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_680 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_654, %reinterpret_cast_679 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_680 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_681 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_680 : memref<1x1x1536xf32>) outs(%alloc_681 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_682 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_682 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_681 : memref<1x1x1536xf32>) outs(%alloc_682 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_683 = memref.reinterpret_cast %alloc_682 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_684 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_683, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_684 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_685 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_684, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_685 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_686 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_685 : memref<1x1x1xf32>) outs(%alloc_686 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_687 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_680, %alloc_686 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_687 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_688, %offset_689, %sizes_690, %strides_691 = memref.extract_strided_metadata %arg89 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %55 = affine.apply #map10()[%strides_691]
    %56 = affine.apply #map10()[%strides_691]
    %reinterpret_cast_692 = memref.reinterpret_cast %base_buffer_688 to offset: [%offset_689], sizes: [1, 1, 1536], strides: [%55, %56, %strides_691] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_693 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_692, %alloc_687 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_693 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_694 = memref.reinterpret_cast %alloc_693 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_695 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_695 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_694, %arg91 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_695 : memref<1x1536xf32>)
    %base_buffer_696, %offset_697, %sizes_698, %strides_699 = memref.extract_strided_metadata %arg90 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %57 = affine.apply #map10()[%strides_699]
    %reinterpret_cast_700 = memref.reinterpret_cast %base_buffer_696 to offset: [%offset_697], sizes: [1, 1536], strides: [%57, %strides_699] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_701 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_700, %alloc_695 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_701 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_702 = memref.reinterpret_cast %alloc_701 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_703 = memref.reinterpret_cast %alloc_693 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_704 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_704 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_703, %arg93 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_704 : memref<1x256xf32>)
    %base_buffer_705, %offset_706, %sizes_707, %strides_708 = memref.extract_strided_metadata %arg92 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %58 = affine.apply #map11()[%strides_708]
    %reinterpret_cast_709 = memref.reinterpret_cast %base_buffer_705 to offset: [%offset_706], sizes: [1, 256], strides: [%58, %strides_708] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_710 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_709, %alloc_704 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_710 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_711 = memref.reinterpret_cast %alloc_710 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_712 = memref.reinterpret_cast %alloc_693 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_713 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_713 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_712, %arg95 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_713 : memref<1x256xf32>)
    %base_buffer_714, %offset_715, %sizes_716, %strides_717 = memref.extract_strided_metadata %arg94 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %59 = affine.apply #map11()[%strides_717]
    %reinterpret_cast_718 = memref.reinterpret_cast %base_buffer_714 to offset: [%offset_715], sizes: [1, 256], strides: [%59, %strides_717] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_719 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_718, %alloc_713 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_719 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_720 = memref.reinterpret_cast %alloc_719 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_721 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_722 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_723 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_702, %reinterpret_cast_721 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_723 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_724 = memref.reinterpret_cast %alloc_701 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_725 = memref.reinterpret_cast %alloc_701 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_726 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_725 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_726 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_727 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_728 = memref.reinterpret_cast %alloc_727 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_726, %reinterpret_cast_728 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_729 = memref.reinterpret_cast %alloc_727 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_724, %reinterpret_cast_729 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_730 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_727, %reinterpret_cast_722 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_730 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_731 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_723, %alloc_730 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_731 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_732 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_711, %reinterpret_cast_721 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_732 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_733 = memref.reinterpret_cast %alloc_710 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_734 = memref.reinterpret_cast %alloc_710 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_735 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_734 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_735 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_736 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_737 = memref.reinterpret_cast %alloc_736 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_735, %reinterpret_cast_737 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_738 = memref.reinterpret_cast %alloc_736 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_733, %reinterpret_cast_738 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_739 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_736, %reinterpret_cast_722 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_739 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_740 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_732, %alloc_739 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_740 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_741 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg96, %alloc_741 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_742, %offset_743, %sizes_744, %strides_745 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_746 = memref.reinterpret_cast %base_buffer_742 to offset: [%offset_743], sizes: [1, 1, 1, 1], strides: [%strides_745, %strides_745, %strides_745, %strides_745] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_747 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_746, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_747 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_740[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_747[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_741[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_748 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg97, %alloc_748 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_749, %offset_750, %sizes_751, %strides_752 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_753 = memref.reinterpret_cast %base_buffer_749 to offset: [%offset_750], sizes: [1, 1, 1, 1], strides: [%strides_752, %strides_752, %strides_752, %strides_752] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_754 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_753, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_754 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_720[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_754[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_748[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_755 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_756 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_755, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_756 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_757 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_758 = memref.alloc() : memref<1x12x1xf32>
    %alloc_759 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_759[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_731[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_741[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_756[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_748[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_759[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_759[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_758[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_759[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_757[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_760 = memref.reinterpret_cast %alloc_757 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_761 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_761 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_760, %arg98 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_761 : memref<1x1536xf32>)
    %reinterpret_cast_762 = memref.reinterpret_cast %alloc_761 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_763 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_680, %reinterpret_cast_762 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_763 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_764 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_763 : memref<1x1x1536xf32>) outs(%alloc_764 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_765 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_765 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_764 : memref<1x1x1536xf32>) outs(%alloc_765 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_766 = memref.reinterpret_cast %alloc_765 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_767 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_766, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_767 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_768 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_767, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_768 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_769 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_768 : memref<1x1x1xf32>) outs(%alloc_769 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_770 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_763, %alloc_769 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_770 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_771, %offset_772, %sizes_773, %strides_774 = memref.extract_strided_metadata %arg99 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %60 = affine.apply #map10()[%strides_774]
    %61 = affine.apply #map10()[%strides_774]
    %reinterpret_cast_775 = memref.reinterpret_cast %base_buffer_771 to offset: [%offset_772], sizes: [1, 1, 1536], strides: [%60, %61, %strides_774] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_776 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_775, %alloc_770 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_776 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_777 = memref.reinterpret_cast %alloc_776 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_778 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_778 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_777, %arg100 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_778 : memref<1x8960xf32>)
    %reinterpret_cast_779 = memref.reinterpret_cast %alloc_778 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_780 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_779 : memref<1x1x8960xf32>) outs(%alloc_780 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_781 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_779, %alloc_780 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_781 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_782 = memref.reinterpret_cast %alloc_776 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_783 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_783 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_782, %arg101 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_783 : memref<1x8960xf32>)
    %reinterpret_cast_784 = memref.reinterpret_cast %alloc_783 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_785 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_781, %reinterpret_cast_784 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_785 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_786 = memref.reinterpret_cast %alloc_785 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_787 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_787 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_786, %arg102 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_787 : memref<1x1536xf32>)
    %reinterpret_cast_788 = memref.reinterpret_cast %alloc_787 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_789 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_763, %reinterpret_cast_788 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_789 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_790 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_789 : memref<1x1x1536xf32>) outs(%alloc_790 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_791 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_791 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_790 : memref<1x1x1536xf32>) outs(%alloc_791 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_792 = memref.reinterpret_cast %alloc_791 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_793 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_792, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_793 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_794 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_793, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_794 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_795 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_794 : memref<1x1x1xf32>) outs(%alloc_795 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_796 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_789, %alloc_795 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_796 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_797, %offset_798, %sizes_799, %strides_800 = memref.extract_strided_metadata %arg103 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %62 = affine.apply #map10()[%strides_800]
    %63 = affine.apply #map10()[%strides_800]
    %reinterpret_cast_801 = memref.reinterpret_cast %base_buffer_797 to offset: [%offset_798], sizes: [1, 1, 1536], strides: [%62, %63, %strides_800] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_802 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_801, %alloc_796 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_802 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_803 = memref.reinterpret_cast %alloc_802 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_804 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_804 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_803, %arg105 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_804 : memref<1x1536xf32>)
    %base_buffer_805, %offset_806, %sizes_807, %strides_808 = memref.extract_strided_metadata %arg104 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %64 = affine.apply #map10()[%strides_808]
    %reinterpret_cast_809 = memref.reinterpret_cast %base_buffer_805 to offset: [%offset_806], sizes: [1, 1536], strides: [%64, %strides_808] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_810 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_809, %alloc_804 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_810 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_811 = memref.reinterpret_cast %alloc_810 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_812 = memref.reinterpret_cast %alloc_802 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_813 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_813 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_812, %arg107 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_813 : memref<1x256xf32>)
    %base_buffer_814, %offset_815, %sizes_816, %strides_817 = memref.extract_strided_metadata %arg106 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %65 = affine.apply #map11()[%strides_817]
    %reinterpret_cast_818 = memref.reinterpret_cast %base_buffer_814 to offset: [%offset_815], sizes: [1, 256], strides: [%65, %strides_817] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_819 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_818, %alloc_813 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_819 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_820 = memref.reinterpret_cast %alloc_819 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_821 = memref.reinterpret_cast %alloc_802 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_822 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_822 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_821, %arg109 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_822 : memref<1x256xf32>)
    %base_buffer_823, %offset_824, %sizes_825, %strides_826 = memref.extract_strided_metadata %arg108 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %66 = affine.apply #map11()[%strides_826]
    %reinterpret_cast_827 = memref.reinterpret_cast %base_buffer_823 to offset: [%offset_824], sizes: [1, 256], strides: [%66, %strides_826] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_828 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_827, %alloc_822 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_828 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_829 = memref.reinterpret_cast %alloc_828 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_830 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_831 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_832 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_811, %reinterpret_cast_830 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_832 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_833 = memref.reinterpret_cast %alloc_810 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_834 = memref.reinterpret_cast %alloc_810 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_835 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_834 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_835 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_836 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_837 = memref.reinterpret_cast %alloc_836 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_835, %reinterpret_cast_837 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_838 = memref.reinterpret_cast %alloc_836 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_833, %reinterpret_cast_838 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_839 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_836, %reinterpret_cast_831 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_839 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_840 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_832, %alloc_839 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_840 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_841 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_820, %reinterpret_cast_830 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_841 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_842 = memref.reinterpret_cast %alloc_819 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_843 = memref.reinterpret_cast %alloc_819 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_844 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_843 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_844 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_845 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_846 = memref.reinterpret_cast %alloc_845 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_844, %reinterpret_cast_846 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_847 = memref.reinterpret_cast %alloc_845 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_842, %reinterpret_cast_847 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_848 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_845, %reinterpret_cast_831 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_848 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_849 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_841, %alloc_848 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_849 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_850 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg110, %alloc_850 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_851, %offset_852, %sizes_853, %strides_854 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_855 = memref.reinterpret_cast %base_buffer_851 to offset: [%offset_852], sizes: [1, 1, 1, 1], strides: [%strides_854, %strides_854, %strides_854, %strides_854] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_856 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_855, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_856 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_849[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_856[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_850[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_857 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg111, %alloc_857 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_858, %offset_859, %sizes_860, %strides_861 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_862 = memref.reinterpret_cast %base_buffer_858 to offset: [%offset_859], sizes: [1, 1, 1, 1], strides: [%strides_861, %strides_861, %strides_861, %strides_861] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_863 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_862, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_863 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_829[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_863[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_857[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_864 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_865 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_864, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_865 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_866 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_867 = memref.alloc() : memref<1x12x1xf32>
    %alloc_868 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_868[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_840[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_850[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_865[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_857[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_868[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_868[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_867[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_868[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_866[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_869 = memref.reinterpret_cast %alloc_866 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_870 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_870 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_869, %arg112 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_870 : memref<1x1536xf32>)
    %reinterpret_cast_871 = memref.reinterpret_cast %alloc_870 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_872 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_789, %reinterpret_cast_871 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_872 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_873 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_872 : memref<1x1x1536xf32>) outs(%alloc_873 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_874 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_874 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_873 : memref<1x1x1536xf32>) outs(%alloc_874 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_875 = memref.reinterpret_cast %alloc_874 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_876 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_875, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_876 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_877 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_876, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_877 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_878 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_877 : memref<1x1x1xf32>) outs(%alloc_878 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_879 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_872, %alloc_878 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_879 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_880, %offset_881, %sizes_882, %strides_883 = memref.extract_strided_metadata %arg113 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %67 = affine.apply #map10()[%strides_883]
    %68 = affine.apply #map10()[%strides_883]
    %reinterpret_cast_884 = memref.reinterpret_cast %base_buffer_880 to offset: [%offset_881], sizes: [1, 1, 1536], strides: [%67, %68, %strides_883] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_885 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_884, %alloc_879 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_885 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_886 = memref.reinterpret_cast %alloc_885 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_887 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_887 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_886, %arg114 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_887 : memref<1x8960xf32>)
    %reinterpret_cast_888 = memref.reinterpret_cast %alloc_887 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_889 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_888 : memref<1x1x8960xf32>) outs(%alloc_889 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_890 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_888, %alloc_889 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_890 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_891 = memref.reinterpret_cast %alloc_885 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_892 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_892 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_891, %arg115 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_892 : memref<1x8960xf32>)
    %reinterpret_cast_893 = memref.reinterpret_cast %alloc_892 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_894 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_890, %reinterpret_cast_893 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_894 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_895 = memref.reinterpret_cast %alloc_894 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_896 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_896 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_895, %arg116 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_896 : memref<1x1536xf32>)
    %reinterpret_cast_897 = memref.reinterpret_cast %alloc_896 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_898 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_872, %reinterpret_cast_897 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_898 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_899 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_898 : memref<1x1x1536xf32>) outs(%alloc_899 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_900 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_900 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_899 : memref<1x1x1536xf32>) outs(%alloc_900 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_901 = memref.reinterpret_cast %alloc_900 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_902 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_901, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_902 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_903 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_902, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_903 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_904 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_903 : memref<1x1x1xf32>) outs(%alloc_904 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_905 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_898, %alloc_904 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_905 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_906, %offset_907, %sizes_908, %strides_909 = memref.extract_strided_metadata %arg117 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %69 = affine.apply #map10()[%strides_909]
    %70 = affine.apply #map10()[%strides_909]
    %reinterpret_cast_910 = memref.reinterpret_cast %base_buffer_906 to offset: [%offset_907], sizes: [1, 1, 1536], strides: [%69, %70, %strides_909] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_911 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_910, %alloc_905 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_911 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_912 = memref.reinterpret_cast %alloc_911 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_913 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_913 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_912, %arg119 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_913 : memref<1x1536xf32>)
    %base_buffer_914, %offset_915, %sizes_916, %strides_917 = memref.extract_strided_metadata %arg118 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %71 = affine.apply #map10()[%strides_917]
    %reinterpret_cast_918 = memref.reinterpret_cast %base_buffer_914 to offset: [%offset_915], sizes: [1, 1536], strides: [%71, %strides_917] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_919 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_918, %alloc_913 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_919 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_920 = memref.reinterpret_cast %alloc_919 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_921 = memref.reinterpret_cast %alloc_911 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_922 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_922 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_921, %arg121 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_922 : memref<1x256xf32>)
    %base_buffer_923, %offset_924, %sizes_925, %strides_926 = memref.extract_strided_metadata %arg120 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %72 = affine.apply #map11()[%strides_926]
    %reinterpret_cast_927 = memref.reinterpret_cast %base_buffer_923 to offset: [%offset_924], sizes: [1, 256], strides: [%72, %strides_926] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_928 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_927, %alloc_922 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_928 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_929 = memref.reinterpret_cast %alloc_928 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_930 = memref.reinterpret_cast %alloc_911 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_931 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_931 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_930, %arg123 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_931 : memref<1x256xf32>)
    %base_buffer_932, %offset_933, %sizes_934, %strides_935 = memref.extract_strided_metadata %arg122 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %73 = affine.apply #map11()[%strides_935]
    %reinterpret_cast_936 = memref.reinterpret_cast %base_buffer_932 to offset: [%offset_933], sizes: [1, 256], strides: [%73, %strides_935] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_937 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_936, %alloc_931 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_937 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_938 = memref.reinterpret_cast %alloc_937 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_939 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_940 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_941 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_920, %reinterpret_cast_939 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_941 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_942 = memref.reinterpret_cast %alloc_919 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_943 = memref.reinterpret_cast %alloc_919 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_944 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_943 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_944 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_945 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_946 = memref.reinterpret_cast %alloc_945 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_944, %reinterpret_cast_946 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_947 = memref.reinterpret_cast %alloc_945 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_942, %reinterpret_cast_947 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_948 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_945, %reinterpret_cast_940 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_948 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_949 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_941, %alloc_948 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_949 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_950 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_929, %reinterpret_cast_939 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_950 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_951 = memref.reinterpret_cast %alloc_928 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_952 = memref.reinterpret_cast %alloc_928 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_953 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_952 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_953 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_954 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_955 = memref.reinterpret_cast %alloc_954 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_953, %reinterpret_cast_955 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_956 = memref.reinterpret_cast %alloc_954 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_951, %reinterpret_cast_956 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_957 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_954, %reinterpret_cast_940 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_957 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_958 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_950, %alloc_957 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_958 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_959 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg124, %alloc_959 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_960, %offset_961, %sizes_962, %strides_963 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_964 = memref.reinterpret_cast %base_buffer_960 to offset: [%offset_961], sizes: [1, 1, 1, 1], strides: [%strides_963, %strides_963, %strides_963, %strides_963] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_965 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_964, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_965 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_958[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_965[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_959[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_966 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg125, %alloc_966 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_967, %offset_968, %sizes_969, %strides_970 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_971 = memref.reinterpret_cast %base_buffer_967 to offset: [%offset_968], sizes: [1, 1, 1, 1], strides: [%strides_970, %strides_970, %strides_970, %strides_970] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_972 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_971, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_972 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_938[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_972[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_966[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_973 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_974 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_973, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_974 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_975 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_976 = memref.alloc() : memref<1x12x1xf32>
    %alloc_977 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_977[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_949[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_959[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_974[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_966[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_977[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_977[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_976[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_977[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_975[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_978 = memref.reinterpret_cast %alloc_975 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_979 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_979 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_978, %arg126 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_979 : memref<1x1536xf32>)
    %reinterpret_cast_980 = memref.reinterpret_cast %alloc_979 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_981 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_898, %reinterpret_cast_980 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_981 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_982 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_981 : memref<1x1x1536xf32>) outs(%alloc_982 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_983 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_983 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_982 : memref<1x1x1536xf32>) outs(%alloc_983 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_984 = memref.reinterpret_cast %alloc_983 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_985 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_984, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_985 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_986 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_985, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_986 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_987 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_986 : memref<1x1x1xf32>) outs(%alloc_987 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_988 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_981, %alloc_987 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_988 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_989, %offset_990, %sizes_991, %strides_992 = memref.extract_strided_metadata %arg127 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %74 = affine.apply #map10()[%strides_992]
    %75 = affine.apply #map10()[%strides_992]
    %reinterpret_cast_993 = memref.reinterpret_cast %base_buffer_989 to offset: [%offset_990], sizes: [1, 1, 1536], strides: [%74, %75, %strides_992] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_994 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_993, %alloc_988 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_994 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_995 = memref.reinterpret_cast %alloc_994 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_996 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_996 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_995, %arg128 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_996 : memref<1x8960xf32>)
    %reinterpret_cast_997 = memref.reinterpret_cast %alloc_996 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_998 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_997 : memref<1x1x8960xf32>) outs(%alloc_998 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_999 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_997, %alloc_998 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_999 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1000 = memref.reinterpret_cast %alloc_994 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1001 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1001 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1000, %arg129 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1001 : memref<1x8960xf32>)
    %reinterpret_cast_1002 = memref.reinterpret_cast %alloc_1001 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1003 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_999, %reinterpret_cast_1002 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1003 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1004 = memref.reinterpret_cast %alloc_1003 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1005 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1005 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1004, %arg130 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1005 : memref<1x1536xf32>)
    %reinterpret_cast_1006 = memref.reinterpret_cast %alloc_1005 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1007 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_981, %reinterpret_cast_1006 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1007 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1008 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1007 : memref<1x1x1536xf32>) outs(%alloc_1008 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1009 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1009 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1008 : memref<1x1x1536xf32>) outs(%alloc_1009 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1010 = memref.reinterpret_cast %alloc_1009 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1011 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1010, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1011 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1012 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1011, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1012 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1013 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1012 : memref<1x1x1xf32>) outs(%alloc_1013 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1014 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1007, %alloc_1013 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1014 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1015, %offset_1016, %sizes_1017, %strides_1018 = memref.extract_strided_metadata %arg131 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %76 = affine.apply #map10()[%strides_1018]
    %77 = affine.apply #map10()[%strides_1018]
    %reinterpret_cast_1019 = memref.reinterpret_cast %base_buffer_1015 to offset: [%offset_1016], sizes: [1, 1, 1536], strides: [%76, %77, %strides_1018] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1020 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1019, %alloc_1014 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1020 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1021 = memref.reinterpret_cast %alloc_1020 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1022 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1022 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1021, %arg133 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1022 : memref<1x1536xf32>)
    %base_buffer_1023, %offset_1024, %sizes_1025, %strides_1026 = memref.extract_strided_metadata %arg132 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %78 = affine.apply #map10()[%strides_1026]
    %reinterpret_cast_1027 = memref.reinterpret_cast %base_buffer_1023 to offset: [%offset_1024], sizes: [1, 1536], strides: [%78, %strides_1026] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_1028 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1027, %alloc_1022 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_1028 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1029 = memref.reinterpret_cast %alloc_1028 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_1030 = memref.reinterpret_cast %alloc_1020 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1031 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1031 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1030, %arg135 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1031 : memref<1x256xf32>)
    %base_buffer_1032, %offset_1033, %sizes_1034, %strides_1035 = memref.extract_strided_metadata %arg134 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %79 = affine.apply #map11()[%strides_1035]
    %reinterpret_cast_1036 = memref.reinterpret_cast %base_buffer_1032 to offset: [%offset_1033], sizes: [1, 256], strides: [%79, %strides_1035] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1037 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1036, %alloc_1031 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1037 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1038 = memref.reinterpret_cast %alloc_1037 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1039 = memref.reinterpret_cast %alloc_1020 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1040 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1040 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1039, %arg137 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1040 : memref<1x256xf32>)
    %base_buffer_1041, %offset_1042, %sizes_1043, %strides_1044 = memref.extract_strided_metadata %arg136 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %80 = affine.apply #map11()[%strides_1044]
    %reinterpret_cast_1045 = memref.reinterpret_cast %base_buffer_1041 to offset: [%offset_1042], sizes: [1, 256], strides: [%80, %strides_1044] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1046 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1045, %alloc_1040 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1046 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1047 = memref.reinterpret_cast %alloc_1046 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1048 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_1049 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_1050 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1029, %reinterpret_cast_1048 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1050 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1051 = memref.reinterpret_cast %alloc_1028 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1052 = memref.reinterpret_cast %alloc_1028 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1053 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1052 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_1053 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1054 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_1055 = memref.reinterpret_cast %alloc_1054 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_1053, %reinterpret_cast_1055 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1056 = memref.reinterpret_cast %alloc_1054 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1051, %reinterpret_cast_1056 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1057 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1054, %reinterpret_cast_1049 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1057 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1058 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1050, %alloc_1057 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_1058 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1059 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1038, %reinterpret_cast_1048 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1059 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1060 = memref.reinterpret_cast %alloc_1037 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1061 = memref.reinterpret_cast %alloc_1037 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1062 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1061 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_1062 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1063 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_1064 = memref.reinterpret_cast %alloc_1063 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_1062, %reinterpret_cast_1064 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1065 = memref.reinterpret_cast %alloc_1063 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1060, %reinterpret_cast_1065 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1066 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1063, %reinterpret_cast_1049 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1066 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1067 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1059, %alloc_1066 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_1067 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1068 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg138, %alloc_1068 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1069, %offset_1070, %sizes_1071, %strides_1072 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1073 = memref.reinterpret_cast %base_buffer_1069 to offset: [%offset_1070], sizes: [1, 1, 1, 1], strides: [%strides_1072, %strides_1072, %strides_1072, %strides_1072] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1074 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1073, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1074 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_1067[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1074[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1068[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_1075 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg139, %alloc_1075 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1076, %offset_1077, %sizes_1078, %strides_1079 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1080 = memref.reinterpret_cast %base_buffer_1076 to offset: [%offset_1077], sizes: [1, 1, 1, 1], strides: [%strides_1079, %strides_1079, %strides_1079, %strides_1079] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1081 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1080, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1081 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_1047[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1081[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1075[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1082 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_1083 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1082, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_1083 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_1084 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_1085 = memref.alloc() : memref<1x12x1xf32>
    %alloc_1086 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_1086[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_1058[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_1068[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_1083[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_1075[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_1086[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_1086[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_1085[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_1086[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_1084[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1087 = memref.reinterpret_cast %alloc_1084 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_1088 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1088 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1087, %arg140 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1088 : memref<1x1536xf32>)
    %reinterpret_cast_1089 = memref.reinterpret_cast %alloc_1088 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1090 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1007, %reinterpret_cast_1089 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1090 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1091 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1090 : memref<1x1x1536xf32>) outs(%alloc_1091 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1092 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1092 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1091 : memref<1x1x1536xf32>) outs(%alloc_1092 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1093 = memref.reinterpret_cast %alloc_1092 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1094 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1093, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1094 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1095 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1094, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1095 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1096 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1095 : memref<1x1x1xf32>) outs(%alloc_1096 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1097 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1090, %alloc_1096 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1097 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1098, %offset_1099, %sizes_1100, %strides_1101 = memref.extract_strided_metadata %arg141 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %81 = affine.apply #map10()[%strides_1101]
    %82 = affine.apply #map10()[%strides_1101]
    %reinterpret_cast_1102 = memref.reinterpret_cast %base_buffer_1098 to offset: [%offset_1099], sizes: [1, 1, 1536], strides: [%81, %82, %strides_1101] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1103 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1102, %alloc_1097 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1103 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1104 = memref.reinterpret_cast %alloc_1103 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1105 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1105 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1104, %arg142 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1105 : memref<1x8960xf32>)
    %reinterpret_cast_1106 = memref.reinterpret_cast %alloc_1105 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1107 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1106 : memref<1x1x8960xf32>) outs(%alloc_1107 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_1108 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1106, %alloc_1107 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1108 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1109 = memref.reinterpret_cast %alloc_1103 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1110 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1110 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1109, %arg143 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1110 : memref<1x8960xf32>)
    %reinterpret_cast_1111 = memref.reinterpret_cast %alloc_1110 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1112 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1108, %reinterpret_cast_1111 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1112 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1113 = memref.reinterpret_cast %alloc_1112 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1114 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1114 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1113, %arg144 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1114 : memref<1x1536xf32>)
    %reinterpret_cast_1115 = memref.reinterpret_cast %alloc_1114 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1116 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1090, %reinterpret_cast_1115 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1116 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1117 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1116 : memref<1x1x1536xf32>) outs(%alloc_1117 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1118 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1118 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1117 : memref<1x1x1536xf32>) outs(%alloc_1118 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1119 = memref.reinterpret_cast %alloc_1118 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1120 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1119, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1120 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1121 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1120, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1121 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1122 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1121 : memref<1x1x1xf32>) outs(%alloc_1122 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1123 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1116, %alloc_1122 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1123 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1124, %offset_1125, %sizes_1126, %strides_1127 = memref.extract_strided_metadata %arg145 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %83 = affine.apply #map10()[%strides_1127]
    %84 = affine.apply #map10()[%strides_1127]
    %reinterpret_cast_1128 = memref.reinterpret_cast %base_buffer_1124 to offset: [%offset_1125], sizes: [1, 1, 1536], strides: [%83, %84, %strides_1127] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1129 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1128, %alloc_1123 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1129 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1130 = memref.reinterpret_cast %alloc_1129 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1131 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1131 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1130, %arg147 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1131 : memref<1x1536xf32>)
    %base_buffer_1132, %offset_1133, %sizes_1134, %strides_1135 = memref.extract_strided_metadata %arg146 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %85 = affine.apply #map10()[%strides_1135]
    %reinterpret_cast_1136 = memref.reinterpret_cast %base_buffer_1132 to offset: [%offset_1133], sizes: [1, 1536], strides: [%85, %strides_1135] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_1137 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1136, %alloc_1131 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_1137 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1138 = memref.reinterpret_cast %alloc_1137 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_1139 = memref.reinterpret_cast %alloc_1129 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1140 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1140 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1139, %arg149 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1140 : memref<1x256xf32>)
    %base_buffer_1141, %offset_1142, %sizes_1143, %strides_1144 = memref.extract_strided_metadata %arg148 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %86 = affine.apply #map11()[%strides_1144]
    %reinterpret_cast_1145 = memref.reinterpret_cast %base_buffer_1141 to offset: [%offset_1142], sizes: [1, 256], strides: [%86, %strides_1144] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1146 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1145, %alloc_1140 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1146 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1147 = memref.reinterpret_cast %alloc_1146 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1148 = memref.reinterpret_cast %alloc_1129 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1149 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1149 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1148, %arg151 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1149 : memref<1x256xf32>)
    %base_buffer_1150, %offset_1151, %sizes_1152, %strides_1153 = memref.extract_strided_metadata %arg150 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %87 = affine.apply #map11()[%strides_1153]
    %reinterpret_cast_1154 = memref.reinterpret_cast %base_buffer_1150 to offset: [%offset_1151], sizes: [1, 256], strides: [%87, %strides_1153] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1155 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1154, %alloc_1149 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1155 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1156 = memref.reinterpret_cast %alloc_1155 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1157 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_1158 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_1159 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1138, %reinterpret_cast_1157 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1159 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1160 = memref.reinterpret_cast %alloc_1137 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1161 = memref.reinterpret_cast %alloc_1137 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1162 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1161 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_1162 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1163 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_1164 = memref.reinterpret_cast %alloc_1163 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_1162, %reinterpret_cast_1164 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1165 = memref.reinterpret_cast %alloc_1163 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1160, %reinterpret_cast_1165 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1166 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1163, %reinterpret_cast_1158 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1166 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1167 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1159, %alloc_1166 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_1167 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1168 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1147, %reinterpret_cast_1157 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1168 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1169 = memref.reinterpret_cast %alloc_1146 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1170 = memref.reinterpret_cast %alloc_1146 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1171 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1170 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_1171 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1172 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_1173 = memref.reinterpret_cast %alloc_1172 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_1171, %reinterpret_cast_1173 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1174 = memref.reinterpret_cast %alloc_1172 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1169, %reinterpret_cast_1174 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1175 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1172, %reinterpret_cast_1158 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1175 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1176 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1168, %alloc_1175 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_1176 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1177 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg152, %alloc_1177 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1178, %offset_1179, %sizes_1180, %strides_1181 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1182 = memref.reinterpret_cast %base_buffer_1178 to offset: [%offset_1179], sizes: [1, 1, 1, 1], strides: [%strides_1181, %strides_1181, %strides_1181, %strides_1181] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1183 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1182, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1183 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_1176[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1183[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1177[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_1184 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg153, %alloc_1184 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1185, %offset_1186, %sizes_1187, %strides_1188 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1189 = memref.reinterpret_cast %base_buffer_1185 to offset: [%offset_1186], sizes: [1, 1, 1, 1], strides: [%strides_1188, %strides_1188, %strides_1188, %strides_1188] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1190 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1189, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1190 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_1156[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1190[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1184[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1191 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_1192 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1191, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_1192 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_1193 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_1194 = memref.alloc() : memref<1x12x1xf32>
    %alloc_1195 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_1195[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_1167[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_1177[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_1192[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_1184[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_1195[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_1195[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_1194[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_1195[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_1193[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1196 = memref.reinterpret_cast %alloc_1193 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_1197 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1197 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1196, %arg154 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1197 : memref<1x1536xf32>)
    %reinterpret_cast_1198 = memref.reinterpret_cast %alloc_1197 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1199 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1116, %reinterpret_cast_1198 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1199 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1200 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1199 : memref<1x1x1536xf32>) outs(%alloc_1200 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1201 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1201 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1200 : memref<1x1x1536xf32>) outs(%alloc_1201 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1202 = memref.reinterpret_cast %alloc_1201 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1203 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1202, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1203 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1204 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1203, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1204 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1205 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1204 : memref<1x1x1xf32>) outs(%alloc_1205 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1206 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1199, %alloc_1205 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1206 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1207, %offset_1208, %sizes_1209, %strides_1210 = memref.extract_strided_metadata %arg155 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %88 = affine.apply #map10()[%strides_1210]
    %89 = affine.apply #map10()[%strides_1210]
    %reinterpret_cast_1211 = memref.reinterpret_cast %base_buffer_1207 to offset: [%offset_1208], sizes: [1, 1, 1536], strides: [%88, %89, %strides_1210] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1212 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1211, %alloc_1206 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1212 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1213 = memref.reinterpret_cast %alloc_1212 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1214 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1214 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1213, %arg156 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1214 : memref<1x8960xf32>)
    %reinterpret_cast_1215 = memref.reinterpret_cast %alloc_1214 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1216 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1215 : memref<1x1x8960xf32>) outs(%alloc_1216 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_1217 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1215, %alloc_1216 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1217 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1218 = memref.reinterpret_cast %alloc_1212 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1219 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1219 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1218, %arg157 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1219 : memref<1x8960xf32>)
    %reinterpret_cast_1220 = memref.reinterpret_cast %alloc_1219 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1221 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1217, %reinterpret_cast_1220 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1221 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1222 = memref.reinterpret_cast %alloc_1221 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1223 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1223 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1222, %arg158 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1223 : memref<1x1536xf32>)
    %reinterpret_cast_1224 = memref.reinterpret_cast %alloc_1223 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1225 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1199, %reinterpret_cast_1224 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1225 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1226 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1225 : memref<1x1x1536xf32>) outs(%alloc_1226 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1227 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1227 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1226 : memref<1x1x1536xf32>) outs(%alloc_1227 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1228 = memref.reinterpret_cast %alloc_1227 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1229 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1228, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1229 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1230 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1229, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1230 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1231 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1230 : memref<1x1x1xf32>) outs(%alloc_1231 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1232 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1225, %alloc_1231 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1232 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1233, %offset_1234, %sizes_1235, %strides_1236 = memref.extract_strided_metadata %arg159 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %90 = affine.apply #map10()[%strides_1236]
    %91 = affine.apply #map10()[%strides_1236]
    %reinterpret_cast_1237 = memref.reinterpret_cast %base_buffer_1233 to offset: [%offset_1234], sizes: [1, 1, 1536], strides: [%90, %91, %strides_1236] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1238 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1237, %alloc_1232 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1238 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1239 = memref.reinterpret_cast %alloc_1238 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1240 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1240 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1239, %arg161 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1240 : memref<1x1536xf32>)
    %base_buffer_1241, %offset_1242, %sizes_1243, %strides_1244 = memref.extract_strided_metadata %arg160 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %92 = affine.apply #map10()[%strides_1244]
    %reinterpret_cast_1245 = memref.reinterpret_cast %base_buffer_1241 to offset: [%offset_1242], sizes: [1, 1536], strides: [%92, %strides_1244] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_1246 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1245, %alloc_1240 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_1246 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1247 = memref.reinterpret_cast %alloc_1246 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_1248 = memref.reinterpret_cast %alloc_1238 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1249 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1249 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1248, %arg163 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1249 : memref<1x256xf32>)
    %base_buffer_1250, %offset_1251, %sizes_1252, %strides_1253 = memref.extract_strided_metadata %arg162 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %93 = affine.apply #map11()[%strides_1253]
    %reinterpret_cast_1254 = memref.reinterpret_cast %base_buffer_1250 to offset: [%offset_1251], sizes: [1, 256], strides: [%93, %strides_1253] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1255 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1254, %alloc_1249 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1255 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1256 = memref.reinterpret_cast %alloc_1255 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1257 = memref.reinterpret_cast %alloc_1238 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1258 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1258 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1257, %arg165 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1258 : memref<1x256xf32>)
    %base_buffer_1259, %offset_1260, %sizes_1261, %strides_1262 = memref.extract_strided_metadata %arg164 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %94 = affine.apply #map11()[%strides_1262]
    %reinterpret_cast_1263 = memref.reinterpret_cast %base_buffer_1259 to offset: [%offset_1260], sizes: [1, 256], strides: [%94, %strides_1262] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1264 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1263, %alloc_1258 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1264 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1265 = memref.reinterpret_cast %alloc_1264 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1266 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_1267 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_1268 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1247, %reinterpret_cast_1266 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1268 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1269 = memref.reinterpret_cast %alloc_1246 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1270 = memref.reinterpret_cast %alloc_1246 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1271 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1270 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_1271 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1272 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_1273 = memref.reinterpret_cast %alloc_1272 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_1271, %reinterpret_cast_1273 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1274 = memref.reinterpret_cast %alloc_1272 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1269, %reinterpret_cast_1274 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1275 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1272, %reinterpret_cast_1267 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1275 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1276 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1268, %alloc_1275 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_1276 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1277 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1256, %reinterpret_cast_1266 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1277 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1278 = memref.reinterpret_cast %alloc_1255 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1279 = memref.reinterpret_cast %alloc_1255 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1280 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1279 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_1280 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1281 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_1282 = memref.reinterpret_cast %alloc_1281 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_1280, %reinterpret_cast_1282 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1283 = memref.reinterpret_cast %alloc_1281 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1278, %reinterpret_cast_1283 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1284 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1281, %reinterpret_cast_1267 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1284 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1285 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1277, %alloc_1284 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_1285 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1286 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg166, %alloc_1286 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1287, %offset_1288, %sizes_1289, %strides_1290 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1291 = memref.reinterpret_cast %base_buffer_1287 to offset: [%offset_1288], sizes: [1, 1, 1, 1], strides: [%strides_1290, %strides_1290, %strides_1290, %strides_1290] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1292 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1291, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1292 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_1285[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1292[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1286[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_1293 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg167, %alloc_1293 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1294, %offset_1295, %sizes_1296, %strides_1297 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1298 = memref.reinterpret_cast %base_buffer_1294 to offset: [%offset_1295], sizes: [1, 1, 1, 1], strides: [%strides_1297, %strides_1297, %strides_1297, %strides_1297] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1299 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1298, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1299 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_1265[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1299[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1293[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1300 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_1301 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1300, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_1301 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_1302 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_1303 = memref.alloc() : memref<1x12x1xf32>
    %alloc_1304 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_1304[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_1276[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_1286[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_1301[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_1293[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_1304[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_1304[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_1303[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_1304[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_1302[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1305 = memref.reinterpret_cast %alloc_1302 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_1306 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1306 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1305, %arg168 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1306 : memref<1x1536xf32>)
    %reinterpret_cast_1307 = memref.reinterpret_cast %alloc_1306 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1308 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1225, %reinterpret_cast_1307 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1308 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1309 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1308 : memref<1x1x1536xf32>) outs(%alloc_1309 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1310 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1310 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1309 : memref<1x1x1536xf32>) outs(%alloc_1310 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1311 = memref.reinterpret_cast %alloc_1310 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1312 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1311, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1312 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1313 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1312, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1313 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1314 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1313 : memref<1x1x1xf32>) outs(%alloc_1314 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1315 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1308, %alloc_1314 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1315 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1316, %offset_1317, %sizes_1318, %strides_1319 = memref.extract_strided_metadata %arg169 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %95 = affine.apply #map10()[%strides_1319]
    %96 = affine.apply #map10()[%strides_1319]
    %reinterpret_cast_1320 = memref.reinterpret_cast %base_buffer_1316 to offset: [%offset_1317], sizes: [1, 1, 1536], strides: [%95, %96, %strides_1319] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1321 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1320, %alloc_1315 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1321 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1322 = memref.reinterpret_cast %alloc_1321 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1323 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1323 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1322, %arg170 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1323 : memref<1x8960xf32>)
    %reinterpret_cast_1324 = memref.reinterpret_cast %alloc_1323 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1325 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1324 : memref<1x1x8960xf32>) outs(%alloc_1325 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_1326 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1324, %alloc_1325 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1326 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1327 = memref.reinterpret_cast %alloc_1321 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1328 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1328 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1327, %arg171 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1328 : memref<1x8960xf32>)
    %reinterpret_cast_1329 = memref.reinterpret_cast %alloc_1328 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1330 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1326, %reinterpret_cast_1329 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1330 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1331 = memref.reinterpret_cast %alloc_1330 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1332 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1332 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1331, %arg172 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1332 : memref<1x1536xf32>)
    %reinterpret_cast_1333 = memref.reinterpret_cast %alloc_1332 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1334 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1308, %reinterpret_cast_1333 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1334 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1335 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1334 : memref<1x1x1536xf32>) outs(%alloc_1335 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1336 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1336 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1335 : memref<1x1x1536xf32>) outs(%alloc_1336 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1337 = memref.reinterpret_cast %alloc_1336 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1338 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1337, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1338 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1339 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1338, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1339 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1340 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1339 : memref<1x1x1xf32>) outs(%alloc_1340 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1341 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1334, %alloc_1340 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1341 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1342, %offset_1343, %sizes_1344, %strides_1345 = memref.extract_strided_metadata %arg173 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %97 = affine.apply #map10()[%strides_1345]
    %98 = affine.apply #map10()[%strides_1345]
    %reinterpret_cast_1346 = memref.reinterpret_cast %base_buffer_1342 to offset: [%offset_1343], sizes: [1, 1, 1536], strides: [%97, %98, %strides_1345] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1347 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1346, %alloc_1341 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1347 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1348 = memref.reinterpret_cast %alloc_1347 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1349 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1349 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1348, %arg175 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1349 : memref<1x1536xf32>)
    %base_buffer_1350, %offset_1351, %sizes_1352, %strides_1353 = memref.extract_strided_metadata %arg174 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %99 = affine.apply #map10()[%strides_1353]
    %reinterpret_cast_1354 = memref.reinterpret_cast %base_buffer_1350 to offset: [%offset_1351], sizes: [1, 1536], strides: [%99, %strides_1353] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_1355 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1354, %alloc_1349 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_1355 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1356 = memref.reinterpret_cast %alloc_1355 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_1357 = memref.reinterpret_cast %alloc_1347 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1358 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1358 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1357, %arg177 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1358 : memref<1x256xf32>)
    %base_buffer_1359, %offset_1360, %sizes_1361, %strides_1362 = memref.extract_strided_metadata %arg176 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %100 = affine.apply #map11()[%strides_1362]
    %reinterpret_cast_1363 = memref.reinterpret_cast %base_buffer_1359 to offset: [%offset_1360], sizes: [1, 256], strides: [%100, %strides_1362] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1364 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1363, %alloc_1358 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1364 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1365 = memref.reinterpret_cast %alloc_1364 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1366 = memref.reinterpret_cast %alloc_1347 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1367 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1367 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1366, %arg179 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1367 : memref<1x256xf32>)
    %base_buffer_1368, %offset_1369, %sizes_1370, %strides_1371 = memref.extract_strided_metadata %arg178 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %101 = affine.apply #map11()[%strides_1371]
    %reinterpret_cast_1372 = memref.reinterpret_cast %base_buffer_1368 to offset: [%offset_1369], sizes: [1, 256], strides: [%101, %strides_1371] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1373 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1372, %alloc_1367 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1373 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1374 = memref.reinterpret_cast %alloc_1373 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1375 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_1376 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_1377 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1356, %reinterpret_cast_1375 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1377 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1378 = memref.reinterpret_cast %alloc_1355 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1379 = memref.reinterpret_cast %alloc_1355 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1380 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1379 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_1380 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1381 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_1382 = memref.reinterpret_cast %alloc_1381 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_1380, %reinterpret_cast_1382 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1383 = memref.reinterpret_cast %alloc_1381 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1378, %reinterpret_cast_1383 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1384 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1381, %reinterpret_cast_1376 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1384 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1385 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1377, %alloc_1384 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_1385 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1386 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1365, %reinterpret_cast_1375 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1386 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1387 = memref.reinterpret_cast %alloc_1364 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1388 = memref.reinterpret_cast %alloc_1364 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1389 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1388 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_1389 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1390 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_1391 = memref.reinterpret_cast %alloc_1390 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_1389, %reinterpret_cast_1391 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1392 = memref.reinterpret_cast %alloc_1390 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1387, %reinterpret_cast_1392 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1393 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1390, %reinterpret_cast_1376 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1393 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1394 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1386, %alloc_1393 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_1394 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1395 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg180, %alloc_1395 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1396, %offset_1397, %sizes_1398, %strides_1399 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1400 = memref.reinterpret_cast %base_buffer_1396 to offset: [%offset_1397], sizes: [1, 1, 1, 1], strides: [%strides_1399, %strides_1399, %strides_1399, %strides_1399] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1401 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1400, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1401 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_1394[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1401[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1395[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_1402 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg181, %alloc_1402 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1403, %offset_1404, %sizes_1405, %strides_1406 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1407 = memref.reinterpret_cast %base_buffer_1403 to offset: [%offset_1404], sizes: [1, 1, 1, 1], strides: [%strides_1406, %strides_1406, %strides_1406, %strides_1406] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1408 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1407, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1408 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_1374[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1408[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1402[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1409 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_1410 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1409, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_1410 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_1411 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_1412 = memref.alloc() : memref<1x12x1xf32>
    %alloc_1413 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_1413[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_1385[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_1395[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_1410[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_1402[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_1413[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_1413[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_1412[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_1413[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_1411[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1414 = memref.reinterpret_cast %alloc_1411 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_1415 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1415 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1414, %arg182 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1415 : memref<1x1536xf32>)
    %reinterpret_cast_1416 = memref.reinterpret_cast %alloc_1415 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1417 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1334, %reinterpret_cast_1416 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1417 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1418 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1417 : memref<1x1x1536xf32>) outs(%alloc_1418 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1419 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1419 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1418 : memref<1x1x1536xf32>) outs(%alloc_1419 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1420 = memref.reinterpret_cast %alloc_1419 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1421 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1420, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1421 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1422 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1421, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1422 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1423 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1422 : memref<1x1x1xf32>) outs(%alloc_1423 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1424 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1417, %alloc_1423 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1424 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1425, %offset_1426, %sizes_1427, %strides_1428 = memref.extract_strided_metadata %arg183 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %102 = affine.apply #map10()[%strides_1428]
    %103 = affine.apply #map10()[%strides_1428]
    %reinterpret_cast_1429 = memref.reinterpret_cast %base_buffer_1425 to offset: [%offset_1426], sizes: [1, 1, 1536], strides: [%102, %103, %strides_1428] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1430 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1429, %alloc_1424 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1430 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1431 = memref.reinterpret_cast %alloc_1430 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1432 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1432 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1431, %arg184 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1432 : memref<1x8960xf32>)
    %reinterpret_cast_1433 = memref.reinterpret_cast %alloc_1432 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1434 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1433 : memref<1x1x8960xf32>) outs(%alloc_1434 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_1435 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1433, %alloc_1434 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1435 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1436 = memref.reinterpret_cast %alloc_1430 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1437 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1437 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1436, %arg185 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1437 : memref<1x8960xf32>)
    %reinterpret_cast_1438 = memref.reinterpret_cast %alloc_1437 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1439 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1435, %reinterpret_cast_1438 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1439 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1440 = memref.reinterpret_cast %alloc_1439 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1441 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1441 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1440, %arg186 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1441 : memref<1x1536xf32>)
    %reinterpret_cast_1442 = memref.reinterpret_cast %alloc_1441 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1443 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1417, %reinterpret_cast_1442 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1443 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1444 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1443 : memref<1x1x1536xf32>) outs(%alloc_1444 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1445 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1445 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1444 : memref<1x1x1536xf32>) outs(%alloc_1445 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1446 = memref.reinterpret_cast %alloc_1445 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1447 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1446, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1447 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1448 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1447, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1448 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1449 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1448 : memref<1x1x1xf32>) outs(%alloc_1449 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1450 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1443, %alloc_1449 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1450 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1451, %offset_1452, %sizes_1453, %strides_1454 = memref.extract_strided_metadata %arg187 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %104 = affine.apply #map10()[%strides_1454]
    %105 = affine.apply #map10()[%strides_1454]
    %reinterpret_cast_1455 = memref.reinterpret_cast %base_buffer_1451 to offset: [%offset_1452], sizes: [1, 1, 1536], strides: [%104, %105, %strides_1454] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1456 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1455, %alloc_1450 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1456 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1457 = memref.reinterpret_cast %alloc_1456 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1458 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1458 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1457, %arg189 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1458 : memref<1x1536xf32>)
    %base_buffer_1459, %offset_1460, %sizes_1461, %strides_1462 = memref.extract_strided_metadata %arg188 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %106 = affine.apply #map10()[%strides_1462]
    %reinterpret_cast_1463 = memref.reinterpret_cast %base_buffer_1459 to offset: [%offset_1460], sizes: [1, 1536], strides: [%106, %strides_1462] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_1464 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1463, %alloc_1458 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_1464 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1465 = memref.reinterpret_cast %alloc_1464 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_1466 = memref.reinterpret_cast %alloc_1456 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1467 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1467 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1466, %arg191 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1467 : memref<1x256xf32>)
    %base_buffer_1468, %offset_1469, %sizes_1470, %strides_1471 = memref.extract_strided_metadata %arg190 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %107 = affine.apply #map11()[%strides_1471]
    %reinterpret_cast_1472 = memref.reinterpret_cast %base_buffer_1468 to offset: [%offset_1469], sizes: [1, 256], strides: [%107, %strides_1471] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1473 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1472, %alloc_1467 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1473 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1474 = memref.reinterpret_cast %alloc_1473 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1475 = memref.reinterpret_cast %alloc_1456 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1476 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1476 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1475, %arg193 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1476 : memref<1x256xf32>)
    %base_buffer_1477, %offset_1478, %sizes_1479, %strides_1480 = memref.extract_strided_metadata %arg192 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %108 = affine.apply #map11()[%strides_1480]
    %reinterpret_cast_1481 = memref.reinterpret_cast %base_buffer_1477 to offset: [%offset_1478], sizes: [1, 256], strides: [%108, %strides_1480] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1482 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1481, %alloc_1476 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1482 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1483 = memref.reinterpret_cast %alloc_1482 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1484 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_1485 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_1486 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1465, %reinterpret_cast_1484 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1486 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1487 = memref.reinterpret_cast %alloc_1464 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1488 = memref.reinterpret_cast %alloc_1464 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1489 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1488 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_1489 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1490 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_1491 = memref.reinterpret_cast %alloc_1490 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_1489, %reinterpret_cast_1491 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1492 = memref.reinterpret_cast %alloc_1490 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1487, %reinterpret_cast_1492 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1493 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1490, %reinterpret_cast_1485 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1493 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1494 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1486, %alloc_1493 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_1494 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1495 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1474, %reinterpret_cast_1484 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1495 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1496 = memref.reinterpret_cast %alloc_1473 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1497 = memref.reinterpret_cast %alloc_1473 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1498 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1497 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_1498 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1499 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_1500 = memref.reinterpret_cast %alloc_1499 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_1498, %reinterpret_cast_1500 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1501 = memref.reinterpret_cast %alloc_1499 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1496, %reinterpret_cast_1501 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1502 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1499, %reinterpret_cast_1485 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1502 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1503 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1495, %alloc_1502 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_1503 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1504 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg194, %alloc_1504 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1505, %offset_1506, %sizes_1507, %strides_1508 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1509 = memref.reinterpret_cast %base_buffer_1505 to offset: [%offset_1506], sizes: [1, 1, 1, 1], strides: [%strides_1508, %strides_1508, %strides_1508, %strides_1508] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1510 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1509, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1510 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_1503[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1510[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1504[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_1511 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg195, %alloc_1511 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1512, %offset_1513, %sizes_1514, %strides_1515 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1516 = memref.reinterpret_cast %base_buffer_1512 to offset: [%offset_1513], sizes: [1, 1, 1, 1], strides: [%strides_1515, %strides_1515, %strides_1515, %strides_1515] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1517 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1516, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1517 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_1483[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1517[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1511[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1518 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_1519 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1518, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_1519 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_1520 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_1521 = memref.alloc() : memref<1x12x1xf32>
    %alloc_1522 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_1522[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_1494[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_1504[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_1519[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_1511[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_1522[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_1522[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_1521[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_1522[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_1520[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1523 = memref.reinterpret_cast %alloc_1520 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_1524 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1524 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1523, %arg196 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1524 : memref<1x1536xf32>)
    %reinterpret_cast_1525 = memref.reinterpret_cast %alloc_1524 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1526 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1443, %reinterpret_cast_1525 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1526 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1527 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1526 : memref<1x1x1536xf32>) outs(%alloc_1527 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1528 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1528 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1527 : memref<1x1x1536xf32>) outs(%alloc_1528 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1529 = memref.reinterpret_cast %alloc_1528 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1530 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1529, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1530 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1531 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1530, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1531 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1532 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1531 : memref<1x1x1xf32>) outs(%alloc_1532 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1533 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1526, %alloc_1532 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1533 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1534, %offset_1535, %sizes_1536, %strides_1537 = memref.extract_strided_metadata %arg197 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %109 = affine.apply #map10()[%strides_1537]
    %110 = affine.apply #map10()[%strides_1537]
    %reinterpret_cast_1538 = memref.reinterpret_cast %base_buffer_1534 to offset: [%offset_1535], sizes: [1, 1, 1536], strides: [%109, %110, %strides_1537] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1539 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1538, %alloc_1533 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1539 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1540 = memref.reinterpret_cast %alloc_1539 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1541 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1541 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1540, %arg198 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1541 : memref<1x8960xf32>)
    %reinterpret_cast_1542 = memref.reinterpret_cast %alloc_1541 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1543 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1542 : memref<1x1x8960xf32>) outs(%alloc_1543 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_1544 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1542, %alloc_1543 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1544 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1545 = memref.reinterpret_cast %alloc_1539 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1546 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1546 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1545, %arg199 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1546 : memref<1x8960xf32>)
    %reinterpret_cast_1547 = memref.reinterpret_cast %alloc_1546 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1548 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1544, %reinterpret_cast_1547 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1548 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1549 = memref.reinterpret_cast %alloc_1548 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1550 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1550 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1549, %arg200 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1550 : memref<1x1536xf32>)
    %reinterpret_cast_1551 = memref.reinterpret_cast %alloc_1550 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1552 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1526, %reinterpret_cast_1551 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1552 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1553 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1552 : memref<1x1x1536xf32>) outs(%alloc_1553 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1554 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1554 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1553 : memref<1x1x1536xf32>) outs(%alloc_1554 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1555 = memref.reinterpret_cast %alloc_1554 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1556 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1555, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1556 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1557 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1556, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1557 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1558 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1557 : memref<1x1x1xf32>) outs(%alloc_1558 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1559 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1552, %alloc_1558 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1559 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1560, %offset_1561, %sizes_1562, %strides_1563 = memref.extract_strided_metadata %arg201 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %111 = affine.apply #map10()[%strides_1563]
    %112 = affine.apply #map10()[%strides_1563]
    %reinterpret_cast_1564 = memref.reinterpret_cast %base_buffer_1560 to offset: [%offset_1561], sizes: [1, 1, 1536], strides: [%111, %112, %strides_1563] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1565 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1564, %alloc_1559 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1565 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1566 = memref.reinterpret_cast %alloc_1565 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1567 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1567 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1566, %arg203 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1567 : memref<1x1536xf32>)
    %base_buffer_1568, %offset_1569, %sizes_1570, %strides_1571 = memref.extract_strided_metadata %arg202 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %113 = affine.apply #map10()[%strides_1571]
    %reinterpret_cast_1572 = memref.reinterpret_cast %base_buffer_1568 to offset: [%offset_1569], sizes: [1, 1536], strides: [%113, %strides_1571] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_1573 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1572, %alloc_1567 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_1573 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1574 = memref.reinterpret_cast %alloc_1573 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_1575 = memref.reinterpret_cast %alloc_1565 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1576 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1576 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1575, %arg205 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1576 : memref<1x256xf32>)
    %base_buffer_1577, %offset_1578, %sizes_1579, %strides_1580 = memref.extract_strided_metadata %arg204 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %114 = affine.apply #map11()[%strides_1580]
    %reinterpret_cast_1581 = memref.reinterpret_cast %base_buffer_1577 to offset: [%offset_1578], sizes: [1, 256], strides: [%114, %strides_1580] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1582 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1581, %alloc_1576 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1582 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1583 = memref.reinterpret_cast %alloc_1582 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1584 = memref.reinterpret_cast %alloc_1565 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1585 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1585 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1584, %arg207 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1585 : memref<1x256xf32>)
    %base_buffer_1586, %offset_1587, %sizes_1588, %strides_1589 = memref.extract_strided_metadata %arg206 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %115 = affine.apply #map11()[%strides_1589]
    %reinterpret_cast_1590 = memref.reinterpret_cast %base_buffer_1586 to offset: [%offset_1587], sizes: [1, 256], strides: [%115, %strides_1589] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1591 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1590, %alloc_1585 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1591 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1592 = memref.reinterpret_cast %alloc_1591 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1593 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_1594 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_1595 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1574, %reinterpret_cast_1593 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1595 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1596 = memref.reinterpret_cast %alloc_1573 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1597 = memref.reinterpret_cast %alloc_1573 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1598 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1597 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_1598 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1599 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_1600 = memref.reinterpret_cast %alloc_1599 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_1598, %reinterpret_cast_1600 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1601 = memref.reinterpret_cast %alloc_1599 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1596, %reinterpret_cast_1601 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1602 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1599, %reinterpret_cast_1594 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1602 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1603 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1595, %alloc_1602 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_1603 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1604 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1583, %reinterpret_cast_1593 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1604 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1605 = memref.reinterpret_cast %alloc_1582 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1606 = memref.reinterpret_cast %alloc_1582 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1607 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1606 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_1607 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1608 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_1609 = memref.reinterpret_cast %alloc_1608 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_1607, %reinterpret_cast_1609 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1610 = memref.reinterpret_cast %alloc_1608 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1605, %reinterpret_cast_1610 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1611 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1608, %reinterpret_cast_1594 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1611 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1612 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1604, %alloc_1611 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_1612 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1613 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg208, %alloc_1613 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1614, %offset_1615, %sizes_1616, %strides_1617 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1618 = memref.reinterpret_cast %base_buffer_1614 to offset: [%offset_1615], sizes: [1, 1, 1, 1], strides: [%strides_1617, %strides_1617, %strides_1617, %strides_1617] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1619 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1618, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1619 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_1612[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1619[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1613[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_1620 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg209, %alloc_1620 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1621, %offset_1622, %sizes_1623, %strides_1624 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1625 = memref.reinterpret_cast %base_buffer_1621 to offset: [%offset_1622], sizes: [1, 1, 1, 1], strides: [%strides_1624, %strides_1624, %strides_1624, %strides_1624] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1626 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1625, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1626 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_1592[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1626[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1620[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1627 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_1628 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1627, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_1628 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_1629 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_1630 = memref.alloc() : memref<1x12x1xf32>
    %alloc_1631 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_1631[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_1603[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_1613[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_1628[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_1620[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_1631[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_1631[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_1630[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_1631[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_1629[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1632 = memref.reinterpret_cast %alloc_1629 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_1633 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1633 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1632, %arg210 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1633 : memref<1x1536xf32>)
    %reinterpret_cast_1634 = memref.reinterpret_cast %alloc_1633 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1635 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1552, %reinterpret_cast_1634 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1635 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1636 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1635 : memref<1x1x1536xf32>) outs(%alloc_1636 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1637 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1637 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1636 : memref<1x1x1536xf32>) outs(%alloc_1637 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1638 = memref.reinterpret_cast %alloc_1637 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1639 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1638, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1639 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1640 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1639, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1640 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1641 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1640 : memref<1x1x1xf32>) outs(%alloc_1641 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1642 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1635, %alloc_1641 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1642 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1643, %offset_1644, %sizes_1645, %strides_1646 = memref.extract_strided_metadata %arg211 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %116 = affine.apply #map10()[%strides_1646]
    %117 = affine.apply #map10()[%strides_1646]
    %reinterpret_cast_1647 = memref.reinterpret_cast %base_buffer_1643 to offset: [%offset_1644], sizes: [1, 1, 1536], strides: [%116, %117, %strides_1646] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1648 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1647, %alloc_1642 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1648 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1649 = memref.reinterpret_cast %alloc_1648 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1650 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1650 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1649, %arg212 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1650 : memref<1x8960xf32>)
    %reinterpret_cast_1651 = memref.reinterpret_cast %alloc_1650 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1652 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1651 : memref<1x1x8960xf32>) outs(%alloc_1652 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_1653 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1651, %alloc_1652 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1653 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1654 = memref.reinterpret_cast %alloc_1648 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1655 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1655 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1654, %arg213 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1655 : memref<1x8960xf32>)
    %reinterpret_cast_1656 = memref.reinterpret_cast %alloc_1655 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1657 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1653, %reinterpret_cast_1656 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1657 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1658 = memref.reinterpret_cast %alloc_1657 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1659 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1659 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1658, %arg214 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1659 : memref<1x1536xf32>)
    %reinterpret_cast_1660 = memref.reinterpret_cast %alloc_1659 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1661 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1635, %reinterpret_cast_1660 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1661 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1662 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1661 : memref<1x1x1536xf32>) outs(%alloc_1662 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1663 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1663 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1662 : memref<1x1x1536xf32>) outs(%alloc_1663 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1664 = memref.reinterpret_cast %alloc_1663 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1665 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1664, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1665 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1666 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1665, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1666 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1667 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1666 : memref<1x1x1xf32>) outs(%alloc_1667 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1668 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1661, %alloc_1667 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1668 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1669, %offset_1670, %sizes_1671, %strides_1672 = memref.extract_strided_metadata %arg215 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %118 = affine.apply #map10()[%strides_1672]
    %119 = affine.apply #map10()[%strides_1672]
    %reinterpret_cast_1673 = memref.reinterpret_cast %base_buffer_1669 to offset: [%offset_1670], sizes: [1, 1, 1536], strides: [%118, %119, %strides_1672] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1674 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1673, %alloc_1668 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1674 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1675 = memref.reinterpret_cast %alloc_1674 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1676 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1676 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1675, %arg217 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1676 : memref<1x1536xf32>)
    %base_buffer_1677, %offset_1678, %sizes_1679, %strides_1680 = memref.extract_strided_metadata %arg216 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %120 = affine.apply #map10()[%strides_1680]
    %reinterpret_cast_1681 = memref.reinterpret_cast %base_buffer_1677 to offset: [%offset_1678], sizes: [1, 1536], strides: [%120, %strides_1680] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_1682 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1681, %alloc_1676 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_1682 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1683 = memref.reinterpret_cast %alloc_1682 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_1684 = memref.reinterpret_cast %alloc_1674 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1685 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1685 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1684, %arg219 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1685 : memref<1x256xf32>)
    %base_buffer_1686, %offset_1687, %sizes_1688, %strides_1689 = memref.extract_strided_metadata %arg218 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %121 = affine.apply #map11()[%strides_1689]
    %reinterpret_cast_1690 = memref.reinterpret_cast %base_buffer_1686 to offset: [%offset_1687], sizes: [1, 256], strides: [%121, %strides_1689] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1691 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1690, %alloc_1685 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1691 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1692 = memref.reinterpret_cast %alloc_1691 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1693 = memref.reinterpret_cast %alloc_1674 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1694 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1694 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1693, %arg221 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1694 : memref<1x256xf32>)
    %base_buffer_1695, %offset_1696, %sizes_1697, %strides_1698 = memref.extract_strided_metadata %arg220 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %122 = affine.apply #map11()[%strides_1698]
    %reinterpret_cast_1699 = memref.reinterpret_cast %base_buffer_1695 to offset: [%offset_1696], sizes: [1, 256], strides: [%122, %strides_1698] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1700 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1699, %alloc_1694 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1700 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1701 = memref.reinterpret_cast %alloc_1700 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1702 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_1703 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_1704 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1683, %reinterpret_cast_1702 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1704 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1705 = memref.reinterpret_cast %alloc_1682 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1706 = memref.reinterpret_cast %alloc_1682 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1707 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1706 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_1707 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1708 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_1709 = memref.reinterpret_cast %alloc_1708 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_1707, %reinterpret_cast_1709 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1710 = memref.reinterpret_cast %alloc_1708 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1705, %reinterpret_cast_1710 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1711 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1708, %reinterpret_cast_1703 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1711 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1712 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1704, %alloc_1711 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_1712 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1713 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1692, %reinterpret_cast_1702 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1713 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1714 = memref.reinterpret_cast %alloc_1691 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1715 = memref.reinterpret_cast %alloc_1691 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1716 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1715 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_1716 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1717 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_1718 = memref.reinterpret_cast %alloc_1717 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_1716, %reinterpret_cast_1718 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1719 = memref.reinterpret_cast %alloc_1717 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1714, %reinterpret_cast_1719 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1720 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1717, %reinterpret_cast_1703 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1720 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1721 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1713, %alloc_1720 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_1721 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1722 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg222, %alloc_1722 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1723, %offset_1724, %sizes_1725, %strides_1726 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1727 = memref.reinterpret_cast %base_buffer_1723 to offset: [%offset_1724], sizes: [1, 1, 1, 1], strides: [%strides_1726, %strides_1726, %strides_1726, %strides_1726] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1728 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1727, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1728 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_1721[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1728[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1722[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_1729 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg223, %alloc_1729 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1730, %offset_1731, %sizes_1732, %strides_1733 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1734 = memref.reinterpret_cast %base_buffer_1730 to offset: [%offset_1731], sizes: [1, 1, 1, 1], strides: [%strides_1733, %strides_1733, %strides_1733, %strides_1733] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1735 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1734, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1735 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_1701[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1735[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1729[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1736 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_1737 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1736, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_1737 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_1738 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_1739 = memref.alloc() : memref<1x12x1xf32>
    %alloc_1740 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_1740[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_1712[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_1722[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_1737[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_1729[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_1740[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_1740[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_1739[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_1740[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_1738[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1741 = memref.reinterpret_cast %alloc_1738 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_1742 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1742 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1741, %arg224 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1742 : memref<1x1536xf32>)
    %reinterpret_cast_1743 = memref.reinterpret_cast %alloc_1742 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1744 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1661, %reinterpret_cast_1743 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1744 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1745 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1744 : memref<1x1x1536xf32>) outs(%alloc_1745 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1746 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1746 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1745 : memref<1x1x1536xf32>) outs(%alloc_1746 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1747 = memref.reinterpret_cast %alloc_1746 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1748 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1747, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1748 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1749 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1748, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1749 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1750 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1749 : memref<1x1x1xf32>) outs(%alloc_1750 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1751 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1744, %alloc_1750 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1751 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1752, %offset_1753, %sizes_1754, %strides_1755 = memref.extract_strided_metadata %arg225 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %123 = affine.apply #map10()[%strides_1755]
    %124 = affine.apply #map10()[%strides_1755]
    %reinterpret_cast_1756 = memref.reinterpret_cast %base_buffer_1752 to offset: [%offset_1753], sizes: [1, 1, 1536], strides: [%123, %124, %strides_1755] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1757 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1756, %alloc_1751 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1757 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1758 = memref.reinterpret_cast %alloc_1757 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1759 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1759 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1758, %arg226 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1759 : memref<1x8960xf32>)
    %reinterpret_cast_1760 = memref.reinterpret_cast %alloc_1759 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1761 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1760 : memref<1x1x8960xf32>) outs(%alloc_1761 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_1762 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1760, %alloc_1761 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1762 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1763 = memref.reinterpret_cast %alloc_1757 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1764 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1764 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1763, %arg227 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1764 : memref<1x8960xf32>)
    %reinterpret_cast_1765 = memref.reinterpret_cast %alloc_1764 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1766 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1762, %reinterpret_cast_1765 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1766 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1767 = memref.reinterpret_cast %alloc_1766 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1768 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1768 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1767, %arg228 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1768 : memref<1x1536xf32>)
    %reinterpret_cast_1769 = memref.reinterpret_cast %alloc_1768 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1770 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1744, %reinterpret_cast_1769 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1770 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1771 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1770 : memref<1x1x1536xf32>) outs(%alloc_1771 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1772 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1772 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1771 : memref<1x1x1536xf32>) outs(%alloc_1772 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1773 = memref.reinterpret_cast %alloc_1772 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1774 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1773, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1774 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1775 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1774, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1775 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1776 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1775 : memref<1x1x1xf32>) outs(%alloc_1776 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1777 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1770, %alloc_1776 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1777 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1778, %offset_1779, %sizes_1780, %strides_1781 = memref.extract_strided_metadata %arg229 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %125 = affine.apply #map10()[%strides_1781]
    %126 = affine.apply #map10()[%strides_1781]
    %reinterpret_cast_1782 = memref.reinterpret_cast %base_buffer_1778 to offset: [%offset_1779], sizes: [1, 1, 1536], strides: [%125, %126, %strides_1781] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1783 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1782, %alloc_1777 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1783 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1784 = memref.reinterpret_cast %alloc_1783 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1785 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1785 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1784, %arg231 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1785 : memref<1x1536xf32>)
    %base_buffer_1786, %offset_1787, %sizes_1788, %strides_1789 = memref.extract_strided_metadata %arg230 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %127 = affine.apply #map10()[%strides_1789]
    %reinterpret_cast_1790 = memref.reinterpret_cast %base_buffer_1786 to offset: [%offset_1787], sizes: [1, 1536], strides: [%127, %strides_1789] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_1791 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1790, %alloc_1785 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_1791 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1792 = memref.reinterpret_cast %alloc_1791 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_1793 = memref.reinterpret_cast %alloc_1783 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1794 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1794 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1793, %arg233 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1794 : memref<1x256xf32>)
    %base_buffer_1795, %offset_1796, %sizes_1797, %strides_1798 = memref.extract_strided_metadata %arg232 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %128 = affine.apply #map11()[%strides_1798]
    %reinterpret_cast_1799 = memref.reinterpret_cast %base_buffer_1795 to offset: [%offset_1796], sizes: [1, 256], strides: [%128, %strides_1798] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1800 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1799, %alloc_1794 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1800 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1801 = memref.reinterpret_cast %alloc_1800 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1802 = memref.reinterpret_cast %alloc_1783 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1803 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1803 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1802, %arg235 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1803 : memref<1x256xf32>)
    %base_buffer_1804, %offset_1805, %sizes_1806, %strides_1807 = memref.extract_strided_metadata %arg234 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %129 = affine.apply #map11()[%strides_1807]
    %reinterpret_cast_1808 = memref.reinterpret_cast %base_buffer_1804 to offset: [%offset_1805], sizes: [1, 256], strides: [%129, %strides_1807] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1809 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1808, %alloc_1803 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1809 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1810 = memref.reinterpret_cast %alloc_1809 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1811 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_1812 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_1813 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1792, %reinterpret_cast_1811 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1813 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1814 = memref.reinterpret_cast %alloc_1791 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1815 = memref.reinterpret_cast %alloc_1791 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1816 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1815 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_1816 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1817 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_1818 = memref.reinterpret_cast %alloc_1817 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_1816, %reinterpret_cast_1818 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1819 = memref.reinterpret_cast %alloc_1817 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1814, %reinterpret_cast_1819 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1820 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1817, %reinterpret_cast_1812 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1820 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1821 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1813, %alloc_1820 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_1821 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1822 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1801, %reinterpret_cast_1811 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1822 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1823 = memref.reinterpret_cast %alloc_1800 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1824 = memref.reinterpret_cast %alloc_1800 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1825 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1824 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_1825 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1826 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_1827 = memref.reinterpret_cast %alloc_1826 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_1825, %reinterpret_cast_1827 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1828 = memref.reinterpret_cast %alloc_1826 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1823, %reinterpret_cast_1828 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1829 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1826, %reinterpret_cast_1812 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1829 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1830 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1822, %alloc_1829 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_1830 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1831 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg236, %alloc_1831 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1832, %offset_1833, %sizes_1834, %strides_1835 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1836 = memref.reinterpret_cast %base_buffer_1832 to offset: [%offset_1833], sizes: [1, 1, 1, 1], strides: [%strides_1835, %strides_1835, %strides_1835, %strides_1835] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1837 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1836, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1837 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_1830[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1837[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1831[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_1838 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg237, %alloc_1838 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1839, %offset_1840, %sizes_1841, %strides_1842 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1843 = memref.reinterpret_cast %base_buffer_1839 to offset: [%offset_1840], sizes: [1, 1, 1, 1], strides: [%strides_1842, %strides_1842, %strides_1842, %strides_1842] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1844 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1843, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1844 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_1810[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1844[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1838[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1845 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_1846 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1845, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_1846 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_1847 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_1848 = memref.alloc() : memref<1x12x1xf32>
    %alloc_1849 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_1849[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_1821[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_1831[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_1846[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_1838[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_1849[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_1849[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_1848[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_1849[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_1847[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1850 = memref.reinterpret_cast %alloc_1847 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_1851 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1851 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1850, %arg238 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1851 : memref<1x1536xf32>)
    %reinterpret_cast_1852 = memref.reinterpret_cast %alloc_1851 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1853 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1770, %reinterpret_cast_1852 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1853 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1854 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1853 : memref<1x1x1536xf32>) outs(%alloc_1854 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1855 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1855 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1854 : memref<1x1x1536xf32>) outs(%alloc_1855 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1856 = memref.reinterpret_cast %alloc_1855 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1857 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1856, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1857 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1858 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1857, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1858 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1859 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1858 : memref<1x1x1xf32>) outs(%alloc_1859 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1860 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1853, %alloc_1859 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1860 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1861, %offset_1862, %sizes_1863, %strides_1864 = memref.extract_strided_metadata %arg239 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %130 = affine.apply #map10()[%strides_1864]
    %131 = affine.apply #map10()[%strides_1864]
    %reinterpret_cast_1865 = memref.reinterpret_cast %base_buffer_1861 to offset: [%offset_1862], sizes: [1, 1, 1536], strides: [%130, %131, %strides_1864] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1866 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1865, %alloc_1860 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1866 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1867 = memref.reinterpret_cast %alloc_1866 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1868 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1868 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1867, %arg240 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1868 : memref<1x8960xf32>)
    %reinterpret_cast_1869 = memref.reinterpret_cast %alloc_1868 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1870 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1869 : memref<1x1x8960xf32>) outs(%alloc_1870 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_1871 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1869, %alloc_1870 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1871 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1872 = memref.reinterpret_cast %alloc_1866 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1873 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1873 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1872, %arg241 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1873 : memref<1x8960xf32>)
    %reinterpret_cast_1874 = memref.reinterpret_cast %alloc_1873 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1875 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1871, %reinterpret_cast_1874 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1875 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1876 = memref.reinterpret_cast %alloc_1875 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1877 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1877 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1876, %arg242 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1877 : memref<1x1536xf32>)
    %reinterpret_cast_1878 = memref.reinterpret_cast %alloc_1877 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1879 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1853, %reinterpret_cast_1878 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1879 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1880 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1879 : memref<1x1x1536xf32>) outs(%alloc_1880 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1881 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1881 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1880 : memref<1x1x1536xf32>) outs(%alloc_1881 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1882 = memref.reinterpret_cast %alloc_1881 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1883 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1882, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1883 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1884 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1883, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1884 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1885 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1884 : memref<1x1x1xf32>) outs(%alloc_1885 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1886 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1879, %alloc_1885 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1886 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1887, %offset_1888, %sizes_1889, %strides_1890 = memref.extract_strided_metadata %arg243 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %132 = affine.apply #map10()[%strides_1890]
    %133 = affine.apply #map10()[%strides_1890]
    %reinterpret_cast_1891 = memref.reinterpret_cast %base_buffer_1887 to offset: [%offset_1888], sizes: [1, 1, 1536], strides: [%132, %133, %strides_1890] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1892 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1891, %alloc_1886 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1892 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1893 = memref.reinterpret_cast %alloc_1892 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1894 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1894 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1893, %arg245 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1894 : memref<1x1536xf32>)
    %base_buffer_1895, %offset_1896, %sizes_1897, %strides_1898 = memref.extract_strided_metadata %arg244 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %134 = affine.apply #map10()[%strides_1898]
    %reinterpret_cast_1899 = memref.reinterpret_cast %base_buffer_1895 to offset: [%offset_1896], sizes: [1, 1536], strides: [%134, %strides_1898] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_1900 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1899, %alloc_1894 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_1900 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1901 = memref.reinterpret_cast %alloc_1900 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_1902 = memref.reinterpret_cast %alloc_1892 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1903 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1903 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1902, %arg247 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1903 : memref<1x256xf32>)
    %base_buffer_1904, %offset_1905, %sizes_1906, %strides_1907 = memref.extract_strided_metadata %arg246 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %135 = affine.apply #map11()[%strides_1907]
    %reinterpret_cast_1908 = memref.reinterpret_cast %base_buffer_1904 to offset: [%offset_1905], sizes: [1, 256], strides: [%135, %strides_1907] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1909 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1908, %alloc_1903 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1909 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1910 = memref.reinterpret_cast %alloc_1909 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1911 = memref.reinterpret_cast %alloc_1892 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1912 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_1912 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1911, %arg249 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1912 : memref<1x256xf32>)
    %base_buffer_1913, %offset_1914, %sizes_1915, %strides_1916 = memref.extract_strided_metadata %arg248 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %136 = affine.apply #map11()[%strides_1916]
    %reinterpret_cast_1917 = memref.reinterpret_cast %base_buffer_1913 to offset: [%offset_1914], sizes: [1, 256], strides: [%136, %strides_1916] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_1918 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_1917, %alloc_1912 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_1918 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1919 = memref.reinterpret_cast %alloc_1918 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_1920 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_1921 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_1922 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1901, %reinterpret_cast_1920 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1922 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1923 = memref.reinterpret_cast %alloc_1900 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1924 = memref.reinterpret_cast %alloc_1900 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1925 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1924 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_1925 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1926 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_1927 = memref.reinterpret_cast %alloc_1926 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_1925, %reinterpret_cast_1927 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_1928 = memref.reinterpret_cast %alloc_1926 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1923, %reinterpret_cast_1928 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_1929 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1926, %reinterpret_cast_1921 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1929 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1930 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1922, %alloc_1929 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_1930 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1931 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1910, %reinterpret_cast_1920 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1931 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1932 = memref.reinterpret_cast %alloc_1909 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1933 = memref.reinterpret_cast %alloc_1909 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1934 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1933 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_1934 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1935 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_1936 = memref.reinterpret_cast %alloc_1935 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_1934, %reinterpret_cast_1936 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_1937 = memref.reinterpret_cast %alloc_1935 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_1932, %reinterpret_cast_1937 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_1938 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1935, %reinterpret_cast_1921 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_1938 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1939 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_1931, %alloc_1938 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_1939 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1940 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg250, %alloc_1940 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1941, %offset_1942, %sizes_1943, %strides_1944 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1945 = memref.reinterpret_cast %base_buffer_1941 to offset: [%offset_1942], sizes: [1, 1, 1, 1], strides: [%strides_1944, %strides_1944, %strides_1944, %strides_1944] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1946 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1945, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1946 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_1939[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1946[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1940[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_1947 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg251, %alloc_1947 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_1948, %offset_1949, %sizes_1950, %strides_1951 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_1952 = memref.reinterpret_cast %base_buffer_1948 to offset: [%offset_1949], sizes: [1, 1, 1, 1], strides: [%strides_1951, %strides_1951, %strides_1951, %strides_1951] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_1953 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1952, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_1953 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_1919[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_1953[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_1947[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1954 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_1955 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1954, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_1955 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_1956 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_1957 = memref.alloc() : memref<1x12x1xf32>
    %alloc_1958 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_1958[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_1930[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_1940[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_1955[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_1947[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_1958[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_1958[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_1957[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_1958[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_1956[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_1959 = memref.reinterpret_cast %alloc_1956 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_1960 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1960 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1959, %arg252 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1960 : memref<1x1536xf32>)
    %reinterpret_cast_1961 = memref.reinterpret_cast %alloc_1960 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1962 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1879, %reinterpret_cast_1961 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1962 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1963 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1962 : memref<1x1x1536xf32>) outs(%alloc_1963 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1964 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1964 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1963 : memref<1x1x1536xf32>) outs(%alloc_1964 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1965 = memref.reinterpret_cast %alloc_1964 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1966 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1965, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1966 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1967 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1966, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1967 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1968 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1967 : memref<1x1x1xf32>) outs(%alloc_1968 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1969 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1962, %alloc_1968 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1969 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1970, %offset_1971, %sizes_1972, %strides_1973 = memref.extract_strided_metadata %arg253 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %137 = affine.apply #map10()[%strides_1973]
    %138 = affine.apply #map10()[%strides_1973]
    %reinterpret_cast_1974 = memref.reinterpret_cast %base_buffer_1970 to offset: [%offset_1971], sizes: [1, 1, 1536], strides: [%137, %138, %strides_1973] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_1975 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1974, %alloc_1969 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_1975 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1976 = memref.reinterpret_cast %alloc_1975 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1977 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1977 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1976, %arg254 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1977 : memref<1x8960xf32>)
    %reinterpret_cast_1978 = memref.reinterpret_cast %alloc_1977 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1979 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1978 : memref<1x1x8960xf32>) outs(%alloc_1979 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_1980 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1978, %alloc_1979 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1980 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1981 = memref.reinterpret_cast %alloc_1975 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_1982 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_1982 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1981, %arg255 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1982 : memref<1x8960xf32>)
    %reinterpret_cast_1983 = memref.reinterpret_cast %alloc_1982 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_1984 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1980, %reinterpret_cast_1983 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_1984 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_1985 = memref.reinterpret_cast %alloc_1984 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_1986 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_1986 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_1985, %arg256 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1986 : memref<1x1536xf32>)
    %reinterpret_cast_1987 = memref.reinterpret_cast %alloc_1986 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_1988 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1962, %reinterpret_cast_1987 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_1988 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1989 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1988 : memref<1x1x1536xf32>) outs(%alloc_1989 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_1990 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_1990 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_1989 : memref<1x1x1536xf32>) outs(%alloc_1990 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_1991 = memref.reinterpret_cast %alloc_1990 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_1992 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_1991, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1992 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1993 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1992, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_1993 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_1994 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1993 : memref<1x1x1xf32>) outs(%alloc_1994 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_1995 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1988, %alloc_1994 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_1995 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_1996, %offset_1997, %sizes_1998, %strides_1999 = memref.extract_strided_metadata %arg257 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %139 = affine.apply #map10()[%strides_1999]
    %140 = affine.apply #map10()[%strides_1999]
    %reinterpret_cast_2000 = memref.reinterpret_cast %base_buffer_1996 to offset: [%offset_1997], sizes: [1, 1, 1536], strides: [%139, %140, %strides_1999] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2001 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2000, %alloc_1995 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2001 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2002 = memref.reinterpret_cast %alloc_2001 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2003 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2003 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2002, %arg259 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2003 : memref<1x1536xf32>)
    %base_buffer_2004, %offset_2005, %sizes_2006, %strides_2007 = memref.extract_strided_metadata %arg258 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %141 = affine.apply #map10()[%strides_2007]
    %reinterpret_cast_2008 = memref.reinterpret_cast %base_buffer_2004 to offset: [%offset_2005], sizes: [1, 1536], strides: [%141, %strides_2007] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2009 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2008, %alloc_2003 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2009 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2010 = memref.reinterpret_cast %alloc_2009 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2011 = memref.reinterpret_cast %alloc_2001 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2012 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2012 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2011, %arg261 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2012 : memref<1x256xf32>)
    %base_buffer_2013, %offset_2014, %sizes_2015, %strides_2016 = memref.extract_strided_metadata %arg260 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %142 = affine.apply #map11()[%strides_2016]
    %reinterpret_cast_2017 = memref.reinterpret_cast %base_buffer_2013 to offset: [%offset_2014], sizes: [1, 256], strides: [%142, %strides_2016] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2018 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2017, %alloc_2012 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2018 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2019 = memref.reinterpret_cast %alloc_2018 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2020 = memref.reinterpret_cast %alloc_2001 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2021 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2021 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2020, %arg263 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2021 : memref<1x256xf32>)
    %base_buffer_2022, %offset_2023, %sizes_2024, %strides_2025 = memref.extract_strided_metadata %arg262 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %143 = affine.apply #map11()[%strides_2025]
    %reinterpret_cast_2026 = memref.reinterpret_cast %base_buffer_2022 to offset: [%offset_2023], sizes: [1, 256], strides: [%143, %strides_2025] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2027 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2026, %alloc_2021 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2027 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2028 = memref.reinterpret_cast %alloc_2027 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2029 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_2030 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_2031 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2010, %reinterpret_cast_2029 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2031 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2032 = memref.reinterpret_cast %alloc_2009 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2033 = memref.reinterpret_cast %alloc_2009 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2034 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2033 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_2034 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2035 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_2036 = memref.reinterpret_cast %alloc_2035 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_2034, %reinterpret_cast_2036 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2037 = memref.reinterpret_cast %alloc_2035 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2032, %reinterpret_cast_2037 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2038 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2035, %reinterpret_cast_2030 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2038 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2039 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2031, %alloc_2038 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_2039 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2040 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2019, %reinterpret_cast_2029 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2040 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2041 = memref.reinterpret_cast %alloc_2018 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2042 = memref.reinterpret_cast %alloc_2018 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2043 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2042 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_2043 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2044 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_2045 = memref.reinterpret_cast %alloc_2044 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_2043, %reinterpret_cast_2045 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2046 = memref.reinterpret_cast %alloc_2044 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2041, %reinterpret_cast_2046 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2047 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2044, %reinterpret_cast_2030 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2047 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2048 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2040, %alloc_2047 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_2048 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2049 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg264, %alloc_2049 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2050, %offset_2051, %sizes_2052, %strides_2053 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2054 = memref.reinterpret_cast %base_buffer_2050 to offset: [%offset_2051], sizes: [1, 1, 1, 1], strides: [%strides_2053, %strides_2053, %strides_2053, %strides_2053] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2055 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2054, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2055 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_2048[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2055[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2049[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_2056 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg265, %alloc_2056 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2057, %offset_2058, %sizes_2059, %strides_2060 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2061 = memref.reinterpret_cast %base_buffer_2057 to offset: [%offset_2058], sizes: [1, 1, 1, 1], strides: [%strides_2060, %strides_2060, %strides_2060, %strides_2060] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2062 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2061, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2062 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_2028[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2062[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2056[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2063 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_2064 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2063, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_2064 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_2065 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_2066 = memref.alloc() : memref<1x12x1xf32>
    %alloc_2067 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_2067[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_2039[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_2049[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_2064[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_2056[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_2067[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_2067[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_2066[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_2067[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_2065[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2068 = memref.reinterpret_cast %alloc_2065 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_2069 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2069 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2068, %arg266 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2069 : memref<1x1536xf32>)
    %reinterpret_cast_2070 = memref.reinterpret_cast %alloc_2069 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2071 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_1988, %reinterpret_cast_2070 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2071 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2072 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2071 : memref<1x1x1536xf32>) outs(%alloc_2072 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2073 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2073 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2072 : memref<1x1x1536xf32>) outs(%alloc_2073 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2074 = memref.reinterpret_cast %alloc_2073 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2075 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2074, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2075 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2076 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2075, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2076 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2077 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2076 : memref<1x1x1xf32>) outs(%alloc_2077 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2078 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2071, %alloc_2077 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2078 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2079, %offset_2080, %sizes_2081, %strides_2082 = memref.extract_strided_metadata %arg267 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %144 = affine.apply #map10()[%strides_2082]
    %145 = affine.apply #map10()[%strides_2082]
    %reinterpret_cast_2083 = memref.reinterpret_cast %base_buffer_2079 to offset: [%offset_2080], sizes: [1, 1, 1536], strides: [%144, %145, %strides_2082] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2084 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2083, %alloc_2078 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2084 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2085 = memref.reinterpret_cast %alloc_2084 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2086 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2086 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2085, %arg268 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2086 : memref<1x8960xf32>)
    %reinterpret_cast_2087 = memref.reinterpret_cast %alloc_2086 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2088 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2087 : memref<1x1x8960xf32>) outs(%alloc_2088 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_2089 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2087, %alloc_2088 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2089 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2090 = memref.reinterpret_cast %alloc_2084 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2091 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2091 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2090, %arg269 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2091 : memref<1x8960xf32>)
    %reinterpret_cast_2092 = memref.reinterpret_cast %alloc_2091 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2093 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2089, %reinterpret_cast_2092 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2093 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2094 = memref.reinterpret_cast %alloc_2093 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_2095 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2095 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2094, %arg270 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2095 : memref<1x1536xf32>)
    %reinterpret_cast_2096 = memref.reinterpret_cast %alloc_2095 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2097 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2071, %reinterpret_cast_2096 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2097 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2098 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2097 : memref<1x1x1536xf32>) outs(%alloc_2098 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2099 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2099 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2098 : memref<1x1x1536xf32>) outs(%alloc_2099 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2100 = memref.reinterpret_cast %alloc_2099 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2101 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2100, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2101 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2102 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2101, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2102 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2103 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2102 : memref<1x1x1xf32>) outs(%alloc_2103 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2104 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2097, %alloc_2103 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2104 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2105, %offset_2106, %sizes_2107, %strides_2108 = memref.extract_strided_metadata %arg271 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %146 = affine.apply #map10()[%strides_2108]
    %147 = affine.apply #map10()[%strides_2108]
    %reinterpret_cast_2109 = memref.reinterpret_cast %base_buffer_2105 to offset: [%offset_2106], sizes: [1, 1, 1536], strides: [%146, %147, %strides_2108] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2110 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2109, %alloc_2104 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2110 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2111 = memref.reinterpret_cast %alloc_2110 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2112 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2112 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2111, %arg273 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2112 : memref<1x1536xf32>)
    %base_buffer_2113, %offset_2114, %sizes_2115, %strides_2116 = memref.extract_strided_metadata %arg272 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %148 = affine.apply #map10()[%strides_2116]
    %reinterpret_cast_2117 = memref.reinterpret_cast %base_buffer_2113 to offset: [%offset_2114], sizes: [1, 1536], strides: [%148, %strides_2116] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2118 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2117, %alloc_2112 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2118 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2119 = memref.reinterpret_cast %alloc_2118 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2120 = memref.reinterpret_cast %alloc_2110 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2121 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2121 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2120, %arg275 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2121 : memref<1x256xf32>)
    %base_buffer_2122, %offset_2123, %sizes_2124, %strides_2125 = memref.extract_strided_metadata %arg274 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %149 = affine.apply #map11()[%strides_2125]
    %reinterpret_cast_2126 = memref.reinterpret_cast %base_buffer_2122 to offset: [%offset_2123], sizes: [1, 256], strides: [%149, %strides_2125] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2127 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2126, %alloc_2121 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2127 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2128 = memref.reinterpret_cast %alloc_2127 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2129 = memref.reinterpret_cast %alloc_2110 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2130 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2130 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2129, %arg277 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2130 : memref<1x256xf32>)
    %base_buffer_2131, %offset_2132, %sizes_2133, %strides_2134 = memref.extract_strided_metadata %arg276 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %150 = affine.apply #map11()[%strides_2134]
    %reinterpret_cast_2135 = memref.reinterpret_cast %base_buffer_2131 to offset: [%offset_2132], sizes: [1, 256], strides: [%150, %strides_2134] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2136 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2135, %alloc_2130 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2136 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2137 = memref.reinterpret_cast %alloc_2136 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2138 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_2139 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_2140 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2119, %reinterpret_cast_2138 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2140 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2141 = memref.reinterpret_cast %alloc_2118 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2142 = memref.reinterpret_cast %alloc_2118 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2143 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2142 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_2143 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2144 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_2145 = memref.reinterpret_cast %alloc_2144 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_2143, %reinterpret_cast_2145 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2146 = memref.reinterpret_cast %alloc_2144 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2141, %reinterpret_cast_2146 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2147 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2144, %reinterpret_cast_2139 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2147 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2148 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2140, %alloc_2147 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_2148 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2149 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2128, %reinterpret_cast_2138 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2149 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2150 = memref.reinterpret_cast %alloc_2127 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2151 = memref.reinterpret_cast %alloc_2127 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2152 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2151 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_2152 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2153 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_2154 = memref.reinterpret_cast %alloc_2153 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_2152, %reinterpret_cast_2154 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2155 = memref.reinterpret_cast %alloc_2153 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2150, %reinterpret_cast_2155 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2156 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2153, %reinterpret_cast_2139 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2156 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2157 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2149, %alloc_2156 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_2157 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2158 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg278, %alloc_2158 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2159, %offset_2160, %sizes_2161, %strides_2162 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2163 = memref.reinterpret_cast %base_buffer_2159 to offset: [%offset_2160], sizes: [1, 1, 1, 1], strides: [%strides_2162, %strides_2162, %strides_2162, %strides_2162] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2164 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2163, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2164 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_2157[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2164[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2158[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_2165 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg279, %alloc_2165 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2166, %offset_2167, %sizes_2168, %strides_2169 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2170 = memref.reinterpret_cast %base_buffer_2166 to offset: [%offset_2167], sizes: [1, 1, 1, 1], strides: [%strides_2169, %strides_2169, %strides_2169, %strides_2169] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2171 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2170, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2171 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_2137[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2171[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2165[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2172 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_2173 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2172, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_2173 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_2174 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_2175 = memref.alloc() : memref<1x12x1xf32>
    %alloc_2176 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_2176[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_2148[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_2158[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_2173[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_2165[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_2176[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_2176[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_2175[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_2176[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_2174[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2177 = memref.reinterpret_cast %alloc_2174 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_2178 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2178 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2177, %arg280 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2178 : memref<1x1536xf32>)
    %reinterpret_cast_2179 = memref.reinterpret_cast %alloc_2178 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2180 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2097, %reinterpret_cast_2179 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2180 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2181 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2180 : memref<1x1x1536xf32>) outs(%alloc_2181 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2182 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2182 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2181 : memref<1x1x1536xf32>) outs(%alloc_2182 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2183 = memref.reinterpret_cast %alloc_2182 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2184 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2183, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2184 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2185 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2184, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2185 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2186 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2185 : memref<1x1x1xf32>) outs(%alloc_2186 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2187 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2180, %alloc_2186 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2187 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2188, %offset_2189, %sizes_2190, %strides_2191 = memref.extract_strided_metadata %arg281 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %151 = affine.apply #map10()[%strides_2191]
    %152 = affine.apply #map10()[%strides_2191]
    %reinterpret_cast_2192 = memref.reinterpret_cast %base_buffer_2188 to offset: [%offset_2189], sizes: [1, 1, 1536], strides: [%151, %152, %strides_2191] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2193 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2192, %alloc_2187 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2193 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2194 = memref.reinterpret_cast %alloc_2193 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2195 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2195 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2194, %arg282 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2195 : memref<1x8960xf32>)
    %reinterpret_cast_2196 = memref.reinterpret_cast %alloc_2195 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2197 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2196 : memref<1x1x8960xf32>) outs(%alloc_2197 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_2198 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2196, %alloc_2197 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2198 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2199 = memref.reinterpret_cast %alloc_2193 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2200 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2200 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2199, %arg283 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2200 : memref<1x8960xf32>)
    %reinterpret_cast_2201 = memref.reinterpret_cast %alloc_2200 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2202 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2198, %reinterpret_cast_2201 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2202 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2203 = memref.reinterpret_cast %alloc_2202 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_2204 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2204 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2203, %arg284 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2204 : memref<1x1536xf32>)
    %reinterpret_cast_2205 = memref.reinterpret_cast %alloc_2204 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2206 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2180, %reinterpret_cast_2205 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2206 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2207 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2206 : memref<1x1x1536xf32>) outs(%alloc_2207 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2208 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2208 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2207 : memref<1x1x1536xf32>) outs(%alloc_2208 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2209 = memref.reinterpret_cast %alloc_2208 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2210 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2209, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2210 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2211 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2210, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2211 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2212 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2211 : memref<1x1x1xf32>) outs(%alloc_2212 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2213 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2206, %alloc_2212 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2213 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2214, %offset_2215, %sizes_2216, %strides_2217 = memref.extract_strided_metadata %arg285 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %153 = affine.apply #map10()[%strides_2217]
    %154 = affine.apply #map10()[%strides_2217]
    %reinterpret_cast_2218 = memref.reinterpret_cast %base_buffer_2214 to offset: [%offset_2215], sizes: [1, 1, 1536], strides: [%153, %154, %strides_2217] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2219 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2218, %alloc_2213 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2219 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2220 = memref.reinterpret_cast %alloc_2219 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2221 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2221 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2220, %arg287 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2221 : memref<1x1536xf32>)
    %base_buffer_2222, %offset_2223, %sizes_2224, %strides_2225 = memref.extract_strided_metadata %arg286 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %155 = affine.apply #map10()[%strides_2225]
    %reinterpret_cast_2226 = memref.reinterpret_cast %base_buffer_2222 to offset: [%offset_2223], sizes: [1, 1536], strides: [%155, %strides_2225] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2227 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2226, %alloc_2221 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2227 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2228 = memref.reinterpret_cast %alloc_2227 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2229 = memref.reinterpret_cast %alloc_2219 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2230 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2230 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2229, %arg289 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2230 : memref<1x256xf32>)
    %base_buffer_2231, %offset_2232, %sizes_2233, %strides_2234 = memref.extract_strided_metadata %arg288 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %156 = affine.apply #map11()[%strides_2234]
    %reinterpret_cast_2235 = memref.reinterpret_cast %base_buffer_2231 to offset: [%offset_2232], sizes: [1, 256], strides: [%156, %strides_2234] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2236 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2235, %alloc_2230 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2236 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2237 = memref.reinterpret_cast %alloc_2236 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2238 = memref.reinterpret_cast %alloc_2219 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2239 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2239 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2238, %arg291 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2239 : memref<1x256xf32>)
    %base_buffer_2240, %offset_2241, %sizes_2242, %strides_2243 = memref.extract_strided_metadata %arg290 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %157 = affine.apply #map11()[%strides_2243]
    %reinterpret_cast_2244 = memref.reinterpret_cast %base_buffer_2240 to offset: [%offset_2241], sizes: [1, 256], strides: [%157, %strides_2243] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2245 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2244, %alloc_2239 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2245 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2246 = memref.reinterpret_cast %alloc_2245 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2247 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_2248 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_2249 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2228, %reinterpret_cast_2247 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2249 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2250 = memref.reinterpret_cast %alloc_2227 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2251 = memref.reinterpret_cast %alloc_2227 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2252 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2251 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_2252 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2253 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_2254 = memref.reinterpret_cast %alloc_2253 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_2252, %reinterpret_cast_2254 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2255 = memref.reinterpret_cast %alloc_2253 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2250, %reinterpret_cast_2255 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2256 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2253, %reinterpret_cast_2248 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2256 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2257 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2249, %alloc_2256 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_2257 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2258 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2237, %reinterpret_cast_2247 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2258 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2259 = memref.reinterpret_cast %alloc_2236 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2260 = memref.reinterpret_cast %alloc_2236 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2261 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2260 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_2261 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2262 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_2263 = memref.reinterpret_cast %alloc_2262 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_2261, %reinterpret_cast_2263 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2264 = memref.reinterpret_cast %alloc_2262 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2259, %reinterpret_cast_2264 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2265 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2262, %reinterpret_cast_2248 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2265 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2266 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2258, %alloc_2265 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_2266 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2267 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg292, %alloc_2267 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2268, %offset_2269, %sizes_2270, %strides_2271 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2272 = memref.reinterpret_cast %base_buffer_2268 to offset: [%offset_2269], sizes: [1, 1, 1, 1], strides: [%strides_2271, %strides_2271, %strides_2271, %strides_2271] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2273 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2272, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2273 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_2266[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2273[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2267[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_2274 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg293, %alloc_2274 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2275, %offset_2276, %sizes_2277, %strides_2278 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2279 = memref.reinterpret_cast %base_buffer_2275 to offset: [%offset_2276], sizes: [1, 1, 1, 1], strides: [%strides_2278, %strides_2278, %strides_2278, %strides_2278] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2280 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2279, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2280 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_2246[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2280[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2274[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2281 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_2282 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2281, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_2282 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_2283 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_2284 = memref.alloc() : memref<1x12x1xf32>
    %alloc_2285 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_2285[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_2257[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_2267[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_2282[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_2274[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_2285[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_2285[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_2284[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_2285[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_2283[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2286 = memref.reinterpret_cast %alloc_2283 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_2287 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2287 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2286, %arg294 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2287 : memref<1x1536xf32>)
    %reinterpret_cast_2288 = memref.reinterpret_cast %alloc_2287 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2289 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2206, %reinterpret_cast_2288 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2289 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2290 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2289 : memref<1x1x1536xf32>) outs(%alloc_2290 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2291 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2291 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2290 : memref<1x1x1536xf32>) outs(%alloc_2291 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2292 = memref.reinterpret_cast %alloc_2291 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2293 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2292, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2293 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2294 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2293, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2294 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2295 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2294 : memref<1x1x1xf32>) outs(%alloc_2295 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2296 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2289, %alloc_2295 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2296 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2297, %offset_2298, %sizes_2299, %strides_2300 = memref.extract_strided_metadata %arg295 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %158 = affine.apply #map10()[%strides_2300]
    %159 = affine.apply #map10()[%strides_2300]
    %reinterpret_cast_2301 = memref.reinterpret_cast %base_buffer_2297 to offset: [%offset_2298], sizes: [1, 1, 1536], strides: [%158, %159, %strides_2300] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2302 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2301, %alloc_2296 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2302 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2303 = memref.reinterpret_cast %alloc_2302 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2304 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2304 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2303, %arg296 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2304 : memref<1x8960xf32>)
    %reinterpret_cast_2305 = memref.reinterpret_cast %alloc_2304 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2306 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2305 : memref<1x1x8960xf32>) outs(%alloc_2306 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_2307 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2305, %alloc_2306 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2307 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2308 = memref.reinterpret_cast %alloc_2302 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2309 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2309 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2308, %arg297 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2309 : memref<1x8960xf32>)
    %reinterpret_cast_2310 = memref.reinterpret_cast %alloc_2309 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2311 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2307, %reinterpret_cast_2310 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2311 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2312 = memref.reinterpret_cast %alloc_2311 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_2313 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2313 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2312, %arg298 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2313 : memref<1x1536xf32>)
    %reinterpret_cast_2314 = memref.reinterpret_cast %alloc_2313 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2315 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2289, %reinterpret_cast_2314 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2315 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2316 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2315 : memref<1x1x1536xf32>) outs(%alloc_2316 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2317 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2317 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2316 : memref<1x1x1536xf32>) outs(%alloc_2317 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2318 = memref.reinterpret_cast %alloc_2317 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2319 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2318, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2319 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2320 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2319, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2320 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2321 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2320 : memref<1x1x1xf32>) outs(%alloc_2321 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2322 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2315, %alloc_2321 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2322 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2323, %offset_2324, %sizes_2325, %strides_2326 = memref.extract_strided_metadata %arg299 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %160 = affine.apply #map10()[%strides_2326]
    %161 = affine.apply #map10()[%strides_2326]
    %reinterpret_cast_2327 = memref.reinterpret_cast %base_buffer_2323 to offset: [%offset_2324], sizes: [1, 1, 1536], strides: [%160, %161, %strides_2326] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2328 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2327, %alloc_2322 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2328 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2329 = memref.reinterpret_cast %alloc_2328 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2330 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2330 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2329, %arg301 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2330 : memref<1x1536xf32>)
    %base_buffer_2331, %offset_2332, %sizes_2333, %strides_2334 = memref.extract_strided_metadata %arg300 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %162 = affine.apply #map10()[%strides_2334]
    %reinterpret_cast_2335 = memref.reinterpret_cast %base_buffer_2331 to offset: [%offset_2332], sizes: [1, 1536], strides: [%162, %strides_2334] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2336 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2335, %alloc_2330 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2336 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2337 = memref.reinterpret_cast %alloc_2336 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2338 = memref.reinterpret_cast %alloc_2328 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2339 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2339 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2338, %arg303 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2339 : memref<1x256xf32>)
    %base_buffer_2340, %offset_2341, %sizes_2342, %strides_2343 = memref.extract_strided_metadata %arg302 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %163 = affine.apply #map11()[%strides_2343]
    %reinterpret_cast_2344 = memref.reinterpret_cast %base_buffer_2340 to offset: [%offset_2341], sizes: [1, 256], strides: [%163, %strides_2343] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2345 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2344, %alloc_2339 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2345 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2346 = memref.reinterpret_cast %alloc_2345 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2347 = memref.reinterpret_cast %alloc_2328 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2348 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2348 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2347, %arg305 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2348 : memref<1x256xf32>)
    %base_buffer_2349, %offset_2350, %sizes_2351, %strides_2352 = memref.extract_strided_metadata %arg304 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %164 = affine.apply #map11()[%strides_2352]
    %reinterpret_cast_2353 = memref.reinterpret_cast %base_buffer_2349 to offset: [%offset_2350], sizes: [1, 256], strides: [%164, %strides_2352] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2354 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2353, %alloc_2348 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2354 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2355 = memref.reinterpret_cast %alloc_2354 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2356 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_2357 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_2358 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2337, %reinterpret_cast_2356 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2358 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2359 = memref.reinterpret_cast %alloc_2336 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2360 = memref.reinterpret_cast %alloc_2336 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2361 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2360 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_2361 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2362 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_2363 = memref.reinterpret_cast %alloc_2362 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_2361, %reinterpret_cast_2363 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2364 = memref.reinterpret_cast %alloc_2362 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2359, %reinterpret_cast_2364 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2365 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2362, %reinterpret_cast_2357 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2365 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2366 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2358, %alloc_2365 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_2366 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2367 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2346, %reinterpret_cast_2356 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2367 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2368 = memref.reinterpret_cast %alloc_2345 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2369 = memref.reinterpret_cast %alloc_2345 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2370 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2369 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_2370 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2371 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_2372 = memref.reinterpret_cast %alloc_2371 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_2370, %reinterpret_cast_2372 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2373 = memref.reinterpret_cast %alloc_2371 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2368, %reinterpret_cast_2373 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2374 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2371, %reinterpret_cast_2357 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2374 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2375 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2367, %alloc_2374 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_2375 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2376 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg306, %alloc_2376 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2377, %offset_2378, %sizes_2379, %strides_2380 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2381 = memref.reinterpret_cast %base_buffer_2377 to offset: [%offset_2378], sizes: [1, 1, 1, 1], strides: [%strides_2380, %strides_2380, %strides_2380, %strides_2380] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2382 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2381, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2382 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_2375[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2382[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2376[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_2383 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg307, %alloc_2383 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2384, %offset_2385, %sizes_2386, %strides_2387 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2388 = memref.reinterpret_cast %base_buffer_2384 to offset: [%offset_2385], sizes: [1, 1, 1, 1], strides: [%strides_2387, %strides_2387, %strides_2387, %strides_2387] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2389 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2388, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2389 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_2355[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2389[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2383[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2390 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_2391 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2390, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_2391 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_2392 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_2393 = memref.alloc() : memref<1x12x1xf32>
    %alloc_2394 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_2394[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_2366[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_2376[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_2391[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_2383[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_2394[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_2394[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_2393[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_2394[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_2392[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2395 = memref.reinterpret_cast %alloc_2392 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_2396 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2396 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2395, %arg308 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2396 : memref<1x1536xf32>)
    %reinterpret_cast_2397 = memref.reinterpret_cast %alloc_2396 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2398 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2315, %reinterpret_cast_2397 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2398 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2399 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2398 : memref<1x1x1536xf32>) outs(%alloc_2399 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2400 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2400 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2399 : memref<1x1x1536xf32>) outs(%alloc_2400 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2401 = memref.reinterpret_cast %alloc_2400 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2402 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2401, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2402 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2403 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2402, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2403 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2404 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2403 : memref<1x1x1xf32>) outs(%alloc_2404 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2405 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2398, %alloc_2404 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2405 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2406, %offset_2407, %sizes_2408, %strides_2409 = memref.extract_strided_metadata %arg309 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %165 = affine.apply #map10()[%strides_2409]
    %166 = affine.apply #map10()[%strides_2409]
    %reinterpret_cast_2410 = memref.reinterpret_cast %base_buffer_2406 to offset: [%offset_2407], sizes: [1, 1, 1536], strides: [%165, %166, %strides_2409] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2411 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2410, %alloc_2405 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2411 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2412 = memref.reinterpret_cast %alloc_2411 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2413 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2413 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2412, %arg310 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2413 : memref<1x8960xf32>)
    %reinterpret_cast_2414 = memref.reinterpret_cast %alloc_2413 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2415 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2414 : memref<1x1x8960xf32>) outs(%alloc_2415 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_2416 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2414, %alloc_2415 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2416 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2417 = memref.reinterpret_cast %alloc_2411 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2418 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2418 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2417, %arg311 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2418 : memref<1x8960xf32>)
    %reinterpret_cast_2419 = memref.reinterpret_cast %alloc_2418 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2420 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2416, %reinterpret_cast_2419 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2420 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2421 = memref.reinterpret_cast %alloc_2420 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_2422 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2422 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2421, %arg312 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2422 : memref<1x1536xf32>)
    %reinterpret_cast_2423 = memref.reinterpret_cast %alloc_2422 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2424 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2398, %reinterpret_cast_2423 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2424 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2425 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2424 : memref<1x1x1536xf32>) outs(%alloc_2425 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2426 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2426 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2425 : memref<1x1x1536xf32>) outs(%alloc_2426 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2427 = memref.reinterpret_cast %alloc_2426 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2428 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2427, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2428 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2429 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2428, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2429 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2430 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2429 : memref<1x1x1xf32>) outs(%alloc_2430 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2431 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2424, %alloc_2430 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2431 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2432, %offset_2433, %sizes_2434, %strides_2435 = memref.extract_strided_metadata %arg313 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %167 = affine.apply #map10()[%strides_2435]
    %168 = affine.apply #map10()[%strides_2435]
    %reinterpret_cast_2436 = memref.reinterpret_cast %base_buffer_2432 to offset: [%offset_2433], sizes: [1, 1, 1536], strides: [%167, %168, %strides_2435] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2437 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2436, %alloc_2431 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2437 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2438 = memref.reinterpret_cast %alloc_2437 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2439 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2439 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2438, %arg315 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2439 : memref<1x1536xf32>)
    %base_buffer_2440, %offset_2441, %sizes_2442, %strides_2443 = memref.extract_strided_metadata %arg314 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %169 = affine.apply #map10()[%strides_2443]
    %reinterpret_cast_2444 = memref.reinterpret_cast %base_buffer_2440 to offset: [%offset_2441], sizes: [1, 1536], strides: [%169, %strides_2443] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2445 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2444, %alloc_2439 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2445 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2446 = memref.reinterpret_cast %alloc_2445 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2447 = memref.reinterpret_cast %alloc_2437 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2448 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2448 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2447, %arg317 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2448 : memref<1x256xf32>)
    %base_buffer_2449, %offset_2450, %sizes_2451, %strides_2452 = memref.extract_strided_metadata %arg316 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %170 = affine.apply #map11()[%strides_2452]
    %reinterpret_cast_2453 = memref.reinterpret_cast %base_buffer_2449 to offset: [%offset_2450], sizes: [1, 256], strides: [%170, %strides_2452] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2454 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2453, %alloc_2448 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2454 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2455 = memref.reinterpret_cast %alloc_2454 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2456 = memref.reinterpret_cast %alloc_2437 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2457 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2457 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2456, %arg319 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2457 : memref<1x256xf32>)
    %base_buffer_2458, %offset_2459, %sizes_2460, %strides_2461 = memref.extract_strided_metadata %arg318 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %171 = affine.apply #map11()[%strides_2461]
    %reinterpret_cast_2462 = memref.reinterpret_cast %base_buffer_2458 to offset: [%offset_2459], sizes: [1, 256], strides: [%171, %strides_2461] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2463 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2462, %alloc_2457 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2463 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2464 = memref.reinterpret_cast %alloc_2463 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2465 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_2466 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_2467 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2446, %reinterpret_cast_2465 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2467 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2468 = memref.reinterpret_cast %alloc_2445 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2469 = memref.reinterpret_cast %alloc_2445 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2470 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2469 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_2470 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2471 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_2472 = memref.reinterpret_cast %alloc_2471 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_2470, %reinterpret_cast_2472 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2473 = memref.reinterpret_cast %alloc_2471 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2468, %reinterpret_cast_2473 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2474 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2471, %reinterpret_cast_2466 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2474 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2475 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2467, %alloc_2474 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_2475 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2476 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2455, %reinterpret_cast_2465 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2476 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2477 = memref.reinterpret_cast %alloc_2454 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2478 = memref.reinterpret_cast %alloc_2454 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2479 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2478 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_2479 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2480 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_2481 = memref.reinterpret_cast %alloc_2480 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_2479, %reinterpret_cast_2481 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2482 = memref.reinterpret_cast %alloc_2480 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2477, %reinterpret_cast_2482 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2483 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2480, %reinterpret_cast_2466 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2483 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2484 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2476, %alloc_2483 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_2484 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2485 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg320, %alloc_2485 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2486, %offset_2487, %sizes_2488, %strides_2489 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2490 = memref.reinterpret_cast %base_buffer_2486 to offset: [%offset_2487], sizes: [1, 1, 1, 1], strides: [%strides_2489, %strides_2489, %strides_2489, %strides_2489] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2491 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2490, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2491 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_2484[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2491[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2485[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_2492 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg321, %alloc_2492 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2493, %offset_2494, %sizes_2495, %strides_2496 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2497 = memref.reinterpret_cast %base_buffer_2493 to offset: [%offset_2494], sizes: [1, 1, 1, 1], strides: [%strides_2496, %strides_2496, %strides_2496, %strides_2496] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2498 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2497, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2498 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_2464[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2498[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2492[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2499 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_2500 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2499, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_2500 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_2501 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_2502 = memref.alloc() : memref<1x12x1xf32>
    %alloc_2503 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_2503[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_2475[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_2485[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_2500[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_2492[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_2503[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_2503[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_2502[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_2503[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_2501[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2504 = memref.reinterpret_cast %alloc_2501 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_2505 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2505 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2504, %arg322 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2505 : memref<1x1536xf32>)
    %reinterpret_cast_2506 = memref.reinterpret_cast %alloc_2505 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2507 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2424, %reinterpret_cast_2506 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2507 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2508 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2507 : memref<1x1x1536xf32>) outs(%alloc_2508 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2509 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2509 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2508 : memref<1x1x1536xf32>) outs(%alloc_2509 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2510 = memref.reinterpret_cast %alloc_2509 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2511 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2510, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2511 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2512 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2511, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2512 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2513 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2512 : memref<1x1x1xf32>) outs(%alloc_2513 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2514 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2507, %alloc_2513 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2514 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2515, %offset_2516, %sizes_2517, %strides_2518 = memref.extract_strided_metadata %arg323 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %172 = affine.apply #map10()[%strides_2518]
    %173 = affine.apply #map10()[%strides_2518]
    %reinterpret_cast_2519 = memref.reinterpret_cast %base_buffer_2515 to offset: [%offset_2516], sizes: [1, 1, 1536], strides: [%172, %173, %strides_2518] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2520 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2519, %alloc_2514 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2520 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2521 = memref.reinterpret_cast %alloc_2520 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2522 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2522 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2521, %arg324 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2522 : memref<1x8960xf32>)
    %reinterpret_cast_2523 = memref.reinterpret_cast %alloc_2522 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2524 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2523 : memref<1x1x8960xf32>) outs(%alloc_2524 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_2525 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2523, %alloc_2524 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2525 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2526 = memref.reinterpret_cast %alloc_2520 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2527 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2527 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2526, %arg325 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2527 : memref<1x8960xf32>)
    %reinterpret_cast_2528 = memref.reinterpret_cast %alloc_2527 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2529 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2525, %reinterpret_cast_2528 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2529 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2530 = memref.reinterpret_cast %alloc_2529 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_2531 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2531 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2530, %arg326 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2531 : memref<1x1536xf32>)
    %reinterpret_cast_2532 = memref.reinterpret_cast %alloc_2531 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2533 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2507, %reinterpret_cast_2532 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2533 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2534 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2533 : memref<1x1x1536xf32>) outs(%alloc_2534 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2535 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2535 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2534 : memref<1x1x1536xf32>) outs(%alloc_2535 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2536 = memref.reinterpret_cast %alloc_2535 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2537 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2536, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2537 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2538 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2537, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2538 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2539 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2538 : memref<1x1x1xf32>) outs(%alloc_2539 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2540 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2533, %alloc_2539 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2540 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2541, %offset_2542, %sizes_2543, %strides_2544 = memref.extract_strided_metadata %arg327 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %174 = affine.apply #map10()[%strides_2544]
    %175 = affine.apply #map10()[%strides_2544]
    %reinterpret_cast_2545 = memref.reinterpret_cast %base_buffer_2541 to offset: [%offset_2542], sizes: [1, 1, 1536], strides: [%174, %175, %strides_2544] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2546 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2545, %alloc_2540 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2546 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2547 = memref.reinterpret_cast %alloc_2546 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2548 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2548 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2547, %arg329 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2548 : memref<1x1536xf32>)
    %base_buffer_2549, %offset_2550, %sizes_2551, %strides_2552 = memref.extract_strided_metadata %arg328 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %176 = affine.apply #map10()[%strides_2552]
    %reinterpret_cast_2553 = memref.reinterpret_cast %base_buffer_2549 to offset: [%offset_2550], sizes: [1, 1536], strides: [%176, %strides_2552] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2554 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2553, %alloc_2548 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2554 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2555 = memref.reinterpret_cast %alloc_2554 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2556 = memref.reinterpret_cast %alloc_2546 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2557 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2557 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2556, %arg331 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2557 : memref<1x256xf32>)
    %base_buffer_2558, %offset_2559, %sizes_2560, %strides_2561 = memref.extract_strided_metadata %arg330 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %177 = affine.apply #map11()[%strides_2561]
    %reinterpret_cast_2562 = memref.reinterpret_cast %base_buffer_2558 to offset: [%offset_2559], sizes: [1, 256], strides: [%177, %strides_2561] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2563 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2562, %alloc_2557 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2563 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2564 = memref.reinterpret_cast %alloc_2563 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2565 = memref.reinterpret_cast %alloc_2546 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2566 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2566 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2565, %arg333 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2566 : memref<1x256xf32>)
    %base_buffer_2567, %offset_2568, %sizes_2569, %strides_2570 = memref.extract_strided_metadata %arg332 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %178 = affine.apply #map11()[%strides_2570]
    %reinterpret_cast_2571 = memref.reinterpret_cast %base_buffer_2567 to offset: [%offset_2568], sizes: [1, 256], strides: [%178, %strides_2570] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2572 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2571, %alloc_2566 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2572 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2573 = memref.reinterpret_cast %alloc_2572 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2574 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_2575 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_2576 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2555, %reinterpret_cast_2574 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2576 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2577 = memref.reinterpret_cast %alloc_2554 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2578 = memref.reinterpret_cast %alloc_2554 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2579 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2578 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_2579 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2580 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_2581 = memref.reinterpret_cast %alloc_2580 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_2579, %reinterpret_cast_2581 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2582 = memref.reinterpret_cast %alloc_2580 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2577, %reinterpret_cast_2582 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2583 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2580, %reinterpret_cast_2575 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2583 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2584 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2576, %alloc_2583 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_2584 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2585 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2564, %reinterpret_cast_2574 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2585 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2586 = memref.reinterpret_cast %alloc_2563 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2587 = memref.reinterpret_cast %alloc_2563 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2588 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2587 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_2588 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2589 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_2590 = memref.reinterpret_cast %alloc_2589 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_2588, %reinterpret_cast_2590 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2591 = memref.reinterpret_cast %alloc_2589 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2586, %reinterpret_cast_2591 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2592 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2589, %reinterpret_cast_2575 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2592 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2593 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2585, %alloc_2592 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_2593 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2594 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg334, %alloc_2594 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2595, %offset_2596, %sizes_2597, %strides_2598 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2599 = memref.reinterpret_cast %base_buffer_2595 to offset: [%offset_2596], sizes: [1, 1, 1, 1], strides: [%strides_2598, %strides_2598, %strides_2598, %strides_2598] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2600 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2599, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2600 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_2593[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2600[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2594[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_2601 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg335, %alloc_2601 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2602, %offset_2603, %sizes_2604, %strides_2605 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2606 = memref.reinterpret_cast %base_buffer_2602 to offset: [%offset_2603], sizes: [1, 1, 1, 1], strides: [%strides_2605, %strides_2605, %strides_2605, %strides_2605] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2607 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2606, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2607 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_2573[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2607[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2601[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2608 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_2609 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2608, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_2609 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_2610 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_2611 = memref.alloc() : memref<1x12x1xf32>
    %alloc_2612 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_2612[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_2584[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_2594[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_2609[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_2601[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_2612[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_2612[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_2611[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_2612[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_2610[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2613 = memref.reinterpret_cast %alloc_2610 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_2614 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2614 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2613, %arg336 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2614 : memref<1x1536xf32>)
    %reinterpret_cast_2615 = memref.reinterpret_cast %alloc_2614 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2616 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2533, %reinterpret_cast_2615 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2616 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2617 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2616 : memref<1x1x1536xf32>) outs(%alloc_2617 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2618 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2618 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2617 : memref<1x1x1536xf32>) outs(%alloc_2618 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2619 = memref.reinterpret_cast %alloc_2618 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2620 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2619, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2620 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2621 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2620, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2621 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2622 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2621 : memref<1x1x1xf32>) outs(%alloc_2622 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2623 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2616, %alloc_2622 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2623 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2624, %offset_2625, %sizes_2626, %strides_2627 = memref.extract_strided_metadata %arg337 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %179 = affine.apply #map10()[%strides_2627]
    %180 = affine.apply #map10()[%strides_2627]
    %reinterpret_cast_2628 = memref.reinterpret_cast %base_buffer_2624 to offset: [%offset_2625], sizes: [1, 1, 1536], strides: [%179, %180, %strides_2627] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2629 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2628, %alloc_2623 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2629 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2630 = memref.reinterpret_cast %alloc_2629 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2631 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2631 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2630, %arg338 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2631 : memref<1x8960xf32>)
    %reinterpret_cast_2632 = memref.reinterpret_cast %alloc_2631 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2633 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2632 : memref<1x1x8960xf32>) outs(%alloc_2633 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_2634 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2632, %alloc_2633 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2634 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2635 = memref.reinterpret_cast %alloc_2629 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2636 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2636 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2635, %arg339 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2636 : memref<1x8960xf32>)
    %reinterpret_cast_2637 = memref.reinterpret_cast %alloc_2636 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2638 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2634, %reinterpret_cast_2637 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2638 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2639 = memref.reinterpret_cast %alloc_2638 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_2640 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2640 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2639, %arg340 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2640 : memref<1x1536xf32>)
    %reinterpret_cast_2641 = memref.reinterpret_cast %alloc_2640 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2642 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2616, %reinterpret_cast_2641 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2642 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2643 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2642 : memref<1x1x1536xf32>) outs(%alloc_2643 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2644 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2644 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2643 : memref<1x1x1536xf32>) outs(%alloc_2644 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2645 = memref.reinterpret_cast %alloc_2644 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2646 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2645, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2646 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2647 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2646, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2647 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2648 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2647 : memref<1x1x1xf32>) outs(%alloc_2648 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2649 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2642, %alloc_2648 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2649 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2650, %offset_2651, %sizes_2652, %strides_2653 = memref.extract_strided_metadata %arg341 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %181 = affine.apply #map10()[%strides_2653]
    %182 = affine.apply #map10()[%strides_2653]
    %reinterpret_cast_2654 = memref.reinterpret_cast %base_buffer_2650 to offset: [%offset_2651], sizes: [1, 1, 1536], strides: [%181, %182, %strides_2653] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2655 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2654, %alloc_2649 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2655 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2656 = memref.reinterpret_cast %alloc_2655 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2657 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2657 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2656, %arg343 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2657 : memref<1x1536xf32>)
    %base_buffer_2658, %offset_2659, %sizes_2660, %strides_2661 = memref.extract_strided_metadata %arg342 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %183 = affine.apply #map10()[%strides_2661]
    %reinterpret_cast_2662 = memref.reinterpret_cast %base_buffer_2658 to offset: [%offset_2659], sizes: [1, 1536], strides: [%183, %strides_2661] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2663 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2662, %alloc_2657 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2663 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2664 = memref.reinterpret_cast %alloc_2663 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2665 = memref.reinterpret_cast %alloc_2655 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2666 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2666 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2665, %arg345 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2666 : memref<1x256xf32>)
    %base_buffer_2667, %offset_2668, %sizes_2669, %strides_2670 = memref.extract_strided_metadata %arg344 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %184 = affine.apply #map11()[%strides_2670]
    %reinterpret_cast_2671 = memref.reinterpret_cast %base_buffer_2667 to offset: [%offset_2668], sizes: [1, 256], strides: [%184, %strides_2670] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2672 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2671, %alloc_2666 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2672 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2673 = memref.reinterpret_cast %alloc_2672 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2674 = memref.reinterpret_cast %alloc_2655 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2675 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2675 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2674, %arg347 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2675 : memref<1x256xf32>)
    %base_buffer_2676, %offset_2677, %sizes_2678, %strides_2679 = memref.extract_strided_metadata %arg346 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %185 = affine.apply #map11()[%strides_2679]
    %reinterpret_cast_2680 = memref.reinterpret_cast %base_buffer_2676 to offset: [%offset_2677], sizes: [1, 256], strides: [%185, %strides_2679] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2681 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2680, %alloc_2675 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2681 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2682 = memref.reinterpret_cast %alloc_2681 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2683 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_2684 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_2685 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2664, %reinterpret_cast_2683 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2685 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2686 = memref.reinterpret_cast %alloc_2663 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2687 = memref.reinterpret_cast %alloc_2663 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2688 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2687 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_2688 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2689 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_2690 = memref.reinterpret_cast %alloc_2689 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_2688, %reinterpret_cast_2690 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2691 = memref.reinterpret_cast %alloc_2689 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2686, %reinterpret_cast_2691 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2692 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2689, %reinterpret_cast_2684 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2692 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2693 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2685, %alloc_2692 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_2693 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2694 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2673, %reinterpret_cast_2683 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2694 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2695 = memref.reinterpret_cast %alloc_2672 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2696 = memref.reinterpret_cast %alloc_2672 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2697 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2696 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_2697 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2698 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_2699 = memref.reinterpret_cast %alloc_2698 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_2697, %reinterpret_cast_2699 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2700 = memref.reinterpret_cast %alloc_2698 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2695, %reinterpret_cast_2700 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2701 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2698, %reinterpret_cast_2684 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2701 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2702 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2694, %alloc_2701 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_2702 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2703 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg348, %alloc_2703 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2704, %offset_2705, %sizes_2706, %strides_2707 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2708 = memref.reinterpret_cast %base_buffer_2704 to offset: [%offset_2705], sizes: [1, 1, 1, 1], strides: [%strides_2707, %strides_2707, %strides_2707, %strides_2707] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2709 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2708, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2709 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_2702[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2709[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2703[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_2710 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg349, %alloc_2710 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2711, %offset_2712, %sizes_2713, %strides_2714 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2715 = memref.reinterpret_cast %base_buffer_2711 to offset: [%offset_2712], sizes: [1, 1, 1, 1], strides: [%strides_2714, %strides_2714, %strides_2714, %strides_2714] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2716 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2715, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2716 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_2682[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2716[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2710[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2717 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_2718 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2717, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_2718 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_2719 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_2720 = memref.alloc() : memref<1x12x1xf32>
    %alloc_2721 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_2721[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_2693[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_2703[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_2718[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_2710[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_2721[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_2721[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_2720[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_2721[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_2719[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2722 = memref.reinterpret_cast %alloc_2719 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_2723 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2723 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2722, %arg350 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2723 : memref<1x1536xf32>)
    %reinterpret_cast_2724 = memref.reinterpret_cast %alloc_2723 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2725 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2642, %reinterpret_cast_2724 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2725 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2726 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2725 : memref<1x1x1536xf32>) outs(%alloc_2726 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2727 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2727 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2726 : memref<1x1x1536xf32>) outs(%alloc_2727 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2728 = memref.reinterpret_cast %alloc_2727 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2729 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2728, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2729 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2730 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2729, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2730 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2731 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2730 : memref<1x1x1xf32>) outs(%alloc_2731 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2732 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2725, %alloc_2731 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2732 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2733, %offset_2734, %sizes_2735, %strides_2736 = memref.extract_strided_metadata %arg351 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %186 = affine.apply #map10()[%strides_2736]
    %187 = affine.apply #map10()[%strides_2736]
    %reinterpret_cast_2737 = memref.reinterpret_cast %base_buffer_2733 to offset: [%offset_2734], sizes: [1, 1, 1536], strides: [%186, %187, %strides_2736] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2738 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2737, %alloc_2732 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2738 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2739 = memref.reinterpret_cast %alloc_2738 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2740 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2740 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2739, %arg352 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2740 : memref<1x8960xf32>)
    %reinterpret_cast_2741 = memref.reinterpret_cast %alloc_2740 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2742 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2741 : memref<1x1x8960xf32>) outs(%alloc_2742 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_2743 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2741, %alloc_2742 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2743 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2744 = memref.reinterpret_cast %alloc_2738 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2745 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2745 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2744, %arg353 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2745 : memref<1x8960xf32>)
    %reinterpret_cast_2746 = memref.reinterpret_cast %alloc_2745 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2747 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2743, %reinterpret_cast_2746 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2747 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2748 = memref.reinterpret_cast %alloc_2747 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_2749 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2749 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2748, %arg354 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2749 : memref<1x1536xf32>)
    %reinterpret_cast_2750 = memref.reinterpret_cast %alloc_2749 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2751 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2725, %reinterpret_cast_2750 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2751 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2752 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2751 : memref<1x1x1536xf32>) outs(%alloc_2752 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2753 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2753 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2752 : memref<1x1x1536xf32>) outs(%alloc_2753 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2754 = memref.reinterpret_cast %alloc_2753 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2755 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2754, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2755 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2756 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2755, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2756 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2757 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2756 : memref<1x1x1xf32>) outs(%alloc_2757 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2758 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2751, %alloc_2757 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2758 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2759, %offset_2760, %sizes_2761, %strides_2762 = memref.extract_strided_metadata %arg355 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %188 = affine.apply #map10()[%strides_2762]
    %189 = affine.apply #map10()[%strides_2762]
    %reinterpret_cast_2763 = memref.reinterpret_cast %base_buffer_2759 to offset: [%offset_2760], sizes: [1, 1, 1536], strides: [%188, %189, %strides_2762] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2764 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2763, %alloc_2758 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2764 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2765 = memref.reinterpret_cast %alloc_2764 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2766 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2766 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2765, %arg357 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2766 : memref<1x1536xf32>)
    %base_buffer_2767, %offset_2768, %sizes_2769, %strides_2770 = memref.extract_strided_metadata %arg356 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %190 = affine.apply #map10()[%strides_2770]
    %reinterpret_cast_2771 = memref.reinterpret_cast %base_buffer_2767 to offset: [%offset_2768], sizes: [1, 1536], strides: [%190, %strides_2770] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2772 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2771, %alloc_2766 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2772 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2773 = memref.reinterpret_cast %alloc_2772 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2774 = memref.reinterpret_cast %alloc_2764 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2775 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2775 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2774, %arg359 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2775 : memref<1x256xf32>)
    %base_buffer_2776, %offset_2777, %sizes_2778, %strides_2779 = memref.extract_strided_metadata %arg358 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %191 = affine.apply #map11()[%strides_2779]
    %reinterpret_cast_2780 = memref.reinterpret_cast %base_buffer_2776 to offset: [%offset_2777], sizes: [1, 256], strides: [%191, %strides_2779] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2781 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2780, %alloc_2775 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2781 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2782 = memref.reinterpret_cast %alloc_2781 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2783 = memref.reinterpret_cast %alloc_2764 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2784 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2784 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2783, %arg361 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2784 : memref<1x256xf32>)
    %base_buffer_2785, %offset_2786, %sizes_2787, %strides_2788 = memref.extract_strided_metadata %arg360 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %192 = affine.apply #map11()[%strides_2788]
    %reinterpret_cast_2789 = memref.reinterpret_cast %base_buffer_2785 to offset: [%offset_2786], sizes: [1, 256], strides: [%192, %strides_2788] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2790 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2789, %alloc_2784 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2790 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2791 = memref.reinterpret_cast %alloc_2790 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2792 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_2793 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_2794 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2773, %reinterpret_cast_2792 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2794 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2795 = memref.reinterpret_cast %alloc_2772 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2796 = memref.reinterpret_cast %alloc_2772 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2797 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2796 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_2797 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2798 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_2799 = memref.reinterpret_cast %alloc_2798 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_2797, %reinterpret_cast_2799 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2800 = memref.reinterpret_cast %alloc_2798 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2795, %reinterpret_cast_2800 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2801 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2798, %reinterpret_cast_2793 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2801 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2802 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2794, %alloc_2801 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_2802 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2803 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2782, %reinterpret_cast_2792 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2803 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2804 = memref.reinterpret_cast %alloc_2781 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2805 = memref.reinterpret_cast %alloc_2781 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2806 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2805 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_2806 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2807 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_2808 = memref.reinterpret_cast %alloc_2807 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_2806, %reinterpret_cast_2808 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2809 = memref.reinterpret_cast %alloc_2807 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2804, %reinterpret_cast_2809 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2810 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2807, %reinterpret_cast_2793 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2810 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2811 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2803, %alloc_2810 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_2811 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2812 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg362, %alloc_2812 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2813, %offset_2814, %sizes_2815, %strides_2816 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2817 = memref.reinterpret_cast %base_buffer_2813 to offset: [%offset_2814], sizes: [1, 1, 1, 1], strides: [%strides_2816, %strides_2816, %strides_2816, %strides_2816] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2818 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2817, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2818 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_2811[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2818[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2812[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_2819 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg363, %alloc_2819 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2820, %offset_2821, %sizes_2822, %strides_2823 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2824 = memref.reinterpret_cast %base_buffer_2820 to offset: [%offset_2821], sizes: [1, 1, 1, 1], strides: [%strides_2823, %strides_2823, %strides_2823, %strides_2823] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2825 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2824, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2825 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_2791[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2825[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2819[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2826 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_2827 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2826, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_2827 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_2828 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_2829 = memref.alloc() : memref<1x12x1xf32>
    %alloc_2830 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_2830[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_2802[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_2812[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_2827[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_2819[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_2830[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_2830[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_2829[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_2830[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_2828[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2831 = memref.reinterpret_cast %alloc_2828 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_2832 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2832 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2831, %arg364 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2832 : memref<1x1536xf32>)
    %reinterpret_cast_2833 = memref.reinterpret_cast %alloc_2832 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2834 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2751, %reinterpret_cast_2833 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2834 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2835 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2834 : memref<1x1x1536xf32>) outs(%alloc_2835 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2836 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2836 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2835 : memref<1x1x1536xf32>) outs(%alloc_2836 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2837 = memref.reinterpret_cast %alloc_2836 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2838 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2837, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2838 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2839 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2838, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2839 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2840 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2839 : memref<1x1x1xf32>) outs(%alloc_2840 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2841 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2834, %alloc_2840 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2841 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2842, %offset_2843, %sizes_2844, %strides_2845 = memref.extract_strided_metadata %arg365 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %193 = affine.apply #map10()[%strides_2845]
    %194 = affine.apply #map10()[%strides_2845]
    %reinterpret_cast_2846 = memref.reinterpret_cast %base_buffer_2842 to offset: [%offset_2843], sizes: [1, 1, 1536], strides: [%193, %194, %strides_2845] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2847 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2846, %alloc_2841 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2847 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2848 = memref.reinterpret_cast %alloc_2847 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2849 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2849 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2848, %arg366 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2849 : memref<1x8960xf32>)
    %reinterpret_cast_2850 = memref.reinterpret_cast %alloc_2849 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2851 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2850 : memref<1x1x8960xf32>) outs(%alloc_2851 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_2852 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2850, %alloc_2851 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2852 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2853 = memref.reinterpret_cast %alloc_2847 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2854 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2854 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2853, %arg367 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2854 : memref<1x8960xf32>)
    %reinterpret_cast_2855 = memref.reinterpret_cast %alloc_2854 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2856 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2852, %reinterpret_cast_2855 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2856 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2857 = memref.reinterpret_cast %alloc_2856 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_2858 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2858 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2857, %arg368 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2858 : memref<1x1536xf32>)
    %reinterpret_cast_2859 = memref.reinterpret_cast %alloc_2858 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2860 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2834, %reinterpret_cast_2859 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2860 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2861 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2860 : memref<1x1x1536xf32>) outs(%alloc_2861 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2862 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2862 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2861 : memref<1x1x1536xf32>) outs(%alloc_2862 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2863 = memref.reinterpret_cast %alloc_2862 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2864 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2863, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2864 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2865 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2864, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2865 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2866 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2865 : memref<1x1x1xf32>) outs(%alloc_2866 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2867 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2860, %alloc_2866 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2867 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2868, %offset_2869, %sizes_2870, %strides_2871 = memref.extract_strided_metadata %arg369 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %195 = affine.apply #map10()[%strides_2871]
    %196 = affine.apply #map10()[%strides_2871]
    %reinterpret_cast_2872 = memref.reinterpret_cast %base_buffer_2868 to offset: [%offset_2869], sizes: [1, 1, 1536], strides: [%195, %196, %strides_2871] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2873 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2872, %alloc_2867 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2873 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2874 = memref.reinterpret_cast %alloc_2873 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2875 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2875 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2874, %arg371 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2875 : memref<1x1536xf32>)
    %base_buffer_2876, %offset_2877, %sizes_2878, %strides_2879 = memref.extract_strided_metadata %arg370 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %197 = affine.apply #map10()[%strides_2879]
    %reinterpret_cast_2880 = memref.reinterpret_cast %base_buffer_2876 to offset: [%offset_2877], sizes: [1, 1536], strides: [%197, %strides_2879] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2881 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2880, %alloc_2875 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2881 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2882 = memref.reinterpret_cast %alloc_2881 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2883 = memref.reinterpret_cast %alloc_2873 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2884 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2884 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2883, %arg373 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2884 : memref<1x256xf32>)
    %base_buffer_2885, %offset_2886, %sizes_2887, %strides_2888 = memref.extract_strided_metadata %arg372 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %198 = affine.apply #map11()[%strides_2888]
    %reinterpret_cast_2889 = memref.reinterpret_cast %base_buffer_2885 to offset: [%offset_2886], sizes: [1, 256], strides: [%198, %strides_2888] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2890 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2889, %alloc_2884 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2890 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2891 = memref.reinterpret_cast %alloc_2890 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2892 = memref.reinterpret_cast %alloc_2873 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2893 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2893 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2892, %arg375 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2893 : memref<1x256xf32>)
    %base_buffer_2894, %offset_2895, %sizes_2896, %strides_2897 = memref.extract_strided_metadata %arg374 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %199 = affine.apply #map11()[%strides_2897]
    %reinterpret_cast_2898 = memref.reinterpret_cast %base_buffer_2894 to offset: [%offset_2895], sizes: [1, 256], strides: [%199, %strides_2897] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2899 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2898, %alloc_2893 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2899 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2900 = memref.reinterpret_cast %alloc_2899 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_2901 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_2902 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_2903 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2882, %reinterpret_cast_2901 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2903 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2904 = memref.reinterpret_cast %alloc_2881 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2905 = memref.reinterpret_cast %alloc_2881 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2906 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2905 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_2906 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2907 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_2908 = memref.reinterpret_cast %alloc_2907 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_2906, %reinterpret_cast_2908 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_2909 = memref.reinterpret_cast %alloc_2907 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2904, %reinterpret_cast_2909 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_2910 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2907, %reinterpret_cast_2902 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2910 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2911 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2903, %alloc_2910 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_2911 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2912 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2891, %reinterpret_cast_2901 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2912 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2913 = memref.reinterpret_cast %alloc_2890 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2914 = memref.reinterpret_cast %alloc_2890 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2915 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2914 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_2915 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2916 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_2917 = memref.reinterpret_cast %alloc_2916 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_2915, %reinterpret_cast_2917 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_2918 = memref.reinterpret_cast %alloc_2916 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_2913, %reinterpret_cast_2918 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_2919 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2916, %reinterpret_cast_2902 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_2919 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2920 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_2912, %alloc_2919 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_2920 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2921 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg376, %alloc_2921 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2922, %offset_2923, %sizes_2924, %strides_2925 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2926 = memref.reinterpret_cast %base_buffer_2922 to offset: [%offset_2923], sizes: [1, 1, 1, 1], strides: [%strides_2925, %strides_2925, %strides_2925, %strides_2925] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2927 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2926, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2927 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_2920[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2927[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2921[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_2928 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg377, %alloc_2928 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_2929, %offset_2930, %sizes_2931, %strides_2932 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_2933 = memref.reinterpret_cast %base_buffer_2929 to offset: [%offset_2930], sizes: [1, 1, 1, 1], strides: [%strides_2932, %strides_2932, %strides_2932, %strides_2932] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_2934 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2933, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_2934 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_2900[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_2934[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_2928[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2935 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_2936 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2935, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_2936 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_2937 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_2938 = memref.alloc() : memref<1x12x1xf32>
    %alloc_2939 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_2939[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_2911[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_2921[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_2936[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_2928[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_2939[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_2939[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_2938[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_2939[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_2937[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_2940 = memref.reinterpret_cast %alloc_2937 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_2941 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2941 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2940, %arg378 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2941 : memref<1x1536xf32>)
    %reinterpret_cast_2942 = memref.reinterpret_cast %alloc_2941 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2943 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2860, %reinterpret_cast_2942 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2943 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2944 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2943 : memref<1x1x1536xf32>) outs(%alloc_2944 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2945 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2945 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2944 : memref<1x1x1536xf32>) outs(%alloc_2945 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2946 = memref.reinterpret_cast %alloc_2945 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2947 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2946, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2947 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2948 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2947, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2948 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2949 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2948 : memref<1x1x1xf32>) outs(%alloc_2949 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2950 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2943, %alloc_2949 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2950 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2951, %offset_2952, %sizes_2953, %strides_2954 = memref.extract_strided_metadata %arg379 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %200 = affine.apply #map10()[%strides_2954]
    %201 = affine.apply #map10()[%strides_2954]
    %reinterpret_cast_2955 = memref.reinterpret_cast %base_buffer_2951 to offset: [%offset_2952], sizes: [1, 1, 1536], strides: [%200, %201, %strides_2954] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2956 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2955, %alloc_2950 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2956 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2957 = memref.reinterpret_cast %alloc_2956 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2958 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2958 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2957, %arg380 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2958 : memref<1x8960xf32>)
    %reinterpret_cast_2959 = memref.reinterpret_cast %alloc_2958 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2960 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2959 : memref<1x1x8960xf32>) outs(%alloc_2960 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_2961 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2959, %alloc_2960 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2961 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2962 = memref.reinterpret_cast %alloc_2956 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2963 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_2963 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2962, %arg381 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2963 : memref<1x8960xf32>)
    %reinterpret_cast_2964 = memref.reinterpret_cast %alloc_2963 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_2965 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2961, %reinterpret_cast_2964 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_2965 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2966 = memref.reinterpret_cast %alloc_2965 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_2967 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2967 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2966, %arg382 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2967 : memref<1x1536xf32>)
    %reinterpret_cast_2968 = memref.reinterpret_cast %alloc_2967 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_2969 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2943, %reinterpret_cast_2968 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_2969 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2970 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2969 : memref<1x1x1536xf32>) outs(%alloc_2970 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_2971 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_2971 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_2970 : memref<1x1x1536xf32>) outs(%alloc_2971 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_2972 = memref.reinterpret_cast %alloc_2971 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_2973 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2972, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2973 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2974 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2973, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_2974 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_2975 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2974 : memref<1x1x1xf32>) outs(%alloc_2975 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_2976 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2969, %alloc_2975 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_2976 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_2977, %offset_2978, %sizes_2979, %strides_2980 = memref.extract_strided_metadata %arg383 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %202 = affine.apply #map10()[%strides_2980]
    %203 = affine.apply #map10()[%strides_2980]
    %reinterpret_cast_2981 = memref.reinterpret_cast %base_buffer_2977 to offset: [%offset_2978], sizes: [1, 1, 1536], strides: [%202, %203, %strides_2980] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_2982 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2981, %alloc_2976 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_2982 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2983 = memref.reinterpret_cast %alloc_2982 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2984 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_2984 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2983, %arg385 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2984 : memref<1x1536xf32>)
    %base_buffer_2985, %offset_2986, %sizes_2987, %strides_2988 = memref.extract_strided_metadata %arg384 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %204 = affine.apply #map10()[%strides_2988]
    %reinterpret_cast_2989 = memref.reinterpret_cast %base_buffer_2985 to offset: [%offset_2986], sizes: [1, 1536], strides: [%204, %strides_2988] : memref<f32> to memref<1x1536xf32, strided<[?, ?], offset: ?>>
    %alloc_2990 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2989, %alloc_2984 : memref<1x1536xf32, strided<[?, ?], offset: ?>>, memref<1x1536xf32>) outs(%alloc_2990 : memref<1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_2991 = memref.reinterpret_cast %alloc_2990 to offset: [0], sizes: [1, 12, 1, 128], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x128xf32>
    %reinterpret_cast_2992 = memref.reinterpret_cast %alloc_2982 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_2993 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_2993 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_2992, %arg387 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_2993 : memref<1x256xf32>)
    %base_buffer_2994, %offset_2995, %sizes_2996, %strides_2997 = memref.extract_strided_metadata %arg386 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %205 = affine.apply #map11()[%strides_2997]
    %reinterpret_cast_2998 = memref.reinterpret_cast %base_buffer_2994 to offset: [%offset_2995], sizes: [1, 256], strides: [%205, %strides_2997] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_2999 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_2998, %alloc_2993 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_2999 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_3000 = memref.reinterpret_cast %alloc_2999 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_3001 = memref.reinterpret_cast %alloc_2982 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_3002 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    memref.copy %7, %alloc_3002 : memref<1x256xf32> to memref<1x256xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_3001, %arg389 : memref<1x1536xf32>, memref<1536x256xf32, strided<[?, ?], offset: ?>>) outs(%alloc_3002 : memref<1x256xf32>)
    %base_buffer_3003, %offset_3004, %sizes_3005, %strides_3006 = memref.extract_strided_metadata %arg388 : memref<256xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %206 = affine.apply #map11()[%strides_3006]
    %reinterpret_cast_3007 = memref.reinterpret_cast %base_buffer_3003 to offset: [%offset_3004], sizes: [1, 256], strides: [%206, %strides_3006] : memref<f32> to memref<1x256xf32, strided<[?, ?], offset: ?>>
    %alloc_3008 = memref.alloc() {alignment = 64 : i64} : memref<1x256xf32>
    linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%reinterpret_cast_3007, %alloc_3002 : memref<1x256xf32, strided<[?, ?], offset: ?>>, memref<1x256xf32>) outs(%alloc_3008 : memref<1x256xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_3009 = memref.reinterpret_cast %alloc_3008 to offset: [0], sizes: [1, 2, 1, 128], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x128xf32>
    %reinterpret_cast_3010 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x128xf32> to memref<1x1x1x128xf32>
    %reinterpret_cast_3011 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [1, 1, 1, 128], strides: [128, 128, 128, 1] : memref<1x1x2x64xf32> to memref<1x1x1x128xf32>
    %alloc_3012 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_2991, %reinterpret_cast_3010 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_3012 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_3013 = memref.reinterpret_cast %alloc_2990 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_3014 = memref.reinterpret_cast %alloc_2990 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x1536xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_3015 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3014 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>) outs(%alloc_3015 : memref<1x12x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_3016 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    %reinterpret_cast_3017 = memref.reinterpret_cast %alloc_3016 to offset: [0], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    memref.copy %alloc_3015, %reinterpret_cast_3017 : memref<1x12x1x64xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>>
    %reinterpret_cast_3018 = memref.reinterpret_cast %alloc_3016 to offset: [64], sizes: [1, 12, 1, 64], strides: [1536, 128, 128, 1] : memref<1x12x1x128xf32> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_3013, %reinterpret_cast_3018 : memref<1x12x1x64xf32, strided<[1536, 128, 128, 1]>> to memref<1x12x1x64xf32, strided<[1536, 128, 128, 1], offset: 64>>
    %alloc_3019 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_3016, %reinterpret_cast_3011 : memref<1x12x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_3019 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3020 = memref.alloc() {alignment = 64 : i64} : memref<1x12x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_3012, %alloc_3019 : memref<1x12x1x128xf32>, memref<1x12x1x128xf32>) outs(%alloc_3020 : memref<1x12x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3021 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3000, %reinterpret_cast_3010 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_3021 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_3022 = memref.reinterpret_cast %alloc_2999 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_3023 = memref.reinterpret_cast %alloc_2999 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x256xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_3024 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x64xf32>
    linalg.generic {indexing_maps = [#map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3023 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>) outs(%alloc_3024 : memref<1x2x1x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      linalg.yield %211 : f32
    }
    %alloc_3025 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    %reinterpret_cast_3026 = memref.reinterpret_cast %alloc_3025 to offset: [0], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    memref.copy %alloc_3024, %reinterpret_cast_3026 : memref<1x2x1x64xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>>
    %reinterpret_cast_3027 = memref.reinterpret_cast %alloc_3025 to offset: [64], sizes: [1, 2, 1, 64], strides: [256, 128, 128, 1] : memref<1x2x1x128xf32> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    memref.copy %reinterpret_cast_3022, %reinterpret_cast_3027 : memref<1x2x1x64xf32, strided<[256, 128, 128, 1]>> to memref<1x2x1x64xf32, strided<[256, 128, 128, 1], offset: 64>>
    %alloc_3028 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map12, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_3025, %reinterpret_cast_3011 : memref<1x2x1x128xf32>, memref<1x1x1x128xf32>) outs(%alloc_3028 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3029 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%alloc_3021, %alloc_3028 : memref<1x2x1x128xf32>, memref<1x2x1x128xf32>) outs(%alloc_3029 : memref<1x2x1x128xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3030 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg390, %alloc_3030 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_3031, %offset_3032, %sizes_3033, %strides_3034 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_3035 = memref.reinterpret_cast %base_buffer_3031 to offset: [%offset_3032], sizes: [1, 1, 1, 1], strides: [%strides_3034, %strides_3034, %strides_3034, %strides_3034] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_3036 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3035, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_3036 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %alloc_3029[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_3036[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_3030[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %alloc_3037 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1024x128xf32>
    memref.copy %arg391, %alloc_3037 : memref<1x2x1024x128xf32, strided<[?, ?, ?, ?], offset: ?>> to memref<1x2x1024x128xf32>
    %base_buffer_3038, %offset_3039, %sizes_3040, %strides_3041 = memref.extract_strided_metadata %arg3 : memref<1xi64, strided<[?], offset: ?>> -> memref<i64>, index, index, index
    %reinterpret_cast_3042 = memref.reinterpret_cast %base_buffer_3038 to offset: [%offset_3039], sizes: [1, 1, 1, 1], strides: [%strides_3041, %strides_3041, %strides_3041, %strides_3041] : memref<i64> to memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_3043 = memref.alloc() {alignment = 64 : i64} : memref<1x2x1x128xi64>
    linalg.generic {indexing_maps = [#map13, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3042, %6 : memref<1x1x1x1xi64, strided<[?, ?, ?, ?], offset: ?>>, memref<1x2x1x128xi64>) outs(%alloc_3043 : memref<1x2x1x128xi64>) {
    ^bb0(%in: i64, %in_3095: i64, %out: i64):
      %211 = arith.addi %in, %in_3095 : i64
      linalg.yield %211 : i64
    }
    scf.for %arg399 = %c0 to %c1 step %c1 {
      scf.for %arg400 = %c0 to %c2 step %c1 {
        scf.for %arg401 = %c0 to %c1 step %c1 {
          scf.for %arg402 = %c0 to %c128 step %c1 {
            %211 = memref.load %reinterpret_cast_3009[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xf32>
            %212 = memref.load %alloc_3043[%arg399, %arg400, %arg401, %arg402] : memref<1x2x1x128xi64>
            %213 = arith.index_cast %212 : i64 to index
            memref.store %211, %alloc_3037[%arg399, %arg400, %213, %arg402] : memref<1x2x1024x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_3044 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [1, 1, 1, 1024], strides: [1024, 1024, 1024, 1] : memref<1x1024xi1> to memref<1x1x1x1024xi1>
    %alloc_3045 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1x1024xf32>
    linalg.generic {indexing_maps = [#map8, #map8, #map8, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3044, %1, %0 : memref<1x1x1x1024xi1>, memref<1x1x1x1024xf32>, memref<1x1x1x1024xf32>) outs(%alloc_3045 : memref<1x1x1x1024xf32>) {
    ^bb0(%in: i1, %in_3095: f32, %in_3096: f32, %out: f32):
      %211 = arith.select %in, %in_3095, %in_3096 : f32
      linalg.yield %211 : f32
    }
    %alloc_3046 = memref.alloc() : memref<1x12x1x128xf32>
    %alloc_3047 = memref.alloc() : memref<1x12x1xf32>
    %alloc_3048 = memref.alloc() : memref<128xf32>
    affine.for %arg399 = 0 to 1 {
      affine.for %arg400 = 0 to 12 {
        %211 = arith.divsi %arg400, %c6 : index
        affine.for %arg401 = 0 to 1 {
          affine.for %arg402 = 0 to 128 {
            memref.store %cst, %alloc_3048[%arg402] : memref<128xf32>
          }
          %212:2 = affine.for %arg402 = 0 to 1024 iter_args(%arg403 = %cst_2, %arg404 = %cst) -> (f32, f32) {
            %213 = affine.for %arg405 = 0 to 128 step 16 iter_args(%arg406 = %cst_3) -> (vector<16xf32>) {
              %228 = vector.load %alloc_3020[%arg399, %arg400, %arg401, %arg405] : memref<1x12x1x128xf32>, vector<16xf32>
              %229 = vector.load %alloc_3030[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>, vector<16xf32>
              %230 = vector.fma %228, %229, %arg406 : vector<16xf32>
              affine.yield %230 : vector<16xf32>
            }
            %214 = vector.reduction <add>, %213 : vector<16xf32> into f32
            %215 = arith.mulf %214, %cst_1 : f32
            %216 = memref.load %alloc_3045[%arg399, %c0, %arg401, %arg402] : memref<1x1x1x1024xf32>
            %217 = arith.addf %215, %216 : f32
            %218 = arith.cmpf ogt, %217, %arg403 : f32
            %219 = arith.select %218, %217, %arg403 : f32
            %220 = arith.subf %arg403, %217 : f32
            %221 = math.exp %220 : f32
            %222 = arith.mulf %221, %arg404 : f32
            %223 = arith.addf %222, %cst_0 : f32
            %224 = arith.subf %217, %arg403 : f32
            %225 = math.exp %224 : f32
            %226 = arith.addf %arg404, %225 : f32
            %227 = arith.select %218, %223, %226 : f32
            affine.for %arg405 = 0 to 128 {
              %228 = memref.load %alloc_3037[%arg399, %211, %arg402, %arg405] : memref<1x2x1024x128xf32>
              %229 = memref.load %alloc_3048[%arg405] : memref<128xf32>
              %230 = arith.mulf %229, %221 : f32
              %231 = arith.addf %230, %228 : f32
              %232 = arith.mulf %225, %228 : f32
              %233 = arith.addf %232, %229 : f32
              %234 = arith.select %218, %231, %233 : f32
              memref.store %234, %alloc_3048[%arg405] : memref<128xf32>
            }
            affine.yield %219, %227 : f32, f32
          }
          memref.store %212#1, %alloc_3047[%arg399, %arg400, %arg401] : memref<1x12x1xf32>
          affine.for %arg402 = 0 to 128 {
            %213 = memref.load %alloc_3048[%arg402] : memref<128xf32>
            %214 = arith.divf %213, %212#1 : f32
            memref.store %214, %alloc_3046[%arg399, %arg400, %arg401, %arg402] : memref<1x12x1x128xf32>
          }
        }
      }
    }
    %reinterpret_cast_3049 = memref.reinterpret_cast %alloc_3046 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x12x1x128xf32> to memref<1x1536xf32>
    %alloc_3050 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_3050 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_3049, %arg392 : memref<1x1536xf32>, memref<1536x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_3050 : memref<1x1536xf32>)
    %reinterpret_cast_3051 = memref.reinterpret_cast %alloc_3050 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_3052 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_2969, %reinterpret_cast_3051 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_3052 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3053 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3052 : memref<1x1x1536xf32>) outs(%alloc_3053 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_3054 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_3054 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_3053 : memref<1x1x1536xf32>) outs(%alloc_3054 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_3055 = memref.reinterpret_cast %alloc_3054 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_3056 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3055, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_3056 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3057 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3056, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_3057 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3058 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3057 : memref<1x1x1xf32>) outs(%alloc_3058 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_3059 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3052, %alloc_3058 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_3059 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_3060, %offset_3061, %sizes_3062, %strides_3063 = memref.extract_strided_metadata %arg393 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %207 = affine.apply #map10()[%strides_3063]
    %208 = affine.apply #map10()[%strides_3063]
    %reinterpret_cast_3064 = memref.reinterpret_cast %base_buffer_3060 to offset: [%offset_3061], sizes: [1, 1, 1536], strides: [%207, %208, %strides_3063] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_3065 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3064, %alloc_3059 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_3065 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_3066 = memref.reinterpret_cast %alloc_3065 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_3067 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_3067 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_3066, %arg394 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_3067 : memref<1x8960xf32>)
    %reinterpret_cast_3068 = memref.reinterpret_cast %alloc_3067 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_3069 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3068 : memref<1x1x8960xf32>) outs(%alloc_3069 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = arith.negf %in : f32
      %212 = math.exp %211 : f32
      %213 = arith.addf %212, %cst_0 : f32
      %214 = arith.divf %cst_0, %213 : f32
      linalg.yield %214 : f32
    }
    %alloc_3070 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3068, %alloc_3069 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_3070 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_3071 = memref.reinterpret_cast %alloc_3065 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_3072 = memref.alloc() {alignment = 64 : i64} : memref<1x8960xf32>
    memref.copy %5, %alloc_3072 : memref<1x8960xf32> to memref<1x8960xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_3071, %arg395 : memref<1x1536xf32>, memref<1536x8960xf32, strided<[?, ?], offset: ?>>) outs(%alloc_3072 : memref<1x8960xf32>)
    %reinterpret_cast_3073 = memref.reinterpret_cast %alloc_3072 to offset: [0], sizes: [1, 1, 8960], strides: [8960, 8960, 1] : memref<1x8960xf32> to memref<1x1x8960xf32>
    %alloc_3074 = memref.alloc() {alignment = 64 : i64} : memref<1x1x8960xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3070, %reinterpret_cast_3073 : memref<1x1x8960xf32>, memref<1x1x8960xf32>) outs(%alloc_3074 : memref<1x1x8960xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_3075 = memref.reinterpret_cast %alloc_3074 to offset: [0], sizes: [1, 8960], strides: [8960, 1] : memref<1x1x8960xf32> to memref<1x8960xf32>
    %alloc_3076 = memref.alloc() {alignment = 64 : i64} : memref<1x1536xf32>
    memref.copy %8, %alloc_3076 : memref<1x1536xf32> to memref<1x1536xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_3075, %arg396 : memref<1x8960xf32>, memref<8960x1536xf32, strided<[?, ?], offset: ?>>) outs(%alloc_3076 : memref<1x1536xf32>)
    %reinterpret_cast_3077 = memref.reinterpret_cast %alloc_3076 to offset: [0], sizes: [1, 1, 1536], strides: [1536, 1536, 1] : memref<1x1536xf32> to memref<1x1x1536xf32>
    %alloc_3078 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3052, %reinterpret_cast_3077 : memref<1x1x1536xf32>, memref<1x1x1536xf32>) outs(%alloc_3078 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3079 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3078 : memref<1x1x1536xf32>) outs(%alloc_3079 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.fpowi %in, %c2_i32 : f32, i32
      linalg.yield %211 : f32
    }
    %alloc_3080 = memref.alloc() {alignment = 64 : i64} : memref<1x1xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_3080 : memref<1x1xf32>)
    linalg.reduce ins(%alloc_3079 : memref<1x1x1536xf32>) outs(%alloc_3080 : memref<1x1xf32>) dimensions = [2]
      (%in: f32, %init: f32) {
        %211 = arith.addf %in, %init : f32
        linalg.yield %211 : f32
      }
    %reinterpret_cast_3081 = memref.reinterpret_cast %alloc_3080 to offset: [0], sizes: [1, 1, 1], strides: [1, 1, 1] : memref<1x1xf32> to memref<1x1x1xf32>
    %alloc_3082 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3081, %2 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_3082 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3083 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3082, %9 : memref<1x1x1xf32>, memref<1x1x1xf32>) outs(%alloc_3083 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.addf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %alloc_3084 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1xf32>
    linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3083 : memref<1x1x1xf32>) outs(%alloc_3084 : memref<1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %211 = math.rsqrt %in : f32
      linalg.yield %211 : f32
    }
    %alloc_3085 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map9, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%alloc_3078, %alloc_3084 : memref<1x1x1536xf32>, memref<1x1x1xf32>) outs(%alloc_3085 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %base_buffer_3086, %offset_3087, %sizes_3088, %strides_3089 = memref.extract_strided_metadata %arg397 : memref<1536xf32, strided<[?], offset: ?>> -> memref<f32>, index, index, index
    %209 = affine.apply #map10()[%strides_3089]
    %210 = affine.apply #map10()[%strides_3089]
    %reinterpret_cast_3090 = memref.reinterpret_cast %base_buffer_3086 to offset: [%offset_3087], sizes: [1, 1, 1536], strides: [%209, %210, %strides_3089] : memref<f32> to memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>
    %alloc_3091 = memref.alloc() {alignment = 64 : i64} : memref<1x1x1536xf32>
    linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%reinterpret_cast_3090, %alloc_3085 : memref<1x1x1536xf32, strided<[?, ?, ?], offset: ?>>, memref<1x1x1536xf32>) outs(%alloc_3091 : memref<1x1x1536xf32>) {
    ^bb0(%in: f32, %in_3095: f32, %out: f32):
      %211 = arith.mulf %in, %in_3095 : f32
      linalg.yield %211 : f32
    }
    %reinterpret_cast_3092 = memref.reinterpret_cast %alloc_3091 to offset: [0], sizes: [1, 1536], strides: [1536, 1] : memref<1x1x1536xf32> to memref<1x1536xf32>
    %alloc_3093 = memref.alloc() {alignment = 64 : i64} : memref<1x151936xf32>
    memref.copy %4, %alloc_3093 : memref<1x151936xf32> to memref<1x151936xf32>
    linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%reinterpret_cast_3092, %arg398 : memref<1x1536xf32>, memref<1536x151936xf32, strided<[?, ?], offset: ?>>) outs(%alloc_3093 : memref<1x151936xf32>)
    %reinterpret_cast_3094 = memref.reinterpret_cast %alloc_3093 to offset: [0], sizes: [1, 1, 151936], strides: [151936, 151936, 1] : memref<1x151936xf32> to memref<1x1x151936xf32>
    scf.if %true {
      memref.dealloc %alloc : memref<1x1xi32>
    }
    scf.if %true {
      memref.dealloc %alloc_4 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_10 : memref<1x1024xi1>
    }
    scf.if %true {
      memref.dealloc %alloc_21 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_22 : memref<1x64x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_24 : memref<1x1x2x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_26 : memref<1x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_27 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_28 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_30 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_31 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_32 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_33 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_39 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_41 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_47 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_50 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_56 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_59 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_65 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_69 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_72 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_73 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_76 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_77 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_78 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_81 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_82 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_85 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_86 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_93 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_100 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_102 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_103 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_104 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_105 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_107 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_109 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_110 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_111 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_113 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_114 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_115 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_116 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_122 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_124 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_126 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_127 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_129 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_131 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_133 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_135 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_136 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_137 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_139 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_140 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_141 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_142 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_148 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_150 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_156 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_159 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_165 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_168 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_174 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_178 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_181 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_182 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_185 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_186 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_187 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_190 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_191 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_194 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_195 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_202 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_209 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_211 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_212 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_213 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_214 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_216 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_218 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_219 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_220 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_222 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_223 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_224 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_225 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_231 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_233 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_235 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_236 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_238 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_240 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_242 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_244 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_245 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_246 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_248 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_249 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_250 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_251 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_257 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_259 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_265 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_268 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_274 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_277 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_283 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_287 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_290 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_291 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_294 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_295 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_296 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_299 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_300 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_303 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_304 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_311 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_318 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_320 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_321 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_322 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_323 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_325 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_327 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_328 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_329 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_331 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_332 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_333 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_334 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_340 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_342 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_344 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_345 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_347 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_349 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_351 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_353 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_354 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_355 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_357 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_358 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_359 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_360 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_366 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_368 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_374 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_377 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_383 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_386 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_392 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_396 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_399 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_400 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_403 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_404 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_405 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_408 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_409 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_412 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_413 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_420 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_427 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_429 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_430 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_431 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_432 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_434 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_436 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_437 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_438 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_440 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_441 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_442 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_443 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_449 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_451 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_453 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_454 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_456 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_458 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_460 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_462 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_463 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_464 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_466 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_467 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_468 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_469 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_475 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_477 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_483 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_486 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_492 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_495 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_501 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_505 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_508 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_509 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_512 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_513 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_514 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_517 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_518 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_521 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_522 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_529 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_536 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_538 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_539 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_540 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_541 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_543 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_545 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_546 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_547 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_549 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_550 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_551 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_552 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_558 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_560 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_562 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_563 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_565 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_567 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_569 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_571 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_572 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_573 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_575 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_576 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_577 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_578 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_584 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_586 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_592 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_595 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_601 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_604 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_610 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_614 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_617 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_618 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_621 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_622 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_623 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_626 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_627 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_630 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_631 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_638 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_645 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_647 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_648 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_649 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_650 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_652 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_654 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_655 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_656 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_658 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_659 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_660 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_661 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_667 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_669 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_671 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_672 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_674 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_676 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_678 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_680 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_681 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_682 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_684 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_685 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_686 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_687 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_693 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_695 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_701 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_704 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_710 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_713 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_719 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_723 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_726 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_727 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_730 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_731 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_732 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_735 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_736 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_739 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_740 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_747 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_754 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_756 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_757 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_758 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_759 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_761 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_763 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_764 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_765 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_767 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_768 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_769 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_770 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_776 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_778 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_780 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_781 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_783 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_785 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_787 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_789 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_790 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_791 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_793 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_794 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_795 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_796 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_802 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_804 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_810 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_813 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_819 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_822 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_828 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_832 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_835 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_836 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_839 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_840 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_841 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_844 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_845 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_848 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_849 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_856 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_863 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_865 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_866 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_867 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_868 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_870 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_872 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_873 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_874 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_876 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_877 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_878 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_879 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_885 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_887 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_889 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_890 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_892 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_894 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_896 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_898 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_899 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_900 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_902 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_903 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_904 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_905 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_911 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_913 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_919 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_922 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_928 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_931 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_937 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_941 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_944 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_945 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_948 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_949 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_950 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_953 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_954 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_957 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_958 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_965 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_972 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_974 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_975 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_976 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_977 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_979 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_981 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_982 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_983 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_985 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_986 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_987 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_988 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_994 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_996 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_998 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_999 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1001 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1003 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1005 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1007 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1008 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1009 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1011 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1012 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1013 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1014 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1020 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1022 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1028 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1031 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1037 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1040 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1046 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1050 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1053 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1054 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1057 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1058 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1059 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1062 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1063 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1066 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1067 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1074 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1081 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1083 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1084 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1085 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1086 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1088 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1090 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1091 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1092 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1094 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1095 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1096 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1097 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1103 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1105 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1107 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1108 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1110 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1112 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1114 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1116 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1117 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1118 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1120 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1121 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1122 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1123 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1129 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1131 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1137 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1140 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1146 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1149 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1155 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1159 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1162 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1163 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1166 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1167 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1168 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1171 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1172 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1175 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1176 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1183 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1190 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1192 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1193 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1194 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1195 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1197 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1199 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1200 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1201 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1203 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1204 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1205 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1206 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1212 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1214 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1216 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1217 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1219 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1221 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1223 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1225 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1226 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1227 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1229 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1230 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1231 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1232 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1238 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1240 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1246 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1249 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1255 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1258 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1264 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1268 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1271 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1272 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1275 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1276 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1277 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1280 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1281 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1284 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1285 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1292 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1299 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1301 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1302 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1303 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1304 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1306 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1308 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1309 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1310 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1312 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1313 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1314 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1315 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1321 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1323 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1325 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1326 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1328 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1330 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1332 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1334 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1335 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1336 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1338 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1339 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1340 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1341 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1347 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1349 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1355 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1358 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1364 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1367 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1373 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1377 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1380 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1381 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1384 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1385 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1386 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1389 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1390 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1393 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1394 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1401 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1408 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1410 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1411 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1412 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1413 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1415 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1417 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1418 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1419 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1421 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1422 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1423 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1424 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1430 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1432 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1434 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1435 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1437 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1439 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1441 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1443 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1444 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1445 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1447 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1448 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1449 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1450 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1456 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1458 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1464 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1467 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1473 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1476 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1482 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1486 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1489 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1490 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1493 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1494 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1495 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1498 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1499 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1502 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1503 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1510 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1517 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1519 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1520 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1521 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1522 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1524 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1526 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1527 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1528 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1530 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1531 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1532 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1533 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1539 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1541 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1543 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1544 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1546 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1548 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1550 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1552 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1553 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1554 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1556 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1557 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1558 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1559 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1565 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1567 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1573 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1576 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1582 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1585 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1591 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1595 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1598 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1599 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1602 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1603 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1604 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1607 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1608 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1611 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1612 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1619 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1626 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1628 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1629 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1630 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1631 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1633 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1635 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1636 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1637 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1639 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1640 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1641 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1642 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1648 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1650 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1652 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1653 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1655 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1657 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1659 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1661 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1662 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1663 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1665 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1666 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1667 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1668 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1674 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1676 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1682 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1685 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1691 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1694 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1700 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1704 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1707 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1708 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1711 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1712 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1713 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1716 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1717 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1720 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1721 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1728 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1735 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1737 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1738 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1739 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1740 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1742 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1744 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1745 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1746 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1748 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1749 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1750 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1751 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1757 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1759 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1761 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1762 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1764 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1766 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1768 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1770 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1771 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1772 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1774 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1775 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1776 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1777 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1783 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1785 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1791 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1794 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1800 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1803 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1809 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1813 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1816 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1817 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1820 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1821 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1822 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1825 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1826 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1829 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1830 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1837 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1844 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1846 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1847 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1848 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1849 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1851 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1853 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1854 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1855 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1857 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1858 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1859 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1860 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1866 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1868 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1870 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1871 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1873 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1875 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1877 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1879 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1880 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1881 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1883 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1884 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1885 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1886 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1892 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1894 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1900 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1903 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1909 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1912 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1918 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1922 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1925 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1926 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1929 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1930 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1931 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1934 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1935 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1938 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1939 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1946 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1953 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_1955 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1956 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1957 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1958 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1960 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1962 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1963 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1964 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1966 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1967 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1968 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1969 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1975 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1977 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1979 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1980 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1982 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1984 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1986 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1988 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1989 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1990 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1992 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1993 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1994 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1995 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2001 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2003 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2009 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2012 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2018 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2021 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2027 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2031 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2034 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2035 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2038 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2039 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2040 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2043 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2044 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2047 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2048 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2055 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2062 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2064 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2065 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2066 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2067 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2069 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2071 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2072 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2073 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2075 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2076 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2077 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2078 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2084 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2086 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2088 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2089 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2091 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2093 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2095 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2097 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2098 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2099 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2101 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2102 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2103 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2104 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2110 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2112 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2118 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2121 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2127 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2130 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2136 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2140 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2143 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2144 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2147 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2148 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2149 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2152 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2153 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2156 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2157 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2164 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2171 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2173 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2174 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2175 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2176 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2178 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2180 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2181 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2182 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2184 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2185 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2186 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2187 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2193 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2195 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2197 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2198 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2200 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2202 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2204 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2206 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2207 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2208 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2210 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2211 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2212 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2213 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2219 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2221 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2227 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2230 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2236 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2239 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2245 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2249 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2252 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2253 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2256 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2257 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2258 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2261 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2262 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2265 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2266 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2273 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2280 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2282 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2283 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2284 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2285 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2287 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2289 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2290 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2291 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2293 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2294 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2295 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2296 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2302 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2304 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2306 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2307 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2309 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2311 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2313 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2315 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2316 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2317 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2319 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2320 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2321 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2322 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2328 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2330 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2336 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2339 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2345 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2348 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2354 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2358 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2361 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2362 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2365 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2366 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2367 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2370 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2371 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2374 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2375 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2382 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2389 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2391 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2392 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2393 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2394 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2396 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2398 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2399 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2400 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2402 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2403 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2404 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2405 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2411 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2413 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2415 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2416 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2418 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2420 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2422 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2424 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2425 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2426 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2428 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2429 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2430 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2431 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2437 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2439 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2445 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2448 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2454 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2457 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2463 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2467 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2470 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2471 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2474 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2475 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2476 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2479 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2480 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2483 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2484 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2491 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2498 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2500 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2501 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2502 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2503 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2505 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2507 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2508 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2509 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2511 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2512 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2513 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2514 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2520 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2522 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2524 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2525 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2527 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2529 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2531 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2533 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2534 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2535 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2537 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2538 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2539 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2540 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2546 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2548 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2554 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2557 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2563 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2566 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2572 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2576 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2579 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2580 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2583 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2584 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2585 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2588 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2589 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2592 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2593 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2600 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2607 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2609 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2610 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2611 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2612 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2614 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2616 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2617 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2618 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2620 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2621 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2622 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2623 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2629 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2631 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2633 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2634 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2636 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2638 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2640 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2642 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2643 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2644 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2646 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2647 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2648 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2649 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2655 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2657 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2663 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2666 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2672 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2675 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2681 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2685 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2688 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2689 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2692 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2693 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2694 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2697 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2698 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2701 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2702 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2709 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2716 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2718 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2719 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2720 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2721 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2723 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2725 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2726 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2727 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2729 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2730 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2731 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2732 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2738 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2740 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2742 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2743 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2745 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2747 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2749 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2751 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2752 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2753 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2755 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2756 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2757 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2758 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2764 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2766 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2772 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2775 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2781 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2784 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2790 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2794 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2797 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2798 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2801 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2802 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2803 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2806 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2807 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2810 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2811 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2818 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2825 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2827 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2828 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2829 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2830 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2832 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2834 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2835 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2836 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2838 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2839 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2840 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2841 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2847 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2849 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2851 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2852 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2854 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2856 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2858 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2860 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2861 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2862 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2864 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2865 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2866 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2867 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2873 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2875 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2881 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2884 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2890 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2893 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2899 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2903 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2906 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2907 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2910 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2911 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2912 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2915 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2916 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2919 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2920 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2927 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2934 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_2936 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2937 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2938 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2939 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2941 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2943 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2944 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2945 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2947 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2948 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2949 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2950 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2956 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2958 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2960 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2961 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2963 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2965 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2967 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2969 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2970 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2971 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2973 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2974 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2975 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2976 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2982 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2984 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2990 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2993 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2999 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3002 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3008 : memref<1x256xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3012 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3015 : memref<1x12x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3016 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3019 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3020 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3021 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3024 : memref<1x2x1x64xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3025 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3028 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3029 : memref<1x2x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3036 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_3043 : memref<1x2x1x128xi64>
    }
    scf.if %true {
      memref.dealloc %alloc_3045 : memref<1x1x1x1024xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3046 : memref<1x12x1x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3047 : memref<1x12x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3048 : memref<128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3050 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3052 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3053 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3054 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3056 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3057 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3058 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3059 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3065 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3067 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3069 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3070 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3072 : memref<1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3074 : memref<1x1x8960xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3076 : memref<1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3078 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3079 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3080 : memref<1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3082 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3083 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3084 : memref<1x1x1xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3085 : memref<1x1x1536xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_3091 : memref<1x1x1536xf32>
    }
    return %alloc_87, %alloc_94, %alloc_196, %alloc_203, %alloc_305, %alloc_312, %alloc_414, %alloc_421, %alloc_523, %alloc_530, %alloc_632, %alloc_639, %alloc_741, %alloc_748, %alloc_850, %alloc_857, %alloc_959, %alloc_966, %alloc_1068, %alloc_1075, %alloc_1177, %alloc_1184, %alloc_1286, %alloc_1293, %alloc_1395, %alloc_1402, %alloc_1504, %alloc_1511, %alloc_1613, %alloc_1620, %alloc_1722, %alloc_1729, %alloc_1831, %alloc_1838, %alloc_1940, %alloc_1947, %alloc_2049, %alloc_2056, %alloc_2158, %alloc_2165, %alloc_2267, %alloc_2274, %alloc_2376, %alloc_2383, %alloc_2485, %alloc_2492, %alloc_2594, %alloc_2601, %alloc_2703, %alloc_2710, %alloc_2812, %alloc_2819, %alloc_2921, %alloc_2928, %alloc_3030, %alloc_3037, %reinterpret_cast_3094 : memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x2x1024x128xf32>, memref<1x1x151936xf32>
  }
}
